<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Ensemble models | Analytics with KNIME and R</title>
  <meta name="description" content="This is a draft." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Ensemble models | Analytics with KNIME and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a draft." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Ensemble models | Analytics with KNIME and R" />
  
  <meta name="twitter:description" content="This is a draft." />
  

<meta name="author" content="F Acito" />


<meta name="date" content="2021-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression.html"/>
<link rel="next" href="naive-bayes.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover page</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1</b> What is analytics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#some-trends-in-analytics"><i class="fa fa-check"></i><b>1.2</b> Some trends in analytics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#broadening-of-application-areas"><i class="fa fa-check"></i><b>1.2.1</b> Broadening of application areas</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#generalization-of-the-notion-of-data"><i class="fa fa-check"></i><b>1.2.2</b> Generalization of the notion of data</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#a-trend-from-slicing-and-dicing-data-to-more-advanced-techniques"><i class="fa fa-check"></i><b>1.2.3</b> A trend from “slicing and dicing” data to more advanced techniques</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#more-advanced-data-visualization"><i class="fa fa-check"></i><b>1.2.4</b> More advanced data visualization</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-analytics-process-model"><i class="fa fa-check"></i><b>1.3</b> The analytics process model</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html"><i class="fa fa-check"></i><b>2</b> Business understanding and problem definition</a>
<ul>
<li class="chapter" data-level="2.1" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#expert-views"><i class="fa fa-check"></i><b>2.1</b> Expert views</a></li>
<li class="chapter" data-level="2.2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#understanding-the-business"><i class="fa fa-check"></i><b>2.2</b> Understanding the business</a></li>
<li class="chapter" data-level="2.3" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#identifying-stakeholders"><i class="fa fa-check"></i><b>2.3</b> Identifying stakeholders</a></li>
<li class="chapter" data-level="2.4" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#structured-versus-unstructured-problems"><i class="fa fa-check"></i><b>2.4</b> Structured versus unstructured problems</a></li>
<li class="chapter" data-level="2.5" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#framing-the-problem"><i class="fa fa-check"></i><b>2.5</b> Framing the problem</a></li>
<li class="chapter" data-level="2.6" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#appendix-some-tools-for-problem-definition"><i class="fa fa-check"></i>Appendix: Some tools for problem definition</a>
<ul>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#right-to-left-thinking"><i class="fa fa-check"></i>Right to left thinking</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#reversing-the-problem"><i class="fa fa-check"></i>Reversing the problem</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#open-the-problem-with-whys"><i class="fa fa-check"></i>Open the problem with “whys”</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#challenge-assumptions"><i class="fa fa-check"></i>Challenge assumptions</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#chunking"><i class="fa fa-check"></i>Chunking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#problems"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html"><i class="fa fa-check"></i><b>3</b> Introduction to KNIME</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#the-knime-workbench"><i class="fa fa-check"></i><b>3.1</b> The KNIME Workbench</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#elements-of-the-knime-workbench"><i class="fa fa-check"></i><b>3.1.1</b> Elements of the KNIME Workbench</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#learning-to-use-knime"><i class="fa fa-check"></i><b>3.2</b> Learning to use KNIME</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-extensions-and-integrations"><i class="fa fa-check"></i><b>3.3</b> KNIME extensions and integrations</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-1-predicting-heart-disease"><i class="fa fa-check"></i><b>3.4</b> KNIME workflow example #1: Predicting heart disease</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-2-preparation-of-hospital-data"><i class="fa fa-check"></i><b>3.5</b> KNIME workflow example #2: Preparation of hospital data</a></li>
<li class="chapter" data-level="3.6" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#summary-1"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#problems-1"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>4</b> Data preparation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-preparation.html"><a href="data-preparation.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="data-preparation.html"><a href="data-preparation.html#obtaining-the-needed-data"><i class="fa fa-check"></i><b>4.2</b> Obtaining the needed data</a></li>
<li class="chapter" data-level="4.3" data-path="data-preparation.html"><a href="data-preparation.html#data-cleaning"><i class="fa fa-check"></i><b>4.3</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data-preparation.html"><a href="data-preparation.html#missing-values"><i class="fa fa-check"></i><b>4.3.1</b> Missing values</a></li>
<li class="chapter" data-level="4.3.2" data-path="data-preparation.html"><a href="data-preparation.html#outliers"><i class="fa fa-check"></i><b>4.3.2</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature engineering</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-preparation.html"><a href="data-preparation.html#data-transformations"><i class="fa fa-check"></i><b>4.4.1</b> Data transformations</a></li>
<li class="chapter" data-level="4.4.2" data-path="data-preparation.html"><a href="data-preparation.html#data-exploration"><i class="fa fa-check"></i><b>4.4.2</b> Data exploration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html"><i class="fa fa-check"></i><b>5</b> Principal components analytics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#approaches-to-dimension-reduction"><i class="fa fa-check"></i><b>5.1</b> Approaches to dimension reduction</a></li>
<li class="chapter" data-level="5.2" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#description"><i class="fa fa-check"></i><b>5.2</b> Description</a></li>
<li class="chapter" data-level="5.3" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#the-pca-model"><i class="fa fa-check"></i><b>5.3</b> The PCA model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html"><i class="fa fa-check"></i><b>6</b> Evaluating predictive models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#training-testing-and-validation-samples"><i class="fa fa-check"></i><b>6.2</b> Training, Testing, and Validation samples</a></li>
<li class="chapter" data-level="6.3" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-continuous-versus-discrete-targets"><i class="fa fa-check"></i><b>6.3</b> Evaluating continuous versus discrete targets</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-continuous-targets"><i class="fa fa-check"></i><b>6.3.1</b> Evaluating performance with continuous targets</a></li>
<li class="chapter" data-level="6.3.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-classification-models"><i class="fa fa-check"></i><b>6.3.2</b> Evaluating performance with classification models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-techniques"><i class="fa fa-check"></i><b>7.2</b> Regression techniques</a></li>
<li class="chapter" data-level="7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-explanation"><i class="fa fa-check"></i><b>7.3</b> Regression for explanation</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-prediction"><i class="fa fa-check"></i><b>7.4</b> Regression for prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#revisiting-regression-assumptions"><i class="fa fa-check"></i><b>7.4.1</b> Revisiting regression assumptions</a></li>
<li class="chapter" data-level="7.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction-example-used-toyota-corollas"><i class="fa fa-check"></i><b>7.4.2</b> Prediction example: Used Toyota Corollas</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-a-brief-history-of-regression"><i class="fa fa-check"></i>Appendix: A brief history of regression</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#problems-2"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#example-with-a-single-predictor"><i class="fa fa-check"></i><b>8.1</b> Example with a single predictor</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-predictive-analytic-in-hr"><i class="fa fa-check"></i><b>8.2</b> Example: Predictive analytic in HR</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#predictor-interpretation-and-importance"><i class="fa fa-check"></i><b>8.3</b> Predictor interpretation and importance</a></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Regularized logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#probability-calibration"><i class="fa fa-check"></i><b>8.5</b> Probability calibration</a></li>
<li class="chapter" data-level="8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluation-of-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Evaluation of logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ensemble-models.html"><a href="ensemble-models.html"><i class="fa fa-check"></i><b>9</b> Ensemble models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ensemble-models.html"><a href="ensemble-models.html#creating-ensemble-models"><i class="fa fa-check"></i><b>9.1</b> Creating ensemble models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ensemble-models.html"><a href="ensemble-models.html#reduced-variation"><i class="fa fa-check"></i><b>9.1.1</b> Reduced variation</a></li>
<li class="chapter" data-level="9.1.2" data-path="ensemble-models.html"><a href="ensemble-models.html#improved-performance"><i class="fa fa-check"></i><b>9.1.2</b> Improved performance</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ensemble-models.html"><a href="ensemble-models.html#parallel-and-sequential-learners"><i class="fa fa-check"></i><b>9.2</b> Parallel and sequential learners</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ensemble-models.html"><a href="ensemble-models.html#bagging-bootstrap-aggregating"><i class="fa fa-check"></i><b>9.2.1</b> Bagging (Bootstrap Aggregating)</a></li>
<li class="chapter" data-level="9.2.2" data-path="ensemble-models.html"><a href="ensemble-models.html#random-forests"><i class="fa fa-check"></i><b>9.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="9.2.3" data-path="ensemble-models.html"><a href="ensemble-models.html#adaboost"><i class="fa fa-check"></i><b>9.2.3</b> AdaBoost</a></li>
<li class="chapter" data-level="9.2.4" data-path="ensemble-models.html"><a href="ensemble-models.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>9.2.4</b> Gradient Boosting Machines</a></li>
<li class="chapter" data-level="9.2.5" data-path="ensemble-models.html"><a href="ensemble-models.html#xgboost"><i class="fa fa-check"></i><b>9.2.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ensemble-models.html"><a href="ensemble-models.html#example-of-ensemble-modeling-for-a-continuous-target"><i class="fa fa-check"></i><b>9.3</b> Example of ensemble modeling for a continuous target</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>10</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="10.1" data-path="naive-bayes.html"><a href="naive-bayes.html#a-thought-problem"><i class="fa fa-check"></i><b>10.1</b> A thought problem</a></li>
<li class="chapter" data-level="10.2" data-path="naive-bayes.html"><a href="naive-bayes.html#bayes-theorem-applied-to-predictive-analytics"><i class="fa fa-check"></i><b>10.2</b> Bayes Theorem applied to predictive analytics</a></li>
<li class="chapter" data-level="10.3" data-path="naive-bayes.html"><a href="naive-bayes.html#illustration-of-naïve-bayes-with-a-toy-data-set"><i class="fa fa-check"></i><b>10.3</b> Illustration of Naïve Bayes with a “toy” data set</a></li>
<li class="chapter" data-level="10.4" data-path="naive-bayes.html"><a href="naive-bayes.html#the-assumption-of-conditional-independence"><i class="fa fa-check"></i><b>10.4</b> The assumption of conditional independence</a></li>
<li class="chapter" data-level="10.5" data-path="naive-bayes.html"><a href="naive-bayes.html#naïve-bayes-with-continuous-predictors"><i class="fa fa-check"></i><b>10.5</b> Naïve Bayes with continuous predictors</a></li>
<li class="chapter" data-level="10.6" data-path="naive-bayes.html"><a href="naive-bayes.html#laplace-smoothing"><i class="fa fa-check"></i><b>10.6</b> Laplace Smoothing</a></li>
<li class="chapter" data-level="10.7" data-path="naive-bayes.html"><a href="naive-bayes.html#example-using-naïve-bayes-with-churn-data"><i class="fa fa-check"></i><b>10.7</b> Example using naïve Bayes with churn data</a></li>
<li class="chapter" data-level="10.8" data-path="naive-bayes.html"><a href="naive-bayes.html#spam-detection-using-naïve-bayes"><i class="fa fa-check"></i><b>10.8</b> Spam detection using naïve Bayes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>11</b> Deep learning</a></li>
<li class="chapter" data-level="12" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>12</b> k Nearest Neighbors</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#k-nearest-neighbors-and-memory-based-learning"><i class="fa fa-check"></i><b>12.1</b> k nearest neighbors and memory-based learning</a></li>
<li class="chapter" data-level="12.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#typical-applications"><i class="fa fa-check"></i><b>12.2</b> Typical applications</a></li>
<li class="chapter" data-level="12.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#what-is-knn"><i class="fa fa-check"></i><b>12.3</b> What is kNN?</a></li>
<li class="chapter" data-level="12.4" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#a-two-dimensional-graphic-example-of-knn"><i class="fa fa-check"></i><b>12.4</b> A two-dimensional graphic example of kNN</a></li>
<li class="chapter" data-level="12.5" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-of-knn-diagnosing-heart-disease"><i class="fa fa-check"></i><b>12.5</b> Example of kNN: Diagnosing heart disease</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#results"><i class="fa fa-check"></i><b>12.5.1</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-continuous-targets"><i class="fa fa-check"></i><b>12.6</b> kNN for continuous targets</a></li>
<li class="chapter" data-level="12.7" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-multiclass-target-variables"><i class="fa fa-check"></i><b>12.7</b> kNN for multiclass target variables</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="tree-models.html"><a href="tree-models.html"><i class="fa fa-check"></i><b>13</b> Tree models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="tree-models.html"><a href="tree-models.html#classification-trees"><i class="fa fa-check"></i><b>13.1</b> Classification trees</a></li>
<li class="chapter" data-level="13.2" data-path="tree-models.html"><a href="tree-models.html#forming-classification-trees"><i class="fa fa-check"></i><b>13.2</b> Forming classification trees</a></li>
<li class="chapter" data-level="13.3" data-path="tree-models.html"><a href="tree-models.html#varieties-of-classification-tree-algorithms"><i class="fa fa-check"></i><b>13.3</b> Varieties of classification tree algorithms</a></li>
<li class="chapter" data-level="13.4" data-path="tree-models.html"><a href="tree-models.html#criteria-for-splitting-and-growing-a-tree"><i class="fa fa-check"></i><b>13.4</b> Criteria for splitting and growing a tree</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="tree-models.html"><a href="tree-models.html#the-gini-index"><i class="fa fa-check"></i><b>13.4.1</b> The Gini index</a></li>
<li class="chapter" data-level="13.4.2" data-path="tree-models.html"><a href="tree-models.html#information-gain"><i class="fa fa-check"></i><b>13.4.2</b> Information Gain</a></li>
<li class="chapter" data-level="13.4.3" data-path="tree-models.html"><a href="tree-models.html#chi-square-as-a-splitting-criterion"><i class="fa fa-check"></i><b>13.4.3</b> Chi-square as a splitting criterion</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="tree-models.html"><a href="tree-models.html#overfitting"><i class="fa fa-check"></i><b>13.5</b> Overfitting</a></li>
<li class="chapter" data-level="13.6" data-path="tree-models.html"><a href="tree-models.html#example-of-a-classification-tree"><i class="fa fa-check"></i><b>13.6</b> Example of a classification tree</a></li>
<li class="chapter" data-level="13.7" data-path="tree-models.html"><a href="tree-models.html#regression-trees"><i class="fa fa-check"></i><b>13.7</b> Regression trees</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="tree-models.html"><a href="tree-models.html#how-regression-trees-work"><i class="fa fa-check"></i><b>13.7.1</b> How regression trees work</a></li>
<li class="chapter" data-level="13.7.2" data-path="tree-models.html"><a href="tree-models.html#example-predicting-home-prices"><i class="fa fa-check"></i><b>13.7.2</b> Example: Predicting home prices</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="tree-models.html"><a href="tree-models.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>13.8</b> Strengths and weaknesses</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>14</b> Neural networks</a>
<ul>
<li class="chapter" data-level="14.1" data-path="neural-networks.html"><a href="neural-networks.html#what-are-artificial-neural-networks"><i class="fa fa-check"></i><b>14.1</b> What are artificial neural networks?</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="neural-networks.html"><a href="neural-networks.html#human-neurons-to-mathematical-models"><i class="fa fa-check"></i><b>14.1.1</b> Human neurons to mathematical models</a></li>
<li class="chapter" data-level="14.1.2" data-path="neural-networks.html"><a href="neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>14.1.2</b> Activation functions</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="neural-networks.html"><a href="neural-networks.html#the-road-to-machine-learning-with-neural-nets"><i class="fa fa-check"></i><b>14.2</b> The road to machine learning with neural nets</a></li>
<li class="chapter" data-level="14.3" data-path="neural-networks.html"><a href="neural-networks.html#example-of-a-neural-network"><i class="fa fa-check"></i><b>14.3</b> Example of a neural network</a></li>
<li class="chapter" data-level="14.4" data-path="neural-networks.html"><a href="neural-networks.html#training-a-neural-net"><i class="fa fa-check"></i><b>14.4</b> Training a neural net</a></li>
<li class="chapter" data-level="14.5" data-path="neural-networks.html"><a href="neural-networks.html#considerations-in-using-neural-nets"><i class="fa fa-check"></i><b>14.5</b> Considerations in using neural nets</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="neural-networks.html"><a href="neural-networks.html#missing-data"><i class="fa fa-check"></i><b>14.5.1</b> Missing data</a></li>
<li class="chapter" data-level="14.5.2" data-path="neural-networks.html"><a href="neural-networks.html#representative-data"><i class="fa fa-check"></i><b>14.5.2</b> Representative data</a></li>
<li class="chapter" data-level="14.5.3" data-path="neural-networks.html"><a href="neural-networks.html#all-eventualities-must-be-covered"><i class="fa fa-check"></i><b>14.5.3</b> All eventualities must be covered</a></li>
<li class="chapter" data-level="14.5.4" data-path="neural-networks.html"><a href="neural-networks.html#unbalanced-data-sets"><i class="fa fa-check"></i><b>14.5.4</b> Unbalanced data sets</a></li>
<li class="chapter" data-level="14.5.5" data-path="neural-networks.html"><a href="neural-networks.html#the-overfitting-problem"><i class="fa fa-check"></i><b>14.5.5</b> The overfitting problem</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="neural-networks.html"><a href="neural-networks.html#neural-network-example"><i class="fa fa-check"></i><b>14.6</b> Neural network example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>15</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#approaches-to-forming-clusters"><i class="fa fa-check"></i><b>15.1</b> Approaches to forming clusters</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hierarchical-versus-partitioning-methods"><i class="fa fa-check"></i><b>15.1.1</b> Hierarchical versus partitioning methods</a></li>
<li class="chapter" data-level="15.1.2" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hard-versus-soft-methods"><i class="fa fa-check"></i><b>15.1.2</b> “Hard” versus “soft” methods</a></li>
<li class="chapter" data-level="15.1.3" data-path="cluster-analysis.html"><a href="cluster-analysis.html#applying-hierarchical-clusters"><i class="fa fa-check"></i><b>15.1.3</b> Applying hierarchical clusters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analytics with KNIME and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ensemble-models" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Ensemble models</h1>
<p>Many different machine learning models have been developed and new variants continue to be explored. This leads to questions such as, “Which algorithm is best?” and “Which algorithm is best for a particular problem context or data set?” As might be expected, there is no simple, definitive answer to these questions. An interesting comparison the performance in terms of error of five algorithms is shown in Figure <a href="ensemble-models.html#fig:FiveAlgorithmsPerformance">9.1</a> <span class="citation">(<a href="#ref-Seni2010" role="doc-biblioref">Seni and Elder 2010</a>)</span>. None of the algorithms performed best on all the data sets.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:FiveAlgorithmsPerformance"></span>
<img src="images_ensembles/FiveAlgorithmsPerformance.png" alt="Relative Performance of 5 Algorithms on 6 Datasets" width="80%" style="background-color: black; padding:2px; display: inline-block;" />
<p class="caption">
Figure 9.1: Relative Performance of 5 Algorithms on 6 Datasets
</p>
</div>
<p>Observations of model performance such as shown in Figure <a href="ensemble-models.html#fig:FiveAlgorithmsPerformance">9.1</a> led to the idea of combining the predictions from two or more models, a process called ensemble learning. Ensemble models can lead to lower prediction errors when the predictions of diverse individual models are independent and aggregated to make a final estimate. Everyday examples of the effectiveness of combining many independent estimates was the theme of the book The Wisdom of Crowds <span class="citation">(<a href="#ref-Surowiecki2004" role="doc-biblioref">Surowiecki 2004</a>)</span>. A famous example of the power of combining estimates is given below.</p>
<div class="insetNoBorder">
<p>In 1907 Sir Francis Galton published an article in Nature about combining individual estimates. He attended the West of England Fat Stock and Poultry Exhibition where he was intrigued by a weight guessing contest. The goal was to guess the weight of an ox when it was butchered and dressed. Around 800 people entered the contest and wrote their guesses on tickets. The person who guessed closest to the butchered weight of the ox won a prize.
After the contest Galton borrowed the tickets and found the median value of the guesses. He discovered that the median guess across all entrants was 1,207 pounds, only 9 pounds from the actual weight of 1,198 pounds. (The mean value of the guesses was exactly 1,198)</p>
</div>
<div id="creating-ensemble-models" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Creating ensemble models</h2>
<p>There are several ways to generate several different base learners that can be combined to create an ensemble model.</p>
<ol style="list-style-type: decimal">
<li>Different algorithms can be used, such as decision trees, logistic regression and neural networks.</li>
<li>Changing the parameters of a single model, such as the number of branches in a decision tree or the number of nodes in neural networks.</li>
<li>Using different subsets of predictor variables.</li>
<li>Using different subsets of observations.</li>
</ol>
<p>While different algorithms can be used as a basis for building ensemble models, in this chapter the focus is on decision tree, both for classifications and regression.</p>
<p>Ensemble models use weaknesses of decision trees to create an advantage. Tree models have been around for a long time, but until the development of ensemble models, their use was limited for two reasons. First, decision trees are unstable to different samples. That is, considerably different decision trees can result if the data set is changed slightly. This, in turn, is due to two characteristics of tree models. (a) The models produced are not globally optimum. Once a split is made, no further consideration is given to altering that split based on splits being made further down the tree. (b) If two potential predictor variables, <em>A</em> and <em>B</em>, are highly correlated, then if variable <em>A</em> is selected for a split, this effectively makes variable <em>B</em> essentially unimportant. That is to be expected. However, if a slightly different sample is selected, for example by randomly splitting a data set into training and test subsets, variable <em>B</em> might be selected first, making variable <em>A</em> apparently less important. Thus, changing the seed for the random split could result in quite different results.</p>
<p>There are two main benefits of ensemble models <span class="citation">(<a href="#ref-Brownlee2020" role="doc-biblioref">Brownlee 2020</a>)</span>:</p>
<ul>
<li>Reduced variation (reliability) in predictions with different data samples.</li>
<li>Improved prediction accuracy.</li>
</ul>
<div id="reduced-variation" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Reduced variation</h3>
<p>Performance evaluation of models is typically based on the accuracy of the predictions. This might be measured by the mean square error in the case of regression trees or errors made in classification trees. While these assessments are important, it is also important to consider the variation of performance as different data samples are analyzed. One way to assess this variation is to use a k-fold partitioning of a data set. If k samples are taken and submitted to the tree model, the performance will typically vary. The range or standard deviation of this variation will provide an indication of the likely performance on unseen data. It is possible that an ensemble model might not improve the average performance across k samples compared with a single tree model and yet have a smaller range of performance values.</p>
</div>
<div id="improved-performance" class="section level3" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Improved performance</h3>
<p>Ensemble models can perform better on average than a single tree model because errors are averaged out. By creating multiple tree models, problems with local optima can be avoided. Since different samples from a data set are likely to lead to different predictors and variation in performance, the risk of relying on a single model can be avoided.</p>
</div>
</div>
<div id="parallel-and-sequential-learners" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Parallel and sequential learners</h2>
<p>Tree ensemble models require two steps:</p>
<ol style="list-style-type: decimal">
<li>Creating a set of distinct predictive tree models.</li>
<li>Combining the predictions from the set of models to produce an overall prediction.</li>
</ol>
<p>Ensemble models can be classified into two types based on how the set of predictive models are created: <em>parallel learners</em> and <em>sequential learners</em>. Parallel learners generate a set of strong (or complex) models and the independence of the models is used to average out errors. Examples include Bagging and Random Forests.</p>
<p>With sequential learners, consecutive weak models are created, and errors captured at each stage of the process. The errors at each stage become the target variables for the next stage. Examples of sequential ensemble models include AdaBoost, Gradient Tree Boosting, and XGBoost.</p>
<div id="bagging-bootstrap-aggregating" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Bagging (Bootstrap Aggregating)</h3>
<p>Bagging multiple data sets by bootstrap sampling and then aggregating the results to create predictions is an early parallel ensemble approach
<span class="citation">(<a href="#ref-Breiman1996" role="doc-biblioref">L. Breiman 1966</a>)</span>. By bootstrap sampling of n observations, k repeated samples of size n are selected with replacement. Figure <a href="ensemble-models.html#fig:BaggedEnsemble">9.2</a> shows a diagram of the bagging approach. Repeated sampling means that some observations are left out of a given sample and some observations are included more than once. For regression trees, the average of the k predictions across becomes the ensemble prediction. For classification trees, a majority vote is taken to obtain the predicted class.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BaggedEnsemble"></span>
<img src="images_ensembles/BaggedEnsemble.png" alt="Creating a bagged ensemble" width="100%" style="background-color: black; padding:2px; display: inline-block;" />
<p class="caption">
Figure 9.2: Creating a bagged ensemble
</p>
</div>
</div>
<div id="random-forests" class="section level3" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Random Forests</h3>
<p>A random forest, another example of a parallel ensemble model, introduces another element of sampling to increase the diversity of models created. Instead of using all available predictors to create each tree, a sample of predictor variables is taken. So, two forms of randomization are involved: sampling observations with replacement and sampling of features. The diagram for random forests would appear similar to that for bagging shown in Figure <a href="ensemble-models.html#fig:BaggedEnsemble">9.2</a>, with the exception that each tree would be created with a randomized subset of the predictors.</p>
</div>
<div id="adaboost" class="section level3" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> AdaBoost</h3>
<p>Boosting is the process of building a large, iterative decision tree by fitting a sequence of smaller decision trees, called layers. The tree at each layer consists of a small number of splits. In fact, the trees can be tiny – only a few splits. The process for boosting is shown in Figure <a href="ensemble-models.html#fig:BoostedEnsemble">9.3</a>.</p>
<p>For regression trees, the process begins by creating a “weak” tree with just a few splits. This is called the “base” learner. Predictions are made from the initial split and the residual error is computed. The residuals have the information that the model could not explain. This residual error then becomes the target for a second “weak” tree. The idea is that the successive trees will tend to correct the errors, resulting in a more accurate model. As trees are added, the existing trees are not changed or revisited, making this another example of a “greedy algorithm.”</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BoostedEnsemble"></span>
<img src="images_ensembles/BoostedEnsemble.png" alt="Creating a boosted tree" width="80%" style="background-color: black; padding:2px; display: inline-block;" />
<p class="caption">
Figure 9.3: Creating a boosted tree
</p>
</div>
<p>The final prediction is an aggregation of the results. For regression trees, a one approach for creating an ensemble prediction is to simply average the predictions from the multiple levels of the small intermediate trees. For classification trees, a majority vote is taken to produce a prediction.</p>
<p>An early boosting algorithm, called AdaBoost, was developed by Freund and Schapire <span class="citation">(<a href="#ref-Freund1996" role="doc-biblioref">Freund and Schapire 1996</a>)</span> and used for categorical target variables (although subsequent extensions to continuous targets were implemented).</p>
<p>Two aspects of the AdaBoost model make this a powerful method. First, at each stage, a weak model is created and the errors in prediction are captured. The errors from a stage become the target for the next stage. Thus, as the model is faced with increasingly difficult predictions, the prediction accuracy improves. The second important aspect of AdaBoost is that at each stage, the errors in prediction from observations in the previous stage are given more weight compared with the observations that are accurate. In the initial stage, all weights are equal.</p>
<p>At the end of the process, AdaBoost creates a weighted ensemble of the predictions made at each, with weights proportional to the accuracy of each stage.</p>
</div>
<div id="gradient-boosting-machines" class="section level3" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> Gradient Boosting Machines</h3>
<p>Gradient Boosting Machines (GBM) <span class="citation">(<a href="#ref-Friedman2001" role="doc-biblioref">Friedman 2001</a>)</span> build on the approach of the AdaBoost algorithm by incorporating gradient descent for optimization. These models have been popular and successfully applied in several contexts. GBM models are quite flexible and have been implemented for time-to-event, Poisson regression, and multinomial classification problems. In this section the focus will be on predicting continuous and binary target variable.</p>
<p>To implement the model, weights must be calculated for each predictor variable that will minimize prediction error, called a “loss function” in the context of the GBM models. For continuous responses, a typical loss function is based on the square of actual minus predicted values. For binary responses the loss function is the negative of the log-likelihood <span class="citation">(<a href="#ref-Natekin2013" role="doc-biblioref">Natekin and Knoll 2013</a>)</span>.</p>
<p>A brute force approach would be to form a grid of all possible values (with some degree of increments) of each predictor. This unguided search process would be grossly inefficient to use in building a tree model. Instead, a calculus-based approach of finding a local minimum which is based on the error between the actual and predicted values of the target variable. The gradient (sort of multivariable slopes or derivatives) directs changes in the model parameters with the steepest slopes toward minimization.</p>
<p>The minimization process proceeds iteratively in small steps. (The step size can be set by the user. Large steps will reduce computer processing time but may miss a local minimum. Small steps are more likely to find a local minimum at the cost of increased time.) The number of iterations required depends on the particular problem and the step size and can range from a few to thousands or more. This method does not guarantee a global minimum, so algorithms typically incorporate multiple runs through the data with different starting values.</p>
<p>Friedman, the creator of Gradient Boosting Machines, updated his model calling it Stochastic Gradient Boosting <span class="citation">(<a href="#ref-Friedman2002" role="doc-biblioref">Friedman 2002</a>)</span>. This model inserted a step prior to the construction of each tree in the sequence which took a sample of the data. The tree was then formed on this sample and the loss function calculated. At the next iteration, another random sample of the full data set is selected, and so on. This reportedly increased the accuracy of the predictions as compared with GBM.</p>
</div>
<div id="xgboost" class="section level3" number="9.2.5">
<h3><span class="header-section-number">9.2.5</span> XGBoost</h3>
<p>Tianqi Chen and Carlos Guestrin <span class="citation">(<a href="#ref-Chen2016" role="doc-biblioref">Chen and Guestrin 1996</a>)</span> developed XGBoost (which stands for “eXtreme Gradient Boosting”) that has become one of the most popular ensemble modeling techniques in recent years, primarily due to its use in Kaggle competitions <span class="citation">(<a href="#ref-Adebayo2020" role="doc-biblioref">Adebayo 2020</a>)</span> and is considered “state of the art” by many practitioners <span class="citation">(<a href="#ref-Kunapuli2021" role="doc-biblioref">Kunapuli 2021</a>)</span>.</p>
<p>XGBoost is very flexible and customizable, it is open source and runs on many operating systems, provides options such as regularization to reduce overfitting, and has many parameters to tune the algorithm to specific applications. Furthermore, it executes faster than most comparable models.</p>
</div>
</div>
<div id="example-of-ensemble-modeling-for-a-continuous-target" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Example of ensemble modeling for a continuous target</h2>
<p>The file ToyotaCorolla.csv contains 1,436 records on used cars on sale during the summer of 2004 in The Netherlands. In addition to price of each automobile, there are details on more than 30 attributes, such as weight, age, number of kilometers, horsepower, and optional accessories such as power steering, central locking, and tow bars.</p>
A KNIME workflow (Figure <a href="ensemble-models.html#fig:ComparisonOLSvsGBT">9.4</a>) was used to create and compare ordinary least regression (OLS) and Gradient Boosted Trees to predict the prices of the Toyota Corollas.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ComparisonOLSvsGBT"></span>
<img src="images_ensembles/ComparisonOLS_vs_GBT.PNG" alt="Workflow for comparison of OLS with Gradient Boosted Trees" width="90%" style="background-color: black; padding:2px; display: inline-block;" />
<p class="caption">
Figure 9.4: Workflow for comparison of OLS with Gradient Boosted Trees
</p>
</div>
<p>The nodes in the workflow are described in Table <a href="ensemble-models.html#tab:OLSvsGBTNnodeDescriptions">9.1</a>,</p>
<table class=" lightable-paper" style="font-size: 12px; font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:OLSvsGBTNnodeDescriptions">Table 9.1: </span>Descriptions of nodes in the OLS vs. GBT workflow
</caption>
<thead>
<tr>
<th style="text-align:center;">
Node
</th>
<th style="text-align:left;">
Label
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
1
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
File Reader
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Read the file ToyotaCorolla.csv
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
2
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Data preparation Metanode
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Column Filter: Remove ID and Model;
Rule Engine: Collapse infrequent colors to “Other”;
One to Many: Create dummy variables;
Column Filter: Drop one level of each dummy set.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
3
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Partitioning
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Create 70/30 split into training and test partitions.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
4
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Linear Regression Learner
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Run OLS with Price as target.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
5
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Regression Predictor
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Compute predicted Price using OLS model.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
6
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Numeric Scorer
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Calculate performance metrics for OLS model.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
7
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Gradient Boosted Trees Learner
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Run GBT with Price as target.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
8
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Gradient Boosted Trees Predictor
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Compute predicted Price using GBT model.
</td>
</tr>
<tr>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 5em; vertical-align: top">
9
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 18em; vertical-align: top">
Numeric Scorer
</td>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 30em; vertical-align: top">
Calculate performance metrics for GBT model.
</td>
</tr>
</tbody>
</table>
<p>The results of the comparative analysis are shown below in Table <a href="ensemble-models.html#tab:OLSvsGBT">9.2</a></p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:OLSvsGBT">Table 9.2: </span>Comparative performance of OLS and GBT
</caption>
<thead>
<tr>
<th style="text-align:left;">
Metric
</th>
<th style="text-align:center;">
OLS
</th>
<th style="text-align:center;">
GBT
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
R2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.896
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.924
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
RMSE
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
1244.000
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
1064.400
</td>
</tr>
</tbody>
</table>
<p>The results show that the Gradient Boosted Tree performed slightly better than ordinary regression, especially in terms of the Root Mean Square Error (RMSE).</p>
<p>A separate comparison was run (the workflow is not shown) in which a 10-fold cross-validation for both OLS and GBT was performed. This was done to compare the stability of both models in terms of producing comparable accuracy. One of the advantages of ensemble models is reduced variation with different samples of a data set. This is important since it implies that ensemble models would perform as expected with new data sets. The results of the stability assessment are shown in Table <a href="ensemble-models.html#tab:StabilityComparisonOLSvsGBT">9.3</a>.</p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:StabilityComparisonOLSvsGBT">Table 9.3: </span>Comparison of OLS and GBT stability
</caption>
<thead>
<tr>
<th style="text-align:left;">
Metric
</th>
<th style="text-align:center;">
OLS
</th>
<th style="text-align:center;">
GBT
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Mean R2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.898
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.909
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Stdev R2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.032
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.013
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Range R2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.119
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
0.042
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Mean RMSE
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
1118.837
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
1074.502
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Stdev RMSE
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
116.257
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
92.774
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Range RMSE
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
349.285
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
251.704
</td>
</tr>
</tbody>
</table>
<p>The results in the table show that the stability of the ensemble model was better than the OLS model over 10 replications both in terms of R^2 and RMSE. The mean R^2 was only slightly better for GBT and the mean RMSE was only about 4% better with GBT.</p>
<p>In summary, for the Toyota Corolla data set, the overall performance in terms of accuracy was nearly the same for the single OLS model versus the GBT ensembles, but GBT exhibited greater consistency of performance. This, of course, will not necessarily be the case for different data sets and different ensemble models, so tests are needed in each actual situation.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Adebayo2020" class="csl-entry">
Adebayo, Samuel. 2020. <span>“How to Solve a Problem.”</span> https://dataaspirant.com.
</div>
<div id="ref-Breiman1996" class="csl-entry">
Breiman, Leo. 1966. <span>“Bagging Predictors.”</span> <em>Machine Learning</em>, 123–40.
</div>
<div id="ref-Brownlee2020" class="csl-entry">
Brownlee, Jason. 2020. <span>“Why Use Ensemble Learning?”</span> https://machinelearningmastery.com/why-use-ensemble-learning/.
</div>
<div id="ref-Chen2016" class="csl-entry">
Chen, Tianqi, and Carlos Guestrin. 1996. <span>“XGBoost: A Scalable Tree Boosting System.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785–94. https://dl.acm.org/doi/10.1145/2939672.2939785.
</div>
<div id="ref-Freund1996" class="csl-entry">
Freund, Yoav, and Robert E. Schapire. 1996. <span>“Experiments with a New Boosting Algorithm.”</span> In <em>Machine Learning: Proceedings of the Thirteenth International Conference</em>. https://cseweb.ucsd.edu// yfreund/papers/boostingexperiments.pdf.
</div>
<div id="ref-Friedman2001" class="csl-entry">
Friedman, Jerome. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
<div id="ref-Friedman2002" class="csl-entry">
———. 2002. <span>“Stochastic Gradient Boosting.”</span> <em>Computational Statistics &amp; Data Analysis</em>, 367–78.
</div>
<div id="ref-Kunapuli2021" class="csl-entry">
Kunapuli, Gautam. 2021. <em>Ensembles for Machine Learning</em>. Manning Publications.
</div>
<div id="ref-Natekin2013" class="csl-entry">
Natekin, Alexey, and Alois Knoll. 2013. <span>“Greedy Function Approximation: A Gradient Boosting Machine Tutorial.”</span> <em>Frontiers in Neurorobotics</em>, 1189–1232.
</div>
<div id="ref-Seni2010" class="csl-entry">
Seni, Giovanni, and John F. Elder. 2010. <em>Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions</em>. Morgan &amp; Claypool Publishers.
</div>
<div id="ref-Surowiecki2004" class="csl-entry">
Surowiecki, James. 2004. <em>The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations</em>. Doubleday; Anchor.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="naive-bayes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TextbookDraft.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
