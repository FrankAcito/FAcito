<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Cluster analysis | Analytics with KNIME and R</title>
  <meta name="description" content="This is a draft." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Cluster analysis | Analytics with KNIME and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a draft." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Cluster analysis | Analytics with KNIME and R" />
  
  <meta name="twitter:description" content="This is a draft." />
  

<meta name="author" content="F Acito" />


<meta name="date" content="2021-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="neural-networks.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover page</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1</b> What is analytics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#some-trends-in-analytics"><i class="fa fa-check"></i><b>1.2</b> Some trends in analytics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#broadening-of-application-areas"><i class="fa fa-check"></i><b>1.2.1</b> Broadening of application areas</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#generalization-of-the-notion-of-data"><i class="fa fa-check"></i><b>1.2.2</b> Generalization of the notion of data</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#a-trend-from-slicing-and-dicing-data-to-more-advanced-techniques"><i class="fa fa-check"></i><b>1.2.3</b> A trend from “slicing and dicing” data to more advanced techniques</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#more-advanced-data-visualization"><i class="fa fa-check"></i><b>1.2.4</b> More advanced data visualization</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-analytics-process-model"><i class="fa fa-check"></i><b>1.3</b> The analytics process model</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html"><i class="fa fa-check"></i><b>2</b> Business understanding and problem definition</a>
<ul>
<li class="chapter" data-level="2.1" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#expert-views"><i class="fa fa-check"></i><b>2.1</b> Expert views</a></li>
<li class="chapter" data-level="2.2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#understanding-the-business"><i class="fa fa-check"></i><b>2.2</b> Understanding the business</a></li>
<li class="chapter" data-level="2.3" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#identifying-stakeholders"><i class="fa fa-check"></i><b>2.3</b> Identifying stakeholders</a></li>
<li class="chapter" data-level="2.4" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#structured-versus-unstructured-problems"><i class="fa fa-check"></i><b>2.4</b> Structured versus unstructured problems</a></li>
<li class="chapter" data-level="2.5" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#framing-the-problem"><i class="fa fa-check"></i><b>2.5</b> Framing the problem</a></li>
<li class="chapter" data-level="2.6" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#appendix-some-tools-for-problem-definition"><i class="fa fa-check"></i>Appendix: Some tools for problem definition</a>
<ul>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#right-to-left-thinking"><i class="fa fa-check"></i>Right to left thinking</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#reversing-the-problem"><i class="fa fa-check"></i>Reversing the problem</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#open-the-problem-with-whys"><i class="fa fa-check"></i>Open the problem with “whys”</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#challenge-assumptions"><i class="fa fa-check"></i>Challenge assumptions</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#chunking"><i class="fa fa-check"></i>Chunking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#problems"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html"><i class="fa fa-check"></i><b>3</b> Introduction to KNIME</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#the-knime-workbench"><i class="fa fa-check"></i><b>3.1</b> The KNIME Workbench</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#elements-of-the-knime-workbench"><i class="fa fa-check"></i><b>3.1.1</b> Elements of the KNIME Workbench</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#learning-to-use-knime"><i class="fa fa-check"></i><b>3.2</b> Learning to use KNIME</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-extensions-and-integrations"><i class="fa fa-check"></i><b>3.3</b> KNIME extensions and integrations</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-1-predicting-heart-disease"><i class="fa fa-check"></i><b>3.4</b> KNIME workflow example #1: Predicting heart disease</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-2-preparation-of-hospital-data"><i class="fa fa-check"></i><b>3.5</b> KNIME workflow example #2: Preparation of hospital data</a></li>
<li class="chapter" data-level="3.6" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#summary-1"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#problems-1"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>4</b> Data preparation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-preparation.html"><a href="data-preparation.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="data-preparation.html"><a href="data-preparation.html#obtaining-the-needed-data"><i class="fa fa-check"></i><b>4.2</b> Obtaining the needed data</a></li>
<li class="chapter" data-level="4.3" data-path="data-preparation.html"><a href="data-preparation.html#data-cleaning"><i class="fa fa-check"></i><b>4.3</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data-preparation.html"><a href="data-preparation.html#missing-values"><i class="fa fa-check"></i><b>4.3.1</b> Missing values</a></li>
<li class="chapter" data-level="4.3.2" data-path="data-preparation.html"><a href="data-preparation.html#outliers"><i class="fa fa-check"></i><b>4.3.2</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature engineering</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-preparation.html"><a href="data-preparation.html#data-transformations"><i class="fa fa-check"></i><b>4.4.1</b> Data transformations</a></li>
<li class="chapter" data-level="4.4.2" data-path="data-preparation.html"><a href="data-preparation.html#data-exploration"><i class="fa fa-check"></i><b>4.4.2</b> Data exploration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html"><i class="fa fa-check"></i><b>5</b> Principal components analytics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#approaches-to-dimension-reduction"><i class="fa fa-check"></i><b>5.1</b> Approaches to dimension reduction</a></li>
<li class="chapter" data-level="5.2" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#description"><i class="fa fa-check"></i><b>5.2</b> Description</a></li>
<li class="chapter" data-level="5.3" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#the-pca-model"><i class="fa fa-check"></i><b>5.3</b> The PCA model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html"><i class="fa fa-check"></i><b>6</b> Evaluating predictive models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#training-testing-and-validation-samples"><i class="fa fa-check"></i><b>6.2</b> Training, Testing, and Validation samples</a></li>
<li class="chapter" data-level="6.3" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-continuous-versus-discrete-targets"><i class="fa fa-check"></i><b>6.3</b> Evaluating continuous versus discrete targets</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-continuous-targets"><i class="fa fa-check"></i><b>6.3.1</b> Evaluating performance with continuous targets</a></li>
<li class="chapter" data-level="6.3.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-classification-models"><i class="fa fa-check"></i><b>6.3.2</b> Evaluating performance with classification models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-techniques"><i class="fa fa-check"></i><b>7.2</b> Regression techniques</a></li>
<li class="chapter" data-level="7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-explanation"><i class="fa fa-check"></i><b>7.3</b> Regression for explanation</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-prediction"><i class="fa fa-check"></i><b>7.4</b> Regression for prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#revisiting-regression-assumptions"><i class="fa fa-check"></i><b>7.4.1</b> Revisiting regression assumptions</a></li>
<li class="chapter" data-level="7.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction-example-used-toyota-corollas"><i class="fa fa-check"></i><b>7.4.2</b> Prediction example: Used Toyota Corollas</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-a-brief-history-of-regression"><i class="fa fa-check"></i>Appendix: A brief history of regression</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#problems-2"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#example-with-a-single-predictor"><i class="fa fa-check"></i><b>8.1</b> Example with a single predictor</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-predictive-analytic-in-hr"><i class="fa fa-check"></i><b>8.2</b> Example: Predictive analytic in HR</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#predictor-interpretation-and-importance"><i class="fa fa-check"></i><b>8.3</b> Predictor interpretation and importance</a></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Regularized logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#probability-calibration"><i class="fa fa-check"></i><b>8.5</b> Probability calibration</a></li>
<li class="chapter" data-level="8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluation-of-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Evaluation of logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ensemble-models.html"><a href="ensemble-models.html"><i class="fa fa-check"></i><b>9</b> Ensemble models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ensemble-models.html"><a href="ensemble-models.html#creating-ensemble-models"><i class="fa fa-check"></i><b>9.1</b> Creating ensemble models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ensemble-models.html"><a href="ensemble-models.html#reduced-variation"><i class="fa fa-check"></i><b>9.1.1</b> Reduced variation</a></li>
<li class="chapter" data-level="9.1.2" data-path="ensemble-models.html"><a href="ensemble-models.html#improved-performance"><i class="fa fa-check"></i><b>9.1.2</b> Improved performance</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ensemble-models.html"><a href="ensemble-models.html#parallel-and-sequential-learners"><i class="fa fa-check"></i><b>9.2</b> Parallel and sequential learners</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ensemble-models.html"><a href="ensemble-models.html#bagging-bootstrap-aggregating"><i class="fa fa-check"></i><b>9.2.1</b> Bagging (Bootstrap Aggregating)</a></li>
<li class="chapter" data-level="9.2.2" data-path="ensemble-models.html"><a href="ensemble-models.html#random-forests"><i class="fa fa-check"></i><b>9.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="9.2.3" data-path="ensemble-models.html"><a href="ensemble-models.html#adaboost"><i class="fa fa-check"></i><b>9.2.3</b> AdaBoost</a></li>
<li class="chapter" data-level="9.2.4" data-path="ensemble-models.html"><a href="ensemble-models.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>9.2.4</b> Gradient Boosting Machines</a></li>
<li class="chapter" data-level="9.2.5" data-path="ensemble-models.html"><a href="ensemble-models.html#xgboost"><i class="fa fa-check"></i><b>9.2.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ensemble-models.html"><a href="ensemble-models.html#example-of-ensemble-modeling-for-a-continuous-target"><i class="fa fa-check"></i><b>9.3</b> Example of ensemble modeling for a continuous target</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>10</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="10.1" data-path="naive-bayes.html"><a href="naive-bayes.html#a-thought-problem"><i class="fa fa-check"></i><b>10.1</b> A thought problem</a></li>
<li class="chapter" data-level="10.2" data-path="naive-bayes.html"><a href="naive-bayes.html#bayes-theorem-applied-to-predictive-analytics"><i class="fa fa-check"></i><b>10.2</b> Bayes Theorem applied to predictive analytics</a></li>
<li class="chapter" data-level="10.3" data-path="naive-bayes.html"><a href="naive-bayes.html#illustration-of-naïve-bayes-with-a-toy-data-set"><i class="fa fa-check"></i><b>10.3</b> Illustration of Naïve Bayes with a “toy” data set</a></li>
<li class="chapter" data-level="10.4" data-path="naive-bayes.html"><a href="naive-bayes.html#the-assumption-of-conditional-independence"><i class="fa fa-check"></i><b>10.4</b> The assumption of conditional independence</a></li>
<li class="chapter" data-level="10.5" data-path="naive-bayes.html"><a href="naive-bayes.html#naïve-bayes-with-continuous-predictors"><i class="fa fa-check"></i><b>10.5</b> Naïve Bayes with continuous predictors</a></li>
<li class="chapter" data-level="10.6" data-path="naive-bayes.html"><a href="naive-bayes.html#laplace-smoothing"><i class="fa fa-check"></i><b>10.6</b> Laplace Smoothing</a></li>
<li class="chapter" data-level="10.7" data-path="naive-bayes.html"><a href="naive-bayes.html#example-using-naïve-bayes-with-churn-data"><i class="fa fa-check"></i><b>10.7</b> Example using naïve Bayes with churn data</a></li>
<li class="chapter" data-level="10.8" data-path="naive-bayes.html"><a href="naive-bayes.html#spam-detection-using-naïve-bayes"><i class="fa fa-check"></i><b>10.8</b> Spam detection using naïve Bayes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>11</b> Deep learning</a></li>
<li class="chapter" data-level="12" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>12</b> k Nearest Neighbors</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#k-nearest-neighbors-and-memory-based-learning"><i class="fa fa-check"></i><b>12.1</b> k nearest neighbors and memory-based learning</a></li>
<li class="chapter" data-level="12.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#typical-applications"><i class="fa fa-check"></i><b>12.2</b> Typical applications</a></li>
<li class="chapter" data-level="12.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#what-is-knn"><i class="fa fa-check"></i><b>12.3</b> What is kNN?</a></li>
<li class="chapter" data-level="12.4" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#a-two-dimensional-graphic-example-of-knn"><i class="fa fa-check"></i><b>12.4</b> A two-dimensional graphic example of kNN</a></li>
<li class="chapter" data-level="12.5" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-of-knn-diagnosing-heart-disease"><i class="fa fa-check"></i><b>12.5</b> Example of kNN: Diagnosing heart disease</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#results"><i class="fa fa-check"></i><b>12.5.1</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-continuous-targets"><i class="fa fa-check"></i><b>12.6</b> kNN for continuous targets</a></li>
<li class="chapter" data-level="12.7" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-multiclass-target-variables"><i class="fa fa-check"></i><b>12.7</b> kNN for multiclass target variables</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="tree-models.html"><a href="tree-models.html"><i class="fa fa-check"></i><b>13</b> Tree models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="tree-models.html"><a href="tree-models.html#classification-trees"><i class="fa fa-check"></i><b>13.1</b> Classification trees</a></li>
<li class="chapter" data-level="13.2" data-path="tree-models.html"><a href="tree-models.html#forming-classification-trees"><i class="fa fa-check"></i><b>13.2</b> Forming classification trees</a></li>
<li class="chapter" data-level="13.3" data-path="tree-models.html"><a href="tree-models.html#varieties-of-classification-tree-algorithms"><i class="fa fa-check"></i><b>13.3</b> Varieties of classification tree algorithms</a></li>
<li class="chapter" data-level="13.4" data-path="tree-models.html"><a href="tree-models.html#criteria-for-splitting-and-growing-a-tree"><i class="fa fa-check"></i><b>13.4</b> Criteria for splitting and growing a tree</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="tree-models.html"><a href="tree-models.html#the-gini-index"><i class="fa fa-check"></i><b>13.4.1</b> The Gini index</a></li>
<li class="chapter" data-level="13.4.2" data-path="tree-models.html"><a href="tree-models.html#information-gain"><i class="fa fa-check"></i><b>13.4.2</b> Information Gain</a></li>
<li class="chapter" data-level="13.4.3" data-path="tree-models.html"><a href="tree-models.html#chi-square-as-a-splitting-criterion"><i class="fa fa-check"></i><b>13.4.3</b> Chi-square as a splitting criterion</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="tree-models.html"><a href="tree-models.html#overfitting"><i class="fa fa-check"></i><b>13.5</b> Overfitting</a></li>
<li class="chapter" data-level="13.6" data-path="tree-models.html"><a href="tree-models.html#example-of-a-classification-tree"><i class="fa fa-check"></i><b>13.6</b> Example of a classification tree</a></li>
<li class="chapter" data-level="13.7" data-path="tree-models.html"><a href="tree-models.html#regression-trees"><i class="fa fa-check"></i><b>13.7</b> Regression trees</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="tree-models.html"><a href="tree-models.html#how-regression-trees-work"><i class="fa fa-check"></i><b>13.7.1</b> How regression trees work</a></li>
<li class="chapter" data-level="13.7.2" data-path="tree-models.html"><a href="tree-models.html#example-predicting-home-prices"><i class="fa fa-check"></i><b>13.7.2</b> Example: Predicting home prices</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="tree-models.html"><a href="tree-models.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>13.8</b> Strengths and weaknesses</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>14</b> Neural networks</a>
<ul>
<li class="chapter" data-level="14.1" data-path="neural-networks.html"><a href="neural-networks.html#what-are-artificial-neural-networks"><i class="fa fa-check"></i><b>14.1</b> What are artificial neural networks?</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="neural-networks.html"><a href="neural-networks.html#human-neurons-to-mathematical-models"><i class="fa fa-check"></i><b>14.1.1</b> Human neurons to mathematical models</a></li>
<li class="chapter" data-level="14.1.2" data-path="neural-networks.html"><a href="neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>14.1.2</b> Activation functions</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="neural-networks.html"><a href="neural-networks.html#the-road-to-machine-learning-with-neural-nets"><i class="fa fa-check"></i><b>14.2</b> The road to machine learning with neural nets</a></li>
<li class="chapter" data-level="14.3" data-path="neural-networks.html"><a href="neural-networks.html#example-of-a-neural-network"><i class="fa fa-check"></i><b>14.3</b> Example of a neural network</a></li>
<li class="chapter" data-level="14.4" data-path="neural-networks.html"><a href="neural-networks.html#training-a-neural-net"><i class="fa fa-check"></i><b>14.4</b> Training a neural net</a></li>
<li class="chapter" data-level="14.5" data-path="neural-networks.html"><a href="neural-networks.html#considerations-in-using-neural-nets"><i class="fa fa-check"></i><b>14.5</b> Considerations in using neural nets</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="neural-networks.html"><a href="neural-networks.html#missing-data"><i class="fa fa-check"></i><b>14.5.1</b> Missing data</a></li>
<li class="chapter" data-level="14.5.2" data-path="neural-networks.html"><a href="neural-networks.html#representative-data"><i class="fa fa-check"></i><b>14.5.2</b> Representative data</a></li>
<li class="chapter" data-level="14.5.3" data-path="neural-networks.html"><a href="neural-networks.html#all-eventualities-must-be-covered"><i class="fa fa-check"></i><b>14.5.3</b> All eventualities must be covered</a></li>
<li class="chapter" data-level="14.5.4" data-path="neural-networks.html"><a href="neural-networks.html#unbalanced-data-sets"><i class="fa fa-check"></i><b>14.5.4</b> Unbalanced data sets</a></li>
<li class="chapter" data-level="14.5.5" data-path="neural-networks.html"><a href="neural-networks.html#the-overfitting-problem"><i class="fa fa-check"></i><b>14.5.5</b> The overfitting problem</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="neural-networks.html"><a href="neural-networks.html#neural-network-example"><i class="fa fa-check"></i><b>14.6</b> Neural network example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>15</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#approaches-to-forming-clusters"><i class="fa fa-check"></i><b>15.1</b> Approaches to forming clusters</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hierarchical-versus-partitioning-methods"><i class="fa fa-check"></i><b>15.1.1</b> Hierarchical versus partitioning methods</a></li>
<li class="chapter" data-level="15.1.2" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hard-versus-soft-methods"><i class="fa fa-check"></i><b>15.1.2</b> “Hard” versus “soft” methods</a></li>
<li class="chapter" data-level="15.1.3" data-path="cluster-analysis.html"><a href="cluster-analysis.html#applying-hierarchical-clusters"><i class="fa fa-check"></i><b>15.1.3</b> Applying hierarchical clusters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analytics with KNIME and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cluster-analysis" class="section level1" number="15">
<h1><span class="header-section-number">Chapter 15</span> Cluster analysis</h1>
<p>Cluster analysis is an unsupervised set of methods for identifying groups of observations according to a measure of proximity, which could mean either similarity or distance. Clustering can be used to find groups of objects (records, people, items, documents) such that the objects within each group are similar in some sense to one another and distinct from the objects in other groups. The goal is to create within group homogeneity and between group heterogeneity.</p>
<p>For example, if three clusters, denoted A with 10 objects, B with 20 objects, and C with 15 objects, are formed from a set of 45 observations, then the 10 objects in cluster A should be similar to one another according to some criterion. The same should be true of clusters B and C. However, it also should be the cases that the observations in cluster A are quite distinct (using the same criterion) from those in cluster B. This should likewise be the cases when considering clusters A and C, and clusters B and C.</p>
<p>Cluster analysis is used in many scientific and applied fields since the goal of many studies is to simplify, condense, or classify situations – to take many data points and somehow extract the essential groups or segments from a large number of observations. [^Cluster analysis goes by different names in various disciplines and includes numerical taxonomy, pattern recognition, typology, and clumping.]</p>
<p>No assumptions are needed made a priori regarding the number of groups or their structure; the goal is usually one of discovery. In some cases the objective is to find “natural groups” within a data structure. Discovering natural groups is not always easy because of the huge number of possible ways that groups can be formed.</p>
<p>One approach might be to investigate all possible groupings and then decide (using some criterion) which is the “best” approach. This would be computationally unfeasible. For example, consider the 16 playing cards Figure <a href="cluster-analysis.html#fig:sixteencards">15.1</a>. The number of possible clusters of size 2 from the set of 16 is 32,767. The number of possible clusters of size 3 from 16 objects is over 7 million.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sixteencards"></span>
<img src="images_clusterAnalysis/16cards.png" alt="16 playing cards." width="30%" />
<p class="caption">
Figure 15.1: 16 playing cards.
</p>
</div>
<p>To cluster objects, it is necessary to define what criterion is to be used to group the objects. Similarity or distance must be computed between (Case 1 and Case 2), (Case 1 and Case 3) and (Case 2 and Case 3). Euclidean distance is typically used to measure dissimilarity. Euclidean distance can be generalized to any number of dimensions. The distance measure provides a quantitative index of the similarity between records or objects. The most common measure of distance is the squared Euclidean distance. There are other measures, which will be discussed later.</p>
<div id="approaches-to-forming-clusters" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Approaches to forming clusters</h2>
<div id="hierarchical-versus-partitioning-methods" class="section level3" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> Hierarchical versus partitioning methods</h3>
<p>There are many different clustering algorithms. One distinction is between hierarchical and partitioning (or non-hierarchical) methods. With the hierarchical approach, clusters are formed sequentially and once a case is grouped with other cases into a cluster, that case is never separated again as the program proceeds. With partitioning methods a pre-set number of clusters is specified and the algorithim proceeds iteratively to assign observations to clusters that are similar within and different across.</p>
<div id="hierarchical-methods" class="section level4" number="15.1.1.1">
<h4><span class="header-section-number">15.1.1.1</span> Hierarchical methods</h4>
<p>A simple conceptual example of hierarchical clustering is shown in Figure <a href="cluster-analysis.html#fig:ConceptualDepictionOfHierarchicalClustering">15.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ConceptualDepictionOfHierarchicalClustering"></span>
<img src="images_clusterAnalysis/ConceptualDepictionOfHierarchicalClustering.PNG" alt="The hierarchical clustering process." width="60%" />
<p class="caption">
Figure 15.2: The hierarchical clustering process.
</p>
</div>
<p>Nine objects (A through I) are assumed to have a set of characteristics (not shown) that are used to compute similarity. [^The type of hierarchical method described here is called the agglomerative approach, since the algorithm proceeds from all observations in individual clusters to all observations in a single cluster. Divisive methods are also available that work in the opposite way, with all objects in a single cluster.] At the start, all nine objects are in separate clusters so there are 36 [9X(9-1)/2] similarity measures. The algorithm ranks the 36 values from most similar to least similar and the two objects most similar (C and D) are joined into a single cluster. This results in eight clusters and the 28 [8X(8-1)/2] similarities among the eight are computed and again ranked from most to least similar. This time, note that there are various ways to compute similarities between clusters with just a single object and those with more than one. This complication will be discussed later. The process of continues until all nine objects are grouped into a single cluster.</p>
The hierarchical nature of this approach is evident from the fact that a cluster is formed, the algorithm does not go back at a later stage to check if that cluster is still optimum. Another way of looking at the hierarchical nature of the clustering process for this hypothetical example is with a <em>dendrogram</em>. (Figure <a href="cluster-analysis.html#fig:Dendrogram">15.3</a>) The closest objects are shown with the shortest links, with the sequence shown by the tree-like structure.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Dendrogram"></span>
<img src="images_clusterAnalysis/Dendrogram.PNG" alt="Example of a dendrogram." width="40%" />
<p class="caption">
Figure 15.3: Example of a dendrogram.
</p>
</div>
</div>
<div id="partitioning-methods" class="section level4" number="15.1.1.2">
<h4><span class="header-section-number">15.1.1.2</span> Partitioning methods</h4>
<p>The most popular partitioning algorithm is the widely available k–means method. The k-means procedure performs a disjoint (each observation is to one and only one cluster) analysis on the basis of Euclidean distances computed using k quantitative variables. The analyst must specify the number of clusters in advance.</p>
<p>k-means works like this:</p>
<ol style="list-style-type: decimal">
<li>Choose initial centroids randomly for specified number of clusters.</li>
<li>Assign each observation to nearest centroid based on Euclidean distance, forming a provisional set of clusters.</li>
<li>Compute new centroids based in the new cluster.<br />
</li>
<li>Reassign observations to nearest new centroids, again based on Euclidean distance.</li>
<li>Repeat until maximum number of iterations is reached or until the largest change in any cluster centroid is less than a prespecified minimum.</li>
</ol>
<p>The k-means method does not form a tree structure for different numbers of clusters as does hierarchical procedures. Objects grouped with one number of clusters will not necessarily be grouped with a different number of clusters. Typically, the program is run with several different settings for the number of clusters and the results evaluated for the most desirable result (discussed in detail later).</p>
<p>As an example, we will start off with the same nine objects as before. Then, the program is run four times with two to five clusters specified.</p>
<p>The two clusters are shown in the red boxes. Next we asked the program to find three clusters. On the next slide, a 3 cluster solution is shown in red boxes. The previous two-cluster solution is shown in blue boxes. Notice that the structure has changed and the three cluster solution is not a subset or superset of the two cluster solution. Next we can obtain a 4 cluster solution. We can continue this this far as we like. But notice that the clusters are not formed in a hierarchical manner. The advantage of this approach is that the program doesn’t get locked in to an early structure that might not be optimal.</p>
</div>
</div>
<div id="hard-versus-soft-methods" class="section level3" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> “Hard” versus “soft” methods</h3>
<p>Another distinction in clustering methods is between “hard clustering” where each observation belongs to a single cluster and “soft” (or “fuzzy”) clustering where observations can belong to more than one cluster <span class="citation">(<a href="#ref-HarmouchMahmoud" role="doc-biblioref">Harmouch, n.d.</a>)</span>. A typical hard clustering method is K-means.</p>
<p>Table <a href="cluster-analysis.html#tab:BeerBrands">15.1</a></p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:BeerBrands">Table 15.1: </span>Beer brands data set
</caption>
<thead>
<tr>
<th style="text-align:left;">
Beer brands
</th>
<th style="text-align:center;">
Calories
</th>
<th style="text-align:center;">
Alcohol
</th>
<th style="text-align:center;">
Cost
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Budweiser
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
144
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.7
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Lowenbrau
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
157
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.9
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.48
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Michelob
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
162
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
5.0
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.50
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Kronenbourg
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
170
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
5.2
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.73
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Heineken
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
152
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
5.0
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.77
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Schmidt’s
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
147
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.7
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.30
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Pabst Blue Ribbon
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
152
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.9
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Miller Lite
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
99
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.3
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Bud Light
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
113
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
3.7
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.44
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Coors Light
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
102
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.1
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.46
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Dos Equis
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
145
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.5
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.70
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
BecKs
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
150
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.7
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.76
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Rolling Rock
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
144
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.7
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.36
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Pabst Extra Light
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
66
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
2.3
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Tuborg
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
155
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
5.0
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Olympia Gold Light
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
72
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
2.9
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.46
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Schlitz Light
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
97
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
4.2
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.47
</td>
</tr>
</tbody>
</table>
<p>Table <a href="cluster-analysis.html#tab:CorrelationMatrixBeerData">15.2</a></p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:CorrelationMatrixBeerData">Table 15.2: </span>Correlation matrix for beer brands data set
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variables
</th>
<th style="text-align:center;">
Calories
</th>
<th style="text-align:center;">
Alcohol
</th>
<th style="text-align:center;">
Cost
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Calories
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
1.000
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.923
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.360
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Alcohol
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.923
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
1.000
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.319
</td>
</tr>
<tr>
<td style="text-align:left;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
Cost
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.360
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
0.319
</td>
<td style="text-align:center;width: 7em; width: 4em; color: black !important;background-color: white !important;padding: 2px;">
1.000
</td>
</tr>
</tbody>
</table>
<p>Table <a href="cluster-analysis.html#tab:EigenvaluesBeerData">15.3</a></p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:EigenvaluesBeerData">Table 15.3: </span>Eigenvalues for beer brands data set
</caption>
<thead>
<tr>
<th style="text-align:center;">
Eigenvalue
</th>
<th style="text-align:center;">
PCT of variance
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
2.127
</td>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
70.9%
</td>
</tr>
<tr>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
0.797
</td>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
97.5%
</td>
</tr>
<tr>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
0.076
</td>
<td style="text-align:center;width: 7em; color: black !important;background-color: white !important;padding: 2px;">
100.0%
</td>
</tr>
</tbody>
</table>
</div>
<div id="applying-hierarchical-clusters" class="section level3" number="15.1.3">
<h3><span class="header-section-number">15.1.3</span> Applying hierarchical clusters</h3>
<p>Hierarchical clustering is not a single technique but rather a set of related techniques. Some software tools limit the choices to a small number of choices while others offer an almost bewildering list of different approaches. (KNIME is intermediate in the number of choices provided, but by using an R or Python node, many other types of hierarchical clustering can be used.)</p>
<p>Some of the questions facing analysts using hierarchical clustering in different software tools include: <span class="citation">(<a href="#ref-Everitt2011" role="doc-biblioref">Everitt 2011</a>)</span></p>
<ol style="list-style-type: decimal">
<li>Should agglomerative or divisive measures be used?</li>
<li>What measure of distance (or similarity) be used?
<ul>
<li>Alternatives include Euclidean, cosine, Minkowski, Manhattan, Jaccard, and Tanimoto.</li>
<li>Should the variables be weighted so that some variables have more influence on determining clusters or should weights be chosen so that all variables have equal influence?</li>
</ul></li>
<li>What measure of inter-cluster distance should be used?
<ul>
<li>Alternatives include single linkage, complete linkage, centroid linkage, average linkage, and Ward’s method.</li>
</ul></li>
</ol>
<p>It is frequently recommended that variables should be standardized prior to computing distances. This is usually a good idea, but “…there is no substitute for careful thought in the context of each individual problem. Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering
algorithm. This aspect of the problem is emphasized less in the clustering literature than the algorithms themselves, since it depends on domain knowledge specifics and is less amenable to general research.” <span class="citation">(<a href="#ref-Hastie2009" role="doc-biblioref">Hastie 2009</a>)</span></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Everitt2011" class="csl-entry">
Everitt, Sabine Landau, Brian S. 2011. <em>Cluster Analysis</em>. John Wiley.
</div>
<div id="ref-HarmouchMahmoud" class="csl-entry">
Harmouch, Mahmoud. n.d. <span>“17 Clustering Algorithms Used in Data Science and Mining.”</span> https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a.
</div>
<div id="ref-Hastie2009" class="csl-entry">
Hastie, Robert Tibshirani Jerome Friedman, Trevor. 2009. <em>The Elements of Statistical Learning</em>. Springer International.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TextbookDraft.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
