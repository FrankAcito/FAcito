[{"path":"index.html","id":"cover-page","chapter":"Cover page","heading":"Cover page","text":"\ndraft online book analytics using KNIME. R also used supplement KNIME specialized tasks.point, book extensively reviewed provided . Revisions progress.Frank Acito","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Two developments past years created great opportunities use analytics support enhance decision making areas business, health care medicine, government, --profit organizations four trends converged create emerging disciplines labels data science analytics. three trends :explosion amount, variety, velocity data.Data driver strategy.\nGreater management sophistication (expectations) regarding use data support decision making.Better, faster, cheaper hardware software.recent years terms business analytics, predictive analytics, data mining, machine learning, big data, others used describe various aspects dealing data.1 first many terms thought represent re-branding discipline statistics. statistics part story.\nFigure 1.1: Google search trends analytics-related terms\ny-axis Google trends) ranges 0 100, 100 representing peak number searches particular term, relative “popularity” terms shown.. 2004, terms “data mining” “business intelligence” peaks terms searches. 2020 “machine learning,” “data science,” “big data” tended upward. related terms specific definitions, easy become confused area. likely existence many terms partly function newness area.emergence nearly 250 graduate programs Analytics, Business Analytics, Data Science since 2007 another indicator popularity field. [^Source: analytics.ncsu.edu]","code":""},{"path":"intro.html","id":"what-is-analytics","chapter":"1 Introduction","heading":"1.1 What is analytics?","text":"term analytics defined many ways. interesting paragraph Gartner’s Information Technology Glossary provides useful perspective.Analytics emerged catch-term variety different business intelligence (BI)- application-related initiatives. , process analyzing information particular domain, website analytics. others, applying breadth BI capabilities specific content area (example, sales, service, supply chain). particular, BI vendors use “analytics” moniker differentiate products competition. Increasingly, “analytics” used describe statistical mathematical data analysis clusters, segments, scores predicts scenarios likely happen. Whatever use cases, “analytics” moved deeper business vernacular. Analytics garnered burgeoning interest business professionals looking exploit huge mounds internally generated externally available data. (Gartner, n.d.)","code":""},{"path":"intro.html","id":"some-trends-in-analytics","chapter":"1 Introduction","heading":"1.2 Some trends in analytics","text":"","code":""},{"path":"intro.html","id":"broadening-of-applicaiton-areas","chapter":"1 Introduction","heading":"1.2.1 Broadening of applicaiton areas","text":"first applications analytics focused management risk financial viewpoint. FICA scores credit Fair Isaacs Corporation began 1950s one first applications. Today applications opportunities virtually functional areas. example, emerging trend use analytics human resources applications developed deal turnover issues, benefits optimization management, succession planning, even training.","code":""},{"path":"intro.html","id":"generalization-of-the-notion-of-data","chapter":"1 Introduction","heading":"1.2.2 Generalization of the notion of data","text":"many years data analysis applied structured quantitative data. data clearly formatted defined fields. recent years advances made applying analytics unstructured data text, images, video, sounds. Examples include media data, surveillance data, weather data, . Internet Things refers use embedded sensors collect data physical objects - machines, automobiles, home appliances. sensors used collect data things provide safety performance monitoring, productivity improvement, guidance, functions.Today analytics includes marketing mix optimization, real time customer offer engines, predictive campaign analytics, web analytics, customer profitability management, profitability modeling optimization goes beyond activity- based costing allocate revenues enable resource allocations maximize profits.Related many new types data trend toward using external data combination internal data. Organizations routinely collected stored extensive information finances, production, employees, sales. combining internal data data governments, social media, mobile devices, many sources richer analyses better insights can formed.","code":""},{"path":"intro.html","id":"a-trend-from-slicing-and-dicing-data-to-more-advanced-techniques","chapter":"1 Introduction","heading":"1.2.3 A trend from “slicing and dicing” data to more advanced techniques","text":"trend reporting, slicing, dicing, drilling databases toward increased use data mining, machine learning, generally called advanced analytics. Advanced applications necessarily complicated, sophisticated, difficult – can . Applications can mapped continuum ranges queries data bases, pivoting, slicing dicing, drilling historical data. Moving one step right descriptive analytics, computing means, ranges, standard deviations, medians, correlations, summary statistics. Next data mining predictive modeling, includes techniques ranging multiple regression, logit analysis, neural nets, decision trees, discriminant analysis, support vector machines. techniques build models make predictions inferences future can help identify new patterns data hitherto unseen.Another step right simulation. Predictive models employed create alternate possible outcomes, perhaps thousands different ones Monte Carlo type random process. Using different sets assumptions scenarios, explorations possible future outcomes can performed.models complete enough data reliable enough, ultimate kind analytics can conducted optimization. models used determine best actions decisions according criterion (maximizing profit minimizing cost) subject constraints budgets, time, /resources. point models become prescriptive rather just predictive descriptive.","code":""},{"path":"intro.html","id":"more-advanced-data-visualization","chapter":"1 Introduction","heading":"1.2.4 More advanced data visualization","text":"Many software programs available can easily produce basic visual representations data bar charts, scatterplots, pie charts, line graphs. visualizations small sliver possible. Advanced data visualization concerned last link communication results – computer human mind. Techniques now used show dynamic content animations, allow real-time querying, alerts, combine multiple visualizations interactive dashboards.","code":""},{"path":"intro.html","id":"the-analytics-process-model","chapter":"1 Introduction","heading":"1.3 The analytics process model","text":"important conceptualize business analytics process. one-shot effort. business analytics process can represented series steps. steps conducted sequentially, almost always involves iteration, since results one step may require previous steps re-examined perhaps redefined. several process frameworks published including CRISP (Cross Industry Standard Process Data Mining) (“Crisp-DM User Guide,” n.d.), SEMMA (Sample, Explore, Modify, Model Assess) (“SEMMA SAS,” n.d.), KDD (“KDD Data Mining,” n.d.) (Knowledge Discovery Databases), Microsoft’s TDSP (“TDSP?” n.d.) (Team Data Science Process), others. CRISP framework updated since developed, remained popular model according KDnuggets survey 2014 CRISP. [“CRISP-DM, Still Top Methodology Analytics, Data Mining, Data Science Projects” (n.d.)} Many analysts developed custom models, CRISP framework remains viable approach, especially due emphasis business understanding first step project. (Figure 1.2)\nFigure 1.2: CRISP model.\nbook focuses process building analytics models. exploratory prescriptive models considered, book mainly concerned prediction. Descriptive models often created prelude constructing predictive model, since important describe data inform model building. Also, prescriptive models becoming common, usually necessary predictive model inform prescriptive recommendations. Furthermore, line predictive descriptive models frequently blurred comes deployment. Deploying predictive model production means output outputs predictive model become scores used support decision making. fact, predictive models alone, without deployment form, usually considered little value organizations.Despite widespread use CRISP model, propose revised modeling process. revised model (Figure 1.3) groups data preparation, exploratory analytics, feature engineering single step. Based experience creating models, seems logical group tasks since closely related usually involve iteration. added “communication” deployment step, since creating clear understanding model important achieving buy-management.. Finally, added step labeled “Performance monitoring,” since deployed model periodically evaluated make sure performing intended. performing well, back arrow signals process starts anew.practice various steps likely iterative, indicated dual arrows linking step. Results one step may requiring revisiting revising previous step even one step. (Figure 1.3)\nFigure 1.3: process based modifications CRISP model.\n","code":""},{"path":"business-understanding-and-problem-definition.html","id":"business-understanding-and-problem-definition","chapter":"2 Business understanding and problem definition","heading":"2 Business understanding and problem definition","text":"collecting data thinking technique use, critical understand business problem. may seem obvious business problem clearly stated, frequently reported biggest reason failures analytics projects poor understanding problem. perhaps goes without saying order define problem result adding value organization, must preceded good understanding business.elements phase analytics process include understanding business, identifying stakeholders, recognizing type problem, finally framing problem.\ntemptation take request client manager without careful thought go directly gathering analyzing data. path taken, likely important business question answered. causes wasted time resources.benefits careful effort understand business define problem include:work analyst becomes efficient fewer dead-ends encountered. Data collection efficient, likely focus getting right data ignoring variables relevant. Rather just getting data can found, directed search variables can occur. also provides stronger justification asking client addiontal data.work analyst becomes efficient fewer dead-ends encountered. Data collection efficient, likely focus getting right data ignoring variables relevant. Rather just getting data can found, directed search variables can occur. also provides stronger justification asking client addiontal data.Second, business analytics adding value. problem well structured results can implemented add value. right problem structure getting useful solution likely. Scattershot model building likely lead poor predictions , importantly, little guidance wrong improve model.Second, business analytics adding value. problem well structured results can implemented add value. right problem structure getting useful solution likely. Scattershot model building likely lead poor predictions , importantly, little guidance wrong improve model.Third, clearly defined problem can communicated everyone involved, two traps providing information already known information unusual unbelievable avoided.Third, clearly defined problem can communicated everyone involved, two traps providing information already known information unusual unbelievable avoided.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"expert-views","chapter":"2 Business understanding and problem definition","heading":"2.1 Expert views","text":"Business understanding defining problem among important tasks analytics projects. quotes variety experts attest importance phase analytics process.“difference great mediocre data science math engineering: asking right question(s)…. amount technical competence statistical rigor can make solved useless problem.” (Cady 2017)Einstein quoted said one hour save world spend fifty-five minutes defining problem five minutes finding solution.“serious mistakes made result wrong answers. truly dangerous thing asking wrong questions.”\nPeter Drucker“Successful analytic teams spend time understanding business problem less time wading lakes data.” (Taylor 2017)finding lot companies, great data scientists great business people missing business people know enough data analytics say, problem like help . can take outcome data scientists see can best leverage . must get next couple years want take advantage digital technologies. (Ittner 2019)following case study demonstrates importance delving deeper situation simply focusing approach asked client.2One common applications analytics help predict understand churn telecommunications providers. Subscribers cell phone plans notorious cancelling contracts signing another carrier. People churn unhappy service, want new phone, get good price another carrier. sorts reasons people leave. can expensive provider. usually far cheaper provider take steps keep customer go find new one.large cell phone carrier United States called consultant help problem. carrier experiencing considerable churn asked business analyst find way predict likely leave company determine might done prevent customer leaving. seemed like reasonable request one addressed analytics. cell phone company lots data customers history customers stayed left. consultant decided look problem digging data.consultant found churn occurring right around time two-year contract . asked managers company detailed explanation customers contacted, asked sign new contract, sort communications sent customers. turned two months prior customer’s contract , company sent reminder letter saying contract coming renewal asking customer sign another two years. letter sent customers.Imagine consumer cell phone company get letter company saying contract ends 60 days. behavior trigger? Well, many customers, got thinking shopping around. turned efforts get customers sign another contract actually motivating many leave company. Many people don’t know contracts , left alone, continue company years without switching. company’s marketing department triggering churn. letters stopped churn went .consultant just gone directly analyzing data, might totally missed root cause problem. fact solving wrong problem. point management wanted predictive model churn. consultant done , real solution found. Understanding real problem therefore extremely important.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"understanding-the-business","chapter":"2 Business understanding and problem definition","heading":"2.2 Understanding the business","text":"start, goal project can stated vague terms. important meet stakeholders (interest results project) refine objectives operational terms obtain buy-stakeholders. Instead, individuals, stakeholders, involved project considered. Stakeholders can include , executives customer department (marketing, accounting, finance, etc.), may expected use resulting model, might affected project.following tasks needed business understanding phase (adapted CRISP model ):Determine business objectives; , basic goal project? business situations might gain new customers, increase loyalty current customers, reduce lost due fraud.Translate business objectives technical metrics can result analysis. example, customer loyalty problem, goal analysis identify customers likely stop using company’s products services. objective might refined stating needed lead time prior customer loss remedial action taken.Develop project plan – list stages, required resources, risks, time duration, contingencies, evaluation metrics.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"identifying-stakeholders","chapter":"2 Business understanding and problem definition","heading":"2.3 Identifying stakeholders","text":"Stakeholders – people organization care problem - can part solution, can potentially derail solution, can provide resources needed arrive solution, can position take action results.questions ask stakeholders:executives stake outcome analysis?executives briefed problem solution approach?Can key executives provide necessary resources implement recommendations?key executives support analytic approaches making decisions? people vested interests particular solution?plan regular communication feedback interim results progress? key stakeholders certain styles using information making decisions?3 (Based (Davenport Kim 2013).)","code":""},{"path":"business-understanding-and-problem-definition.html","id":"structured-versus-unstructured-problems","chapter":"2 Business understanding and problem definition","heading":"2.4 Structured versus unstructured problems","text":"types problems addressed analytics wide-ranging:can Red Cross increase blood donations?consumer products company increase ad spending certain brand lower price stimulate sales?increase number people agree organ donors?can university increase student retention rates?much discount resort offer booking tickets online 6 months early?hotel offer “surprise discount” rooms time check-offer people leaving?can casino increase lifetime value customer?particular person approved loan?credit card transaction fraudulent?may seem specific questions, attempting structure question guide analytics project likely reveal questions well structured.Many challenging analytics problems unstructured difficult. Professionals rewarded ability solve difficult problems, simply following rote procedures relaying memorized information. faced unstructured problem, even important work carefully defining . following, based (Jonassen 1997), shows characteristics distinguish well-structured versus unstructured problems. (Figure2.1)\nFigure 2.1: Structured versus unstructured problems.\n","code":""},{"path":"business-understanding-and-problem-definition.html","id":"framing-the-problem","chapter":"2 Business understanding and problem definition","heading":"2.5 Framing the problem","text":"term “framing” means process describing interpreting situation: framing focuses attention. problem frame helps define importance problem also sets direction solving . Depending upon problem question framed can lead different answers. example, two questions ask number 10 different ways lead different answers:sum 5 plus 5?two numbers add 10?tentative analytics problem identified, different frames lead different analytics approaches. example, insurance company may want reduce cost fraudulent claims. Analytics used address problem, depending problem framed lead different target variables probably different analytic techniques. Consider following different goals associated reducing fraud:Identify cases highest propensity fraud?Identify current customers likely commit fraud near future?Identify customers apply policy likely commit fraud rejecting applicants?Identify cases highest likelihood recover monies?Identify case largest potential dollar amount fraud?Identify cases maximize hourly return investigators assigned deal fraudulent cases?importance problem framing illustrated old story (Shoemaker Russo 2001).old story Franciscan priest Jesuit priest heavy smokers somewhat troubled human fragility, especially smoking praying Lord.Franciscan met prefect asked, “Father, permitted smoke praying Lord?” answer resounding, “!”Jesuit also sought counsel, asked question differently. “Father, moments weakness smoke, permitted say prayer Lord?” answer , “Yes, course son.”cases, first attempt problem statement focuses symptoms, may address root problem. example, symptom might : “losing market share rapidly.” problem definition might : “regain market share?” (Hauch 2018) may direct analyst team real issue.One suggestion get root problem framing use “40-20-10-5 rule.” problems difficult define quickly, can apply rule goes 4 basic steps. State problem 40 words. Cut 20, 10 end 5 words problem statement. can keep simple, probably reached roots yet.Another data scientist summarized key considerations guide team problem framing exercise. (Arnuld 2020)See can solve problem without using ML. Sometimes simple heuristic good enough.clear want state without using ML. 3. Write desired outcome simple EnglishWhat expect model output?kind ML problem solving e.g., supervised kind: classification regression. classification kind? binary multi-class? etc.successful model case?failed model case? Write cases. “able succeed” failure case.Quantify success. measure model?Try keep model simple. simple model’s result can justify need complex model. Complex models slower train difficult understand.Build simple model deploy . biggest gain ML tends first launch. can improve later launch another versionExample solving simpler problemOften, engineers make things complicated necessary. Consider example NASA space pen. 1960s, NASA focused major program developing pen write zero gravity, Soviet space program used much simpler already-invented pencil.don’t know sure wasn’t , strongly suspect came miss problem statement. Americans working problem: get pen write zero gravity? Soviets working different problem: write zero gravity? seems like small, subtle difference, huge impact work followed. (Flinchbaugh 2009)","code":""},{"path":"business-understanding-and-problem-definition.html","id":"summary","chapter":"2 Business understanding and problem definition","heading":"2.6 Summary","text":"Preparing effective, actionable definition problem prior machine learning universally recognized important. However, specifying exactly create problem definition codified simple process. many complex concepts, likely “know see .” , getting point requires creativity hard work.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"appendix-some-tools-for-problem-definition","chapter":"2 Business understanding and problem definition","heading":"Appendix: Some tools for problem definition","text":"critical importance problem definition, many tools strategies developed get started. listed .","code":""},{"path":"business-understanding-and-problem-definition.html","id":"right-to-left-thinking","chapter":"2 Business understanding and problem definition","heading":"Right to left thinking","text":"One approach can helpful turn business objective decision made. example, problem company needs customers, lead goal getting new customers working keeping current customers. goals lead different data needs analyses. goal keeping current customers, model predicts specific customers likely leave can lead actions reduce probability customers leaving. hand, goal get new customers, model can used indicate potential individuals likely respond favorably offer.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"reversing-the-problem","chapter":"2 Business understanding and problem definition","heading":"Reversing the problem","text":"Clearly identify problem challenge, write . Brainstorm reverse problem generate reverse solution ideas. Allow brainstorm ideas flow freely. reject anything stage.Instead asking, “solve prevent problem?” ask, “possibly cause problem?” Instead asking “achieve results?” ask, “possibly achieve opposite effect?”brainstormed ideas solve reverse problem, now reverse solution ideas original problem challenge. Evaluate solution ideas determine potential solution suggested least attributes potential solution? (DeRusha WOlfson, n.d.)","code":""},{"path":"business-understanding-and-problem-definition.html","id":"open-the-problem-with-whys","chapter":"2 Business understanding and problem definition","heading":"Open the problem with “whys”","text":"asked build bridge , go build bridge. come back another question: “need bridge?” likely tell need bridge get side river. Aha! response opens frame possible solutions. clearly many ways get across river besides using bridge. dig tunnel, take ferry, paddle canoe, use zip line, fly hot-air balloon, name .can open frame even farther asking want get side river. Imagine told work side. , , provides valuable information broadens range possible solutions even . probably viable ways earn living without ever going across river.\nSource: (Seelig 2013)","code":""},{"path":"business-understanding-and-problem-definition.html","id":"challenge-assumptions","chapter":"2 Business understanding and problem definition","heading":"Challenge assumptions","text":"Every problem — matter apparently simple may — comes long list assumptions attached. Many assumptions may inaccurate make problem statement inadequate even misguided.first step get rid bad assumptions make explicit. Write list expose many assumptions can — especially may seem obvious ‘untouchable.’ , , brings clarity problem hand. Essentially, need learn think like philosopher.go test assumption validity: think ways might valid consequences. find may surprise : many bad assumptions self-imposed — just bit scrutiny able safely drop . skeptic. (Clissold 2021)Example challenging assumptionsConsider interesting example mathematician Abraham Wald British Air Ministry. World War II, British Air Ministry engaged Mr. Wald study damage suffered airplanes engaged combat missions use results recommend appropriate places armor reinforcement. returning planes assessed damage data collected analyzed. Patterns damage soon became apparent, officers Royal Air Force concluded planes reinforced based patterns. (Ellenberg 2016)Mr. Wald took different approach. reasoned damaged planes made back weren’t real problem. Additional armor needed planes didn’t make back. didn’t know certain damage lost planes, reason different planes safely returned. task see problem wasn’t easily observed.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"chunking","chapter":"2 Business understanding and problem definition","heading":"Chunking","text":"Chunk \nChunking taking broader view. Helicopter 30,000 feet. Survey landscape see whole system.\nAsk ‘’ things happen find higher-level purpose. Ask ‘instance ’ find general classification.\nUse inductive reasoning go specific detail general theories explanations.\nChunking taking broader view. Helicopter 30,000 feet. Survey landscape see whole system.\nAsk ‘’ things happen find higher-level purpose. Ask ‘instance ’ find general classification.\nUse inductive reasoning go specific detail general theories explanations.Chunk \nChunking going detail find smaller specific elements system.\nAsk ‘’ things happen find lower-level detail. Ask ‘, specifically’ probe information. Ask ‘Give example’ get specific instances class.\nUse deductive reasoning go general theories ideas specific cases instances.\nChunking going detail find smaller specific elements system.\nAsk ‘’ things happen find lower-level detail. Ask ‘, specifically’ probe information. Ask ‘Give example’ get specific instances class.\nUse deductive reasoning go general theories ideas specific cases instances.Chunk \nChunking go well together way looking differently situation.\nChunk existing situation find general broader view. chunk somewhere else.\nChunking go well together way looking differently situation.\nChunk existing situation find general broader view. chunk somewhere else.","code":""},{"path":"business-understanding-and-problem-definition.html","id":"problems","chapter":"2 Business understanding and problem definition","heading":"Problems","text":"Problem 1 Virtually data base systems today include capabilities retrieve records reate manipulate tables efficiently. Queries return data database fulfills specific constraints criteria set user. OLAP uses process preaggregation form data cubes, enables retrieval quickly, often less computational power. Neither database queries OLAP, however, considered data mining tools. Data mining addresses fundamentally different questions \nconstructing models data.Consider following business questions. Take minutes “twist” question analytic modeling question.\nbest customers?\ngeographic areas sales come ?\nlong customers stay company?\ncustomers buy, specifically?\nshare customer wallet ?\nsuppliers reliable? characteristics reliable suppliers?\nsalespeople provide profit?\nperceived quality products?\nproducts profitable?\nbest customers?geographic areas sales come ?long customers stay company?customers buy, specifically?share customer wallet ?suppliers reliable? characteristics reliable suppliers?salespeople provide profit?perceived quality products?products profitable?Problem 2 bank interested running major promotion increase share new car loan market. Assuming promotion successful, number applications new car loans increase substantially. Therefore, suggested predictive analytics model developed speed process loan acceptance rejection, specific goal quickly automatically identifying applicants likely default loan.database 5,500 automobile loans available bank’s information system, covering loans made July 1999 June 2010.Variables loan include:\nWhether loan resulted default\nAmount loan\nPercentage loan value automobile\nAge applicant\nGender applicant\nCurrent debt (monthly payments)\nMonthly household income\nYears current residence\nYears previous residence\nNumber dependents\nMarital status\nCredit score\nDate loan\nDuration loan months\nWhether loan resulted defaultAmount loanPercentage loan value automobileAge applicantGender applicantCurrent debt (monthly payments)Monthly household incomeYears current residenceYears previous residenceNumber dependentsMarital statusCredit scoreDate loanDuration loan monthsWhat issues questions regarding data set prior developing predictive model?","code":""},{"path":"introduction-to-knime.html","id":"introduction-to-knime","chapter":"3 Introduction to KNIME","heading":"3 Introduction to KNIME","text":"KNIME platform presented along links resources including tutorials documentation. step--step example using KNIME shown demonstrate KNIME can used analyze small data set.KNIME comprehensive tool analytics data mining uses intuitive drag drop workflow canvas. KNIME can () used professional data analysts, excellent “low code” platform learning predictive analytics data mining. KNIME workflows provide graphic representation steps taken analysis, makes analysis self-documenting. makes easy communicate analyses others see reproduce analyses.KNIME developed group University Konstanz Germany beginning 2004 first release 2006. noted web site, KNIME committed open source:“Unlike open-source products, KNIME Analytics Platform cut-version artificial limitations execution environment data size: enough local cloud-based space compute power, can run projects billions rows, many KNIME users currently .”\nref: https://www.knime.com/knime-open-source-storyThere commercial server version KNIME needed deploying KNIME web, needed learning. Everything else available open-source version.several features make KNIME stand competitors:KNIME free use machine.KNIME runs Windows, Mac, Linux machines.KNIME 4,000 nodes data source connections, transformations, machine learning, visualization.KNIME includes broad array data processing analysis capabilities, fully extensible creating custom nodes using Python R.Many capabilities two analytic platforms, H2O WEKA, also integrated KNIME work seamlessly drag drop workflow.variety data file types can used including csv, Excel, , using easily installed extension, databases.Data can exported Excel, Tableau, Spotfire, Power BI, reporting platforms.large active community users can answer questions provide help.Many ready--use workflows available can easily installed work environment dragging dropping KNIME site.Extensive documentation, learning modules, videos, training events available.","code":""},{"path":"introduction-to-knime.html","id":"the-knime-workbench","chapter":"3 Introduction to KNIME","heading":"3.1 The KNIME Workbench","text":"\nFigure 3.1: KNIME Workbench\n","code":""},{"path":"introduction-to-knime.html","id":"elements-of-the-knime-workbench","chapter":"3 Introduction to KNIME","heading":"3.1.1 Elements of the KNIME Workbench","text":"KNIME Explorer provides links available workflows machine well available KNIME servers including workflow examples KNIME KNIME community.Workflow Coach handy tool help build analytic workflow. node selected workflow, suggested next nodes listed. suggestions based usage statistics KNIME Community.Node Repository Listed area analysis nodes installed machine. Nodes available read write files, explore transform data, run basic advanced analytics, create visualizations. core set nodes included install KNIME, thousands additional nodes available easily installed. nodes organized categories, can also use search box top node repository find nodes.Workflow Editor canvas editing currently active workflow. workflow created dragging nodes Node Repository linking appropriately.Outline area shows small overview current workflow.Console shows processing taking place executing workflow. also provides warnings error messages.Node Description node selected workflow, detailed description provided including general function node, available settings, input /output ports.","code":""},{"path":"introduction-to-knime.html","id":"learning-to-use-knime","chapter":"3 Introduction to KNIME","heading":"3.2 Learning to use KNIME","text":"Learning use KNIME takes time, extensive, free written video resources available. “Getting Started Guide” available Getting started KNIMEA series self-paced courses online free. courses include exercises solutions. KNIME Self-Paced Courses four levels self-paced courses:Level 1 courses\nKNIME Analytics Platform Data Scientists: Basics\nKNIME Analytics Platform Data Wranglers: Basics\nKNIME Analytics Platform Data Scientists: BasicsKNIME Analytics Platform Data Wranglers: BasicsLevel 2 courses\nKNIME Analytics Platform Data Scientists: Advanced\nKNIME Analytics Platform Data Wranglers: Advanced\nKNIME Analytics Platform Data Scientists: AdvancedKNIME Analytics Platform Data Wranglers: AdvancedLevel 3 course\nKNIME Server Course: Productionizing Collaboration\nKNIME Server Course: Productionizing CollaborationLevel 4 courses\nIntroduction Big Data KNIME Analytics Platform\nIntroduction Text Processing\nIntroduction Big Data KNIME Analytics PlatformIntroduction Text ProcessingThis link documentation can read downloaded. KNIME DocumentationIndividual documents specific topics available follows:KNIME QuickStart GuideKNIME Analytics Platform Installation GuideKNIME Workbench GuideExtensions Integrations GuideKNIME Flow Control GuideKNIME Components GuideKNIME Integrated Deployment GuideKNIME File Handling GuideA free “bootcamp” KNIME available UDEMY 50 video instructional lectures running four hours. course starts installation setup proceeds demonstrate practical applications machine learning. Bootcamp KNIME Analytics Platform","code":""},{"path":"introduction-to-knime.html","id":"knime-workflow-example-1-predicting-heart-disease","chapter":"3 Introduction to KNIME","heading":"3.3 KNIME workflow example #1: Predicting heart disease","text":"following example included illustrate KNIME can used build model. concerned details unfamiliar point; details covered later chapters book.example involves cardiovascular disease data set. Mortality rates cardiovascular diseases high increasing, especially developing regions world. United States 1 4 deaths year heart disease. presence heart disease can predicted using physician’s examination laboratory tests, valuable diagnostic tool. prediction difficult several contributory variables, including diabetes, high blood pressure, high cholesterol, abnormal pulse rate, factors.data set Cleveland Clinic available UCI Machine Learning Repository (Aha 1988). data set 2974 rows, 13 predictor values, 1 target (heart disease versus heart disease). Since target variable just two levels, logistic regression run KNIME. (Details logistic regression models provided later chapters.) step--step illustration running KNIME shown next six figures.first rows data set shown Figure 3.2. KNIME node “CSV Reader” used input data. (Many data formats can read KNIME well.)\nFigure 3.2: Read data .CSV file.\nGood practice building machine learning models create model using part data test model using separate section data. Accordingly, KNIME node “Partitioning” used create training test data subsets. 70% sample used training 30% testing. (Figure 3.3) 70/30 ratio typical, split can used.data split randomly; make splitting repeatable, seed set (arbitrarily “123”). Also, random selection done stratified manner ratio “heart disease” versus “heart disease” equal (nearly equal) training test subsets.Partitioning node two outputs: upper output training data lower output test data subset.\nFigure 3.3: Create training test data sets\nPerforming logistic regression KNIME involves two nodes. first node, shown Figure 3.4, “learns” logistic model. Note upper output Partitioning node (training data) used build model. three outputs Logistic Regression Learning: upper output (shown black square) contains model ; middle output arrow contains coefficients model; lower output arrow information modeling process (used example).\nFigure 3.4: Run logistic regression training data\nLogistic Regression Predictor (Figure 3.5) two inputs: square input takes model built Learner node. lower arrow input takes test data Partitioning node. Logistic Regression Predictor uses inputs create predictions test data output single arrow right node.\nFigure 3.5: Predict ‘heart disease’ using logistic regression model test data.\nScorer node (Figure 3.6)KNIME designed evaluate performance models target variable binary, case example. input node data table actual predicted values test data set. Using data, cross-tabulation called “confusion matrix” can derived.\nFigure 3.6: Score logistic regression results test data.\n2 X 2 confusion matrix shown Figure 3.7. table shows accurate inaccurate predictions. example, prediction accuracy 80%, sufficient clinical diagnoses. Perhaps different model data developed improve accuracy. Especially concern results shown confusion matrix 11 cases heart disease “present,” model predicted “present.” Adjustments prediction mechanism made err side predicting “present” fact “present,” since arguably less serious type error.\nFigure 3.7: confusion matrix accuracy measures predicting ‘heart disease’\nAdditional performance measures also provided Scorer node discussed chapter model evaluation.","code":""},{"path":"introduction-to-knime.html","id":"knime-workflow-example-2-preparation-of-hospital-data","chapter":"3 Introduction to KNIME","heading":"3.4 KNIME workflow example #2: Preparation of hospital data","text":"length stay patients hospitals important indicator efficiency hospital management. Extended stays can increase likelihood infection complications. Long stays hospital negatively impact patient’s experience result higher health care costs.example uses 2015 data set 2.3 million patient hospitalizations New York State. Thirty-four variables included data set, analysis focus following:Age Group (0 17, 18 29, 30 49, 50 69, 70 Older)Gender (M,F, U)Length Stay (days)Type admission (Elective, Urgent)Severity illness( Minor, Moderate, Major, Extreme)Total Charges (amount billed hospital)Total Cost (expense incurred hospital)steps taken preparing data shown Table 3.1.\nTable 3.1: Nodes preparing hospital data\ntable Data Explorer node continuous variables shown \nFigure 3.8 table nominal variables shown Figure 3.9.\nFigure 3.8: Summary continuous variables hospital data subset\n\nFigure 3.9: Summary continuous variables hospital data subset\n","code":""},{"path":"introduction-to-knime.html","id":"summary-1","chapter":"3 Introduction to KNIME","heading":"3.5 Summary","text":"KNIME main platform used throughout text analyses. important download install KNIME prior continuing next chapters become familiar KNIME working problems.","code":""},{"path":"introduction-to-knime.html","id":"problems-1","chapter":"3 Introduction to KNIME","heading":"Problems","text":"Download data set sales.csv.Open KNIME create new workflow named Problem_KNIME_1.Drag drop sales.csv KNIME workflow.Right click CSV Reader select Transformation list top.Click OK.Right click CSV Reader node select Execute.Right click output arrow CSV Reader select File Table examine file.\nNode Repository, search Column Filter node.Drag Column Filter node workflow connect input port output port CSV Reader.Right click Column Filter choose Configure.Drag variables Exclude area using << icon. drag Country, date, amount Include area.Click OK.Execute Column Filter node.Right click output arrow Column Filter select Filtered Table examine file far.Node Repository, search Row Filter node.Drag Row Filter node workflow connect input port output port Column Filter.Right click Row Filter choose Configure.Column test area select Country using drop .Make sure “use pattern matching” selected type unknown space.right side configuration dialog, select Exclude rows attribute value. remove observation country unknown.Click OK.Execute Row Filter.Node Repository, search Data Explorer node.Drag Data Explorer node workflow connect input port output port Row Filter. (Leave configuration default.)Execute Data Explorer. Note date variable omitted Data Explorer.Right click Data Explorer select Interactive View: Data Explorer View. (may take seconds process.)variable amount appears Numeric page. Click plus sign red circle obtain histogram amount.Questions:\nQ1.1 many variables sales_data.csv file?\nQ1.2 many observations file?\nQ1.3 Skewness value variable amount consistent appearance histogram? Explain.Problem 2 problem builds KNIME workflow example preparing hospital data. data set produced example available file SubsetOfHospitalData.csv. Create new workflow named Problem_KNIME_2. Read data file answer following questions.","code":""},{"path":"data-preparation.html","id":"data-preparation","chapter":"4 Data preparation","heading":"4 Data preparation","text":"","code":""},{"path":"data-preparation.html","id":"introduction","chapter":"4 Data preparation","heading":"4.1 Introduction","text":"Data preparation includes processes obtaining data, cleaning , feature engineering, exploratory analysis. Data analysts report 60 90 percent time project often consumed processes.many possible ways perform data preparation processes many steps might taken data ready analysis. important document whatever steps taken join, clean, transform, otherwise prepare data reference can made new data analyzed. *[common problem working spreadsheets making several modifications data, difficult reproduce steps unless somehow recorded. added benefit using KNIME since workflows provide graphical record steps taken.]","code":""},{"path":"data-preparation.html","id":"obtaining-the-needed-data","chapter":"4 Data preparation","heading":"4.2 Obtaining the needed data","text":"One aspect analytics process surprising many newcomers company’s organization’s data warehouse provide data needed correct form. data must assembled, cleaned custom fitted analytics problem.5 process efficient effective analyst good knowledge business domain expert project team.Data company’s internal systems may need integrated census data. weather data. traffic data. calendar data indicating holidays weekends, etc. Data base tools joins involve linking records two data bases matching known key field.External data web, government, commercial sources may needed integrated usable structure. predictive data mining software requires data analyzed well-formed tabular structure flat file. Variables columns observations rows. data source variables rows observations columns, data table must transposed.also important proper type data available. example, predictive model developed, necessary predictors outcome variables. binary outcome predicted, must enough instances binary outcomes. example, predictive model identify fraud requires observations fraudulent fraudulent.data assembled one may find going appropriate sufficient address problem identified first step. , business problem might revised new ways explored obtain required data.","code":""},{"path":"data-preparation.html","id":"data-cleaning","chapter":"4 Data preparation","heading":"4.3 Data cleaning","text":"obtaining data might include inaccurate, incomplete, inconsistent values; might duplicate records, spelling errors, simply errors original data capture. Trying analyze data errors bound lead incorrect misleading results. Data cleaning (aka data scrubbing data cleansing) prior model building extremely important. simple deleting records apparent records. Instead, important maximize accuracy deleting information. goal make sure data set submitted modeling process highest quality. steps data cleaning process include:Removing unneeded columns (variables) /rows.\nRemoving duplicate observations.\nRemoving variables constant values observations.\nRemoving duplicate observations.Removing variables constant values observations.Removing data scope /within required time frame.Finding correcting impossible non-sensible values logical constraints can specified.\nChecking conditions across variable must satisfied. (example, age years equal current year minus birth year, population density equal total population divided area, .)\nChecking conditions across variable must satisfied. (example, age years equal current year minus birth year, population density equal total population divided area, .)Removing duplicate observations.Removing variables constant values observations.Identifying dealing outliers.Identifying dealing missing values.Finding misspellings, --range, impossible values.\nChecking make sure range variable correct. (example, ensuring dates, geographic regions, variables fall proper range.)\nChecking make sure range variable correct. (example, ensuring dates, geographic regions, variables fall proper range.)Making sure formats units measurement consistent.\nvariables measured way time data definitions changed?\nvariables different data sets available level granularity?\nvariables measured way time data definitions changed?variables different data sets available level granularity?ensuring regular expressions phone numbers, zip codes, social security numbers, etc. consistent correct format.imposing list may additional steps might needed. Also, issues may relevant particular project. simple process follow since depends specific variables data.KNIME provides several nodes can used cleaning data. (Figure 4.1)\nFigure 4.1: KNIME nodes useful data cleaning.\n","code":""},{"path":"data-preparation.html","id":"missing-values","chapter":"4 Data preparation","heading":"4.3.1 Missing values","text":"presence missing values one difficult problems analytics. ideal situation valid values every observation. However, likely missing values present variables real data sets.first thing try find value missing information. great values found, practice unlikely.Missing values can indicated variety ways data set, analysts need look indicators. ways missing values indicated data sets include following:Null\n99, 999, -999, etc.\n-1\n?\n.\nNA\nNoneIt important recognize original values coded recode values common indicator. especially true numeric variable, missing value code distort statistics mean variance.Missing values occur many reasons. Missing values generated due structural reasons. example, variable might ask person homeowner . yes, question might ask approximate value home. , value home missing, .survey research, common respondents refuse answer questions income. Respondents may simply know answer question, resulting missing value.","code":""},{"path":"data-preparation.html","id":"types-of-missing-values","chapter":"4 Data preparation","heading":"4.3.1.0.1 Types of missing values","text":"classification scheme missing values developed (Rubin 1976) stipulated three types missing values:MCAR - Missing completely random.MAR - Missing random.MNAR - missing random.MCAR favorable situation, since means values missing relation either target predictor values. observations missing values similar distributions without missing values. missing values rows data set reflect random errors data collection.MAR means likely systematic differences observations versus without missing values. assumption related observed predictor variables. example, missing values income might relate income age, higher income older likely omit income.MNAR refers situations likelihood value missing related variables instead related value . example, data set credit worthiness, question personal bankruptcies may left blank persons declare bankruptcy. pernicious type missingness. difficult determine whether missing values MNAR versus MAR MCAR. Domain knowledge can helpful identify MNAR present.","code":""},{"path":"data-preparation.html","id":"handling-missing-values","chapter":"4 Data preparation","heading":"4.3.1.1 Handling missing values","text":"proportion observations missing information small large data set (e.g., less 1%), observations can probably deleted. known listwise deletion. dropping observations, however, important check whether missing values concentrated just couple variables many observations least one missing value. latter case, possible, even small percentage missing values, data set million observations reduced thousands using listwise deletion.Another approach consider delete columns large percentages missing data. example, 75% observations particular variable missing, column might deleted. , course, done situation deemed NMAR.","code":""},{"path":"data-preparation.html","id":"imputation-methods","chapter":"4 Data preparation","heading":"4.3.1.2 Imputation methods","text":"Imputation refers substitution value replace missing values data set. (MNAR suspected, imputation techniques discussed following used since results analyses can biased misleading.) many ways :numeric variables:\nSubstitute constant value missing values numeric variables.\nSubstitute missing values mean median non-missing values.\nRegress non-missing values variable variables data set use regression equation predict likely values observations missing values.\nUse k nearest neighbors find observations similar values non-missing variables use observations impute missing values, example, taking average non-missing values k nearest neighbors.\nSubstitute constant value missing values numeric variables.Substitute missing values mean median non-missing values.Regress non-missing values variable variables data set use regression equation predict likely values observations missing values.Use k nearest neighbors find observations similar values non-missing variables use observations impute missing values, example, taking average non-missing values k nearest neighbors.categorical variables:\nSubstitute missing values mode values non-missing values.\nExplicitly consider missing values simply another level categorical variable.\nUse model predict category level missing values non-missing observations. One technique consider decision trees.\nSubstitute missing values mode values non-missing values.Explicitly consider missing values simply another level categorical variable.Use model predict category level missing values non-missing observations. One technique consider decision trees.Missing values variable data set can identified using KNIME Statistics node. KNIME node “Missing Value” handles missing values numeric string variables. node can replace numeric string variables manner type imputation can set differently variable. approach handling missing values depends type variable. following options available “Missing Value” node numeric variables:type variable missing values:\nnothing.\nRemove row missing value.\nImpute fixed value – replace missing values user specified constant strings numeric variables.\nnothing.Remove row missing value.Impute fixed value – replace missing values user specified constant strings numeric variables.numeric variables missing values:\nReplace values summary statistic – maximum, minimum, mean, median, rounded mean.\nReplace values summary statistic – maximum, minimum, mean, median, rounded mean.time series data missing values:\nLinear Interpolation – estimate missing value via straight-line trend either increasing decreasing.\nAverage Interpolation – take average number previous next non-missing values.\nMoving average – calculate mean specified number look-ahead look-behind values.\nLinear Interpolation – estimate missing value via straight-line trend either increasing decreasing.Average Interpolation – take average number previous next non-missing values.Moving average – calculate mean specified number look-ahead look-behind values.string nominal variables:\nfrequent value – replace missing values mode variable.\nfrequent value – replace missing values mode variable.Additional methods missing value imputation available R :MICE (Multivariate Imputation Chained Equations) (mice 2021)Amelia (Program Missing Data) (Amelia 2021)VIM (Visualization Imputation Missing Values) (VIM 2021)mlr (Machine learning R) (mlr 2021)","code":""},{"path":"data-preparation.html","id":"outliers","chapter":"4 Data preparation","heading":"4.3.2 Outliers","text":"Outliers defined values variable “abnormal distance” values data set - extremely high extremely low. Whether value outlier always easy say may depend domain knowledge. Extreme outliers can cause problems many algorithms can distort supervised unsupervised models. many graphical statistical techniques can used detect outliers.Outliers sometimes due data entry errors, e.g., extra zero two added value mistake. cases, pattern outlying values can identified may suggest deal .general, good idea simply remove outliers without investigation. sure outlier error, observation can removed fraction cases outliers small.Graphical approach detecting outliersStatistical methodsOne statistical method standardize variable subtracting mean dividing standard deviation. , assuming normality, values outside plus minus two three standard deviations can evaluated possible outliers.Another popular approach calculate Tukey’s upper lower fences shown graphically histograms (Tukey 1977). formulas based first (Q1) third (Q3) quartiles. Using formula, k can set analyst. advantage calculating fences rather using histograms analyst can define outliers setting k. deviations normality, k typically set 1.5. extreme outliers, value k can set 3.0Lower fence = Q1 - k(Q3 - Q1)\nUpper fence = Q3 + k(Q3 - Q1)KNIME node Numeric Outliers can used detect potentially deal outliers. node allows user specify k. identify extreme outliers typical practice set k equal 3.advantage calculating fences rather using histograms analyst can define outliers setting k. value k depends purpose test. deviations normality, k typically set 1.5, although others tests normality available normality predictor values required.Multivariate outliersTo point discussion focused univariate outliers. Even univariate outliers found, possible outliers present can detected using multivariate methods.example, following scatterplot shows outlier red.Box plots created X Y variables. outliers identified.identify multivariate outliers, Mahalanobis Distance can used. detailed discussion (Cansiz 2020). article uses R. KNIME component called “outliers 1 column” detects outliers using Mahalanobis distance requires Python integration. R code snippet can developed Mahalanobis distance using R package mvoutlier (mvoutlier 2021).","code":""},{"path":"data-preparation.html","id":"handling-outliers","chapter":"4 Data preparation","heading":"4.3.2.1 Handling outliers","text":"Identifying potential outliers variable usually straightforward, deciding outliers simple. Outlying values legitimate correct, routinely deleting observations recommended practice. fact, cases detecting outliers actually object analysis applications intrusion detection computer systems, determining fraud financial activity, detecting unusual patterns medical data, areas. (Aggarwal 2017)KNIME node Number Outliers detects provides several options treat identified outliers. treatment options :Remove row containing outlier (although done.Replace outlier missing value indicator.Replace outlier closest value within permitted interval.Sometimes log transformation reduce variability predictor lessen remove outliers. Math node KNIME can used create transformations.Another approach bin continuous variable dividing groups. creates ordinal variable values bin can controlled. KNIME node Numeric Binner can used define create bins. numeric variate transformed string column. course, detail lost binning, frequently useful technique.","code":""},{"path":"data-preparation.html","id":"feature-engineering","chapter":"4 Data preparation","heading":"4.4 Feature engineering","text":"Feature engineering refers steps taken improve accuracy predictive models though data transformations, constructions new variables available variables, acquiring additional variables enhance model performance. Feature engineering, despite technical connotation really art science learned experience. However, still useful suggestions starting process.Features independent variables basically thing. Independent variables (related terms predictors, inputs, ) used statisticians term features prevalent context machine learning.6","code":""},{"path":"data-preparation.html","id":"data-transformations","chapter":"4 Data preparation","heading":"4.4.1 Data transformations","text":"identify need transforming predictor variables, descriptive statistics data visualizations important tools. Histograms, box plots, scatter plots, basic visualization techniques can routinely applied numeric predictors.","code":""},{"path":"data-preparation.html","id":"handling-skewed-data","chapter":"4 Data preparation","heading":"4.4.1.1 Handling skewed data","text":"outliers, missing values, anomalies dealt , distributions variables checked extreme skewness. predictor values machine learning models perfectly symmetrical, skewed variables can distort bias model performance. models can overly sensitive tails variable distributions.Detecting skewness usually straightforward using histograms. Skewness can reduced using transformations:positive left skew, try following variable x:\nlog(k + x)\nsqrt(k + x)\nk large enough positive number insure log square root applied numbers > 0.\nlog(k + x)sqrt(k + x)k large enough positive number insure log square root applied numbers > 0.negative right skew, try following variable x:\nxn, n positive number.\nsqrt(k - x) k greater largest number x.\nxn, n positive number.sqrt(k - x) k greater largest number x.","code":""},{"path":"data-preparation.html","id":"transformations-to-achieve-linearity","chapter":"4 Data preparation","heading":"4.4.1.2 Transformations to achieve linearity","text":"machine learning methods, ordinary regression logistic regression important relationship predictors target variable straight line linear. predictive models, neural nets decision trees reliant linearity, transformations linearity hurt models.check linearity separate scatterplot target variable versus continuous predictor usually diagnostic. non-linearity found, transformations can tried either target variable predictors. experience, better try transformations predictors leave target variable . Transforming target variable can sometimes cause problems modeling. predictive models objective almost always make predictions units original variable. Therefore, target variable transformed using log (common suggestion many texts), final stage analysis inverse log must applied result. always straightforward step. Performance measures R2 root mean squared error compared models target transformed versus transformed. general, ’s better attempt achieve linearity using predictors.following chart shows typical transformations different non-linear relationships.\n(Figure 4.2)\nFigure 4.2: Transformations achieve linearity.\ndemonstration transformations work straighten relationships shown .\nFigure 4.3: Examples transformations.\n\nFigure 4.4: Examples transformations.\n\nFigure 4.5: Examples transformations.\n\nFigure 4.6: Examples transformations.\nDeriving new variablesNew variables can created used two variables already data set. requires statistical computer skills. requires understanding business problem.example, predicting whether person cancel cell phone contract , data may number calls made phone. new measure might number calls per day. number days call made. number calls made divided number calls received.","code":""},{"path":"data-preparation.html","id":"data-exploration","chapter":"4 Data preparation","heading":"4.4.2 Data exploration","text":"data set cleaned, next step thoroughly explore data. may done previous step, descriptive statistics graphical representations useful. Exploratory data analysis approach/philosophy data analysis employs variety graphical descriptive statistical techniques provide insight data set.Exploratory analysis open-ended, since goal generate clues happening can open new avenues investigation (“hypothesis generation” rather formal “hypothesis testing” featured prominently many books statistics). analysis can also support selection appropriate modeling techniques, uncover underlying structure data, assess clean data .Exploratory analysis highly linear, rigid process. Different people literally find things differently. software used exploratory analysis ideally agile functional.Commonly used graphs include:Scatter plotsBar plotsHistogramsBox whisker plotsDensity plotsBubble chartsCommonly used descriptive statistics include:Mean, median, modeVariance, standard deviationRange, Max, MinCorrelations covariancesFrequencies percentagesPercentiles quartiles","code":""},{"path":"principal-components-analytics.html","id":"principal-components-analytics","chapter":"5 Principal components analytics","heading":"5 Principal components analytics","text":"One characteristics data mining predictive analytics problems lots observations lots variables. Instead perhaps 1,000 2,000 rows data, might tens thousands even millions rows data.\n’s another dimension plethora data, however. fact might many variables. say many variables, mean potentially huge number variables, perhaps one 200 even thousands. example, one study predicting onset personal bankruptcy found 2,244 cases actual bankruptcy versus millions reporting bankruptcy among users credit cards. predict , analysts began 255 features added missing value indicators pairwise interactions resulted set 67,000 potential predictors.might seem like problem – frequently hear data better. fact, many data mining algorithms techniques talk specifically designed comb variables pick ones predictive effective terms answering business problem. turns practice many variables, especially variables wrong kind, actually reduce effectiveness business analytics project.Several simulations experiments analytic techniques shown fairly sensitive irrelevant variables tossed mix. throw random variable. might actually cause predictive ability model degrade. exact effect depends upon kind technique ’re using, many techniques suffer problem.large number variables predictive model can increase computer processing time. probably problem developing model, can factor goal employ model real time near-instantaneous results desired.","code":""},{"path":"principal-components-analytics.html","id":"approaches-to-dimension-reduction","chapter":"5 Principal components analytics","heading":"5.1 Approaches to dimension reduction","text":"basic (possibly important approach) dimension reduction apply domain knowledge. Using understanding particular problem situation information knowledge gained experts, variables potentially relevant analysis can specifice. Naturally, careful succumb preconceived notions, using domain knowledge approach sometimes advisable err side many versus variables building predictive model.cases want drop variables, just re-express smaller subset. One approach create index sort rank variable individually terms simple relationship criterion variable. can done simple correlations. , course, work situations mutliple variables work together ways captured examining one variable time.","code":""},{"path":"principal-components-analytics.html","id":"description","chapter":"5 Principal components analytics","heading":"5.2 Description","text":"Principal components analysis (PCA) unsupervised model, means target dependent variable trying estimate predict.aim principal components analysis reduce dimensionality data set p variates smaller number k variates information original data set preserved. used many disciplines including psychology, biology chemistry, education, astronomony,business. one oldest multivariate techniques, early developments (Pearson 1902) (Hotelling 1933)), necessary create training validation subsets data. really need validate principal components analysis since purely descriptive summary data. basic objective PCA re-specify variables using new set axes mutually uncorrelated right angles. many principal components extracted set variables variables, variance captured.Components extracted order variance captured, first component much variance possible along one dimension, second component much variance along second dimension uncorrelated first, etc.Typically, PCA performed covariance correlation matrix; results cases. Typical steps performing PCA :Opening n X p data set.Selecting variables submit PCA.Computing correlation (covariance) matrix.Extracting p components.Determining k , number components retain.Obtaining matrix n X k scores components used analyses.issues PCA:strong criteria test solution exist.Deciding number components somewhat arbitrary.Virtually data set (even “bad” data) can submitted PCA (meaningless) “results” obtained.","code":""},{"path":"principal-components-analytics.html","id":"the-pca-model","chapter":"5 Principal components analytics","heading":"5.3 The PCA model","text":"mathematics principal components fairly straightforward, presentations texts articles often opague terms eigenvalues, orthogonality, etc. many excellent expositions details, e.g., (Shlens 2003). Instead repeating development , provide hopefully intuitive explanation PCA.begin, PCA meaningful data sets two variables correlated covary. Typical applications PCA involve data sets many variables, nearly redundant. PCA usually final objective project, although times case. Instead, PCA dimension reduction tool make analyses effective efficient. PCA sometimes precurser ordinary regression, logistic regression, cluster analysis, techniques., given assumption correlated variables data set, question extract much information data set using reduced set variables. process can intuitively explained asking question, “one component accounts variance data set?”principal component nothing special weighted average observed variables. , consider “toy” data set shown Table 5.1.\nTable 5.1: Toy data set PCA demo\ncorrelation matrix toy data set :keep things simple, can standardize data means variable X1, X2, X3 0.0 respective standard deviations 1.0. consistent performing PCA correlation matrix rather covariance matrix. standarized variables shown Table 5.2.\nTable 5.2: Standardized data set PCA demo\nNext, weighted average observations taken using vector , three values (one variable). , random numbers entered elements shown Table 5.3.\nTable 5.3: Random weights 1 - 3\nweights applied row shown following equation. variance weighted sum becomes eigenvalue first principal component. weights random point. weights adjusted maximize variance first component, subject constraint sum squared values weights 1.0. done otherwise variance increase indefinitely. Table 5.4 shows results far.\nTable 5.4: Variance first component random weights\ncalculation first observation using random weights :\\[\\begin{equation}\n\\begin{split}\nComponent1, obs 1  = &  (0.103) \\; \\; \\; \\times (-1.10) + \\\\\n             &  (0.938) \\; \\; \\; \\times (-0.14) + \\\\\n             &  (-0.331) \\times (-1.07)  \\\\\n             & = 0.11\n\\end{split}\n\\tag{5.1}  \n\\end{equation}\\]Next, variance first component maximized (constraint noted), resulting following estimates first principal component. (Table 5.5)\nTable 5.5: First component variance (eigenvalue)\nvariance first component 1.72. Since three variables original data, variance 1.00, total variance original data 3.00. , first component accounts 1.72/3.00 = .573 = 57.3% variance.Next, second principal component computed maximizing variance component subject constraint sum squared values weights 1.0 second component right angles (.e., orthogonal first component. second component accounts 0.83/3.00 = .270 = 27.0% variance. total accounted first compoents = (1.72 + .83)/3.00 = .850 = 85.0% variance (Table 5.6).\nTable 5.6: Second component variance (eigenvalue)\nthird component (shown) account 15.0% variance. toy example demonstrated variance data require two three components, reducing dimensionality data set. Furthermore, three components uncorrelated: (Table 5.7).\nTable 5.7: Correlation matrix components\n","code":"            Observation    X1    X2    X3\nObservation       1.000 0.848 0.429 0.168\nX1                0.848 1.000 0.456 0.165\nX2                0.429 0.456 1.000 0.435\nX3                0.168 0.165 0.435 1.000"},{"path":"evaluating-predictive-models.html","id":"evaluating-predictive-models","chapter":"6 Evaluating predictive models","heading":"6 Evaluating predictive models","text":"chapter various metrics evaluating predictive models, discrete targets continuous targets discussed. content used throughout remainder text different algorithms presented.","code":""},{"path":"evaluating-predictive-models.html","id":"introduction-1","chapter":"6 Evaluating predictive models","heading":"6.1 Introduction","text":"predictive analytics applications common practice try multiple different models assess predictions using training sample select best model terms criterion., however, many things can go wrong. data may bad, underlying model used may wrong, right predictors included. can result false positives false negatives binary classification models. One outcomes usually one ’re concerned one favorable result. model accurately predicts outcome, ’s true positive. model accurately predicts outcome interested , ’s true negative.looking many different models normally frowned upon statistics, large data sets prediction objective, common practice.Even single model used, logistic regression, different formulations tried, e.g., different predictor variables, different transformations predictors, .practice trying different models, need means compare models.","code":""},{"path":"evaluating-predictive-models.html","id":"training-testing-and-validation-samples","chapter":"6 Evaluating predictive models","heading":"6.2 Training, Testing, and Validation samples","text":"Since process building predictive model usually involves trying several (many) different models determine performs best given data set, important separate data set used build model.strategy validating predictive model depends number observations available. question whether create two subsets (training validation) three (training, testing, validation). sufficient number observations training set can created least 50 - 100 observations per predictor variable considered. data set enough observations, common approach create three subsets: training, test, validation.7A model created using training set. , performance model assessed using test set. model doesn’t perform well test set, process returns training subset retrained using different settings algorithm totally different algorithm. different different models setting assumptions different techniques. process may repeated several times.relative proportions three subsets somewhat arbitrary, typical ratios 60/20/20, 50/30/20, 50/25/25 training, test, validation sets respectively.KNIME software tools provide convenient ways construct training, testing, validation sets. subsets created via random sampling process way avoiding bias.One approach number observations limited create just training validation sets. firm rule split , typical ratios 50/50, 60/40, 70/30, 80/20 training validation sets respectively. However, two subsets data, using validation set inform model building process provide proper test ability model predict new data. example, procedures, stepwise regression, use validation set evaluate accuracy iteration.two subsets used, validation held separate model building process. final model developed, validation set can used evaluate performance “new” data. Testing can done using training data via k-fold validation.k-fold performance assessment, training data randomly divided k groups, k typically five ten. analyis run k times, k subsets assigned role test data remaining data used build model. goal create model perform unseen data, held validation set.process illustrated Figure 6.1 k = 5. training data randomly divided five equally sized subsets. , first subset held remaining four subsets used build model. model assessed held first subset. process repeated five times. model performance averaged across five iterations.\nFigure 6.1: K-fold validation.\nKNIME software tools provide convenient ways construct training, testing, validation sets. subsets created via random sampling process way avoiding bias. KNIME also nodes perform k-fold validation.","code":""},{"path":"evaluating-predictive-models.html","id":"evaluating-continuous-versus-discrete-targets","chapter":"6 Evaluating predictive models","heading":"6.3 Evaluating continuous versus discrete targets","text":"two main types supervised models evaluation measures needed. First, prediction models involve continuous dependent target variables. case typically multiple regression, although techniques make predictions continuous variables well.second main type performance evaluation classification. involves assignment case group. second main type performance evaluation classification, involves assignment case group.","code":""},{"path":"evaluating-predictive-models.html","id":"evaluating-performance-with-continuous-targets","chapter":"6 Evaluating predictive models","heading":"6.3.1 Evaluating performance with continuous targets","text":"objective find well model predicts new data. , prediction metrics computed new data set, e.g., validation /test set k-fold validation methods. Sometimes useful compare performance training data holdout new dat assess whether model overfit training dataTypical measures used evaluate continuous targets listed y contains actual target values \\(\\hat{y}\\) contains predicted values using validation data.R-squared${R}^2 can interpreted variance “explained” model. , value ranges 0 1.0. ${R}^2 probably familiar measure continuous targets use multiple regression.formula ${R}^2 :\\[\\begin{equation}\n  {R}^{2} = 1 - \n\\frac { \\sum_{=1}^{n} (\\ y_i -\\hat{y_i} )^{2} }\n      {   \\sum_{=1}^{n} (\\ y_i -\\bar{y_i} )^{2}                           }\n\\end{equation}\\]MAEThe mean absolute error measured units data. square errors, measures, less sensitive occasional large errors.formula MAE :\\[\\begin{equation}\n   MAE =  \\frac{1}{n} \\sum_{=1}^{n} |{y_i}-\\hat{y_i}|\n\\tag{6.1}    \n\\end{equation}\\]MAPEThe MAPE, mean absolute percent error, one attractive measures becuase report percentage makes intuitive sense. However, limitations. First , actual values zero, error becomes apparently infinite division zero. Second, found favor models systematically predicting values low rather high. ’s estimates low, percentage error exceed 100%. predictions high, upper limit percentage error. Finally, positive negative values estimates, MAPE used.formula MAPE :\\[\\begin{equation}\nMAPE = \\frac{1}{n} \\sum_{=1}^{n} {\\left|\\frac{{y_i}-\\hat{y_i}}{y_i}\\right|} \n \\tag{6.2}    \n\\end{equation}\\]RMSEThis square root mean squared error. metric captures far predicted values actual values units target variable.\nOne issue root mean squared error affected occasional large error MAE MAPE squaring errors. Whether problem depends extent cost seriousness large errors situation modeled. costs roughly linear respect error, statistic might best one use.formula RMSE :\\[\\begin{equation}\n   RMSE = \\sqrt{\\frac{1}{n} \\sum_{=1}^{n} ({y_i}-\\hat{y_i}){^2}}\n\\tag{6.3}  \n\\end{equation}\\]","code":""},{"path":"evaluating-predictive-models.html","id":"evaluating-performance-with-classification-models","chapter":"6 Evaluating predictive models","heading":"6.3.2 Evaluating performance with classification models","text":"discussion focused assessing performance target variable takes two nominal levels (.e., binary). binary case situation might encountered, however. cases target three categorical levels, assessment can reduced binary evaluation purposes. done selecting one multiple levels positive 1 level assigning rest levels negative 0 level.basic structure evaluating binary models contingency table classification matrix (sometimes called confusion matrix). two two table actual classifications rows predicted classifications columns.8","code":""},{"path":"evaluating-predictive-models.html","id":"x-2-classification-matrix","chapter":"6 Evaluating predictive models","heading":"2 X 2 Classification Matrix","text":"Figure 6.2 shows general two two contingency table actual rows predicted columns. predictions can either positive negative; can thought buy buy, try try, churn churn case telecommunications, error error, even fraudulent fraudulent.\nFigure 6.2: labeled 2 X 2 table.\ncells can labeled True Positive True Negative (correct) false positive false negative (incorrect). begin evaluation, “naive rule” can used basic benchmark. rule classifies records belonging prevalent actual level. hope model perform better naive model.several measures frequently used assess model performance using 2 X 2 table. :Accuracy = (True positive + True negative) / Total cases; also known “fraction correct.”Sensitivity = (True positive) / (True positive + False negative); also known “recall,” “hit rate,” “true positive rate.”Specificity = (True negative) / ( True negative + False positive); also known “true negative rate.”Precision= (True positive) / (True positive + False positive); also known “positive predictive value.”Negative predictive value = (True negative) / (True negative + False negative).F-score = 2 /[(sensitivity-1) + (precision-1)]important select metric appropriate situation modeled. Accuracy good measure number false positives false negatives approximately equal cost seriousness false positives false negatives similar.Consider hypothetical data results shown Figure 6.3.\nFigure 6.3: Evaluation metrics two situations.\nresults left two different models testing Covid-19. results right two models SPAM detection. four 2 X 2 tables accuracy, yet models equally favorable. Accuracy good measure two situations since consequences false positives false negatives .Covid tests model 1 preferred model 2 10 actual Covid cases mistakenly classified Covid. Sensitivity (0.998) best measure performance since arguably important avoid classifying cases Covid Covid. person without Covid classifyied Covid, course causes stress individual, can remedied conducting tests.hand, SPAM tests best metric Precision model 2 higher precision (0.998). Classifying cases SPAM exists SPAM important filtering messages SPAM SPAM. latter case, critical messages might never seen email user.general, false positives serious false negatives, sensitivity good metric. false positive serious false negatives, precision good metric.","code":""},{"path":"evaluating-predictive-models.html","id":"evaluating-models-with-class-imbalance","chapter":"6 Evaluating predictive models","heading":"6.3.2.1 Evaluating models with class imbalance","text":"situations outcome interest binary prediction model occurs infrequently. causes imbalance outcomes. example, say developing model predict fraud 50,000 cases 500 cases fraudulent 49,500 .many domains imbalance occurs, including:Churn prediction telecoms users churn.Predicting insurance fraud - claims fraudulent.Diagnoses rare medical conditionsOnline advertising click-rates - viewers click.Pharmaceutical research - potential modules useful development.Many classification models handle situation well. One likely outcome situations model simply predict cases fraudulent. , example cited, 50,000 cases classified non-fraudulent, accuracy equals 49,500/50,000 99%. accuracy!several ways handle situations severe class imbalance. One way use stratified sampling training set force balance two outcomes. case fraudulent data case, assume training set 40,000 cases validation set 10,000 cases formed stratefied presence fraud. training sample, expected 400 cases fraud training set 39,600 cases non-fraud. random sample 400 taken 39,600 non-fraud cases. creates balanced data set 400 cases fraud non-fraud.model created using balanced data set. evaluation metrics model useful, however. Instead, model applied 10,000 cases validation set evaluation metrics computed. disadvantage approach cases training set used create model.Another approach oversample rare cases data set. training data set fraud situation, 400 fraud cases repeatedly copied 99 times 39,600 cases fraud non-fraud set. validation set left alone. sophisticated approach oversample using similar cases. Using characteristics case minority, set similar cases selected. implemented R package smotefamily (SMOTE 2019) SMOTE node KNIME.approaches dealing imbalance include following.Lower cutoff increase number predictions minority class.Adjust prior probabilities (e.g., naïve Bayes discriminant analysis).Weight minority class heavily, case weighting available algorithm; essentially like oversampling.Use costs differentially weight specific types errors.Examples using approaches discussed chapters classification prediction models.","code":""},{"path":"evaluating-predictive-models.html","id":"roc-curves","chapter":"6 Evaluating predictive models","heading":"6.3.2.2 ROC curves","text":"Another useful method assessing accuracy predictive binary classification models ROC (receiver operating characteristic) curve. ROC curves show tradeoff sensitivity (1-specificity) tradeoff true positive rate false positive rate predictive model. curves especially useful comparing performance two models.ROC curve Figure 6.4 shows line random performance red. blue line represents model achieves much better random performance. curve generated varying cutoff threshold assigning cases one binary outcomes computing Sensitivity 1 - Specificity plotting values. cutoffs 0.0 1.0 result curve shown figure. closer ROC curve upper left corner (1.0, 1.0), accurate model.measure quality model area curve AUC, case approximately 0.72. maximum possible area model predicts perfectly 1.0; area red line model predicts randomly line 0.50.\nFigure 6.4: Example ROC curve.\nconcept tradeoff true positives false positives 2 X 2 table illustrated Figure 6.5.\nFigure 6.5: Tradeoff true positives false positives.\nthreshold cutoff selected achieves .85 true positives (chart left), model represented blue line result .45 false positives. lower percentage false positives .20 (chart center), tradeoff shows true positives identified .65 time. way achieve higher true positives fewer false positives create model results ROC curve left, shown panel right. model achieved true positive rate .85 .10 false positive rate.","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"7 Multiple regression","heading":"7 Multiple regression","text":"","code":""},{"path":"multiple-regression.html","id":"introduction-2","chapter":"7 Multiple regression","heading":"7.1 Introduction","text":"chapter covers one versatile useful techniques, ordinary least squares regression. Included chapter discussion distinction explanatory predictive modeling. Several examples predictive modeling using regression included.Linear regression arguably well-known many algorithms used predictive analytics. several reasons . First, logical, linear model many uses conceptually attractive. Linear relationships easy think .Second, technique relatively easy program, many, many software tools available. can easily programmed language just statements.Third, flexible – can applied many types problems, even first might seem linear regression candidates. Many phenomena can cast, least approximately, linear model.Fourth, regression multiple, distinct uses. often used build models explain one independent variables affect continuous, dependent variable. also used control, sense regression models can help identify cases error problematic. Finally, focus course regression prediction.Fifth, regression long elaborate history development. huge topic. many books courses devoted subject. ’s development 100 years. basic idea quite simple, many exceptions, special cases, assumptions might fit particular situation. ordinary regression, target variable continuous, years many modifications basic model led new regression-type models Cox regression, Poisson regression, logistic regression, others.","code":""},{"path":"multiple-regression.html","id":"regression-techniques","chapter":"7 Multiple regression","heading":"7.2 Regression techniques","text":"exist several algorithms perform different forms regression developed better align objectives analysis characteristics data. Ordinary least squares, topic chapter applies problems single continuous target variable one predictor variables, can continuous categorical (mixture ).models covered chapter :Ordinary linear regressionForward selection featuresBackward selection featuresStepwise selection featuresLasso regressionNote regression-type problems can also addressed using techniques neural networks, support vector machines, others. discussed chapters","code":""},{"path":"multiple-regression.html","id":"regression-for-explanation","chapter":"7 Multiple regression","heading":"7.3 Regression for explanation","text":"Regression used differently different disciplines. economics, psychology, sociology fields, regression mostly used goal developing causal explanations.(Shmueli 2010)assumed readers text introduced multiple regression. context regression applications statistics texts building models objective making causal inferences explanation. theoretical model posed, predictor variables identified, observational data collected (cross-sectional, longitudinal, ). Hypotheses based theoretical model tested using regression related techniques. aims studies include assessing statistical significance magnitude independent variables. challenging task accomplishing degree confidence requires considerations several strict assumptions.Violations assumptions can cast doubt validity conclusions made analysis three questions asked : (1) assumption met current situation? (2) , serious consequences violation assumption? (3) assumption violated critical analysis, can remedial techniques used alleviate consequences?assumptions need met confidently deduce statistical significance predictors overall regression model , interpretability effect predictor target, confidence intervals estimates made model. Gauss-Markov theorem states additive linear model ordinary lease squares regression produces unbiased estimates lowest variance possible linear estimators. usually given acronym “BLUE” “Best Linear Unbiased Estimators.” four Gauss-Markov assumptions :dependent variable linear, additive function set predictors plus error term.error term conditional mean zero.variance error term constant values predictors (homoskedasticity).error term independently distributed (autocorrelation).list, following assumptions sometimes added:Predictor variables correlated error term.perfect collinearity among predictors.error term normally distributed (relatedly, outliers observations undue influence).number observations must greater number predictor variables (usually 5 10 times many observations predictors.)predictor variables non-zero variability, .e., constants.Violation assumption can result biased estimates parameters regression model. estimated size variance predictor coefficients can biased upward downward. cases, can render results totally misleading.","code":""},{"path":"multiple-regression.html","id":"example-omitted-variable-bias","chapter":"7 Multiple regression","heading":"Example: Omitted variable bias","text":"considerable controversy question whether increased spending public education leads better student outcomes. example, Heritage Foundation published report concluded:Federal state policymakers resist proposals increase funding public education. Historical trends evidence suggest simply increasing funding public elementary secondary Education led corresponding improvement academic achievement.(insert reference)One approach investigating question look expenditures per pupil 50 states plus District Columbia measuring student performance. performance criterion data set average combined SAT scores. first 10 observations shown Table 7.1. hypothesis students states spend per pupil primary secondary schools better prepared college thus perform better SAT college prep test.SAT scores regressed expenditures (1,000’s). coefficient expense significant (p < .001) negative (-22.28), suggests increasing spending education results lower performance SATs. true, finding important implications school funding. argue spending education may right.turns important variable omitted regression: percentage students taking SAT state. considerable variation measure, shown chart Figure 7.1. Connecticut, 80% students took SAT Mississippi 4% took SAT. regression run expenditure per pupil percentage taking SAT, conclusion different. coefficient expense positive (8.60) significant (p = 0.046).second analysis controlled percentage taking SAT. means effect expenditures estimated removing effect percentage taking SAT. including one control variables regression, effects separated variable interest. control variable usually primary interest analyst. point omitting key variable can lead incorrect conclusions regression. Explanatory models difficult demanding using regression prediction.\nFigure 7.1: Percent taking SAT state DC.\n\nTable 7.1: Expenditures education SAT scores.\n","code":""},{"path":"multiple-regression.html","id":"regression-for-prediction","chapter":"7 Multiple regression","heading":"7.4 Regression for prediction","text":"Predictive modeling process using statistical model regression make predictions new future observations predictor variables. Image recognition, natural language processing, many business problems frequently emphasis prediction rather explanation.Breiman (L. Breiman 2001) described considered two cultures statistics. called prevalent approach “data modeling culture.” Applications typically involve relatively small numbers observations smaller number predictors, important aim causal inference. approach consistent explanatory use regression.second approach (culture) Breiman called “algorithmic culture,” focuses almost purely predictive accuracy rather statistical tests interpretation model parameters. applications typically involved huge data sets, sometimes millions observations well many potential predictors. (cases, number potential predictors even larger number observations.) new paradigm began 1980s gained popularity 1990s present. domain data mining, predictive analytics, machine learning, big data.Many critical newer approach, beginning comments Breiman’s Cox (Cox 2001), Efron (Efron 2001), leading classical statisticians. Despite criticism many successful applications “pure prediction algorithms” (Efron 2020), business, medicine, biology, fields. potential misleading even dangerous predictions using models remains responsibility analyst carefully acknowledge limitations well benefits models.","code":""},{"path":"multiple-regression.html","id":"revisiting-regression-assumptions","chapter":"7 Multiple regression","heading":"7.4.1 Revisiting regression assumptions","text":"regression used prediction rather explanation, role assumptions changed. primary objective prediction accuracy assessed new data. usually assessed creating training test subsets data set many observations. training data used develop predictive model test set used represent “new” data.9 fact, assumptions may violated, model predicts well, can useful.Allison (2014) noted outside academia, “…regression primarily used prediction.” notes important differences using regression explanation versus prediction. Specifically, writes omitted variable bias much less issue goal prediction. Multicollinearity (extreme) can tolerated predictive model since coefficients individual variables primary concern. Measurement error predictor variables leads bias estimates regression coefficients, estimates predictor coefficients critical. (course, predictive applications, high degrees measurement error can make predictions less accurate).differences two uses regression include following:--sample prediction accuracy important R2 original data set. However, high R2 hold-sample samples important prediction. causal modeling, even low R2 values using original data set much concern. hypothesis tests predictors applications important.Normality error term requirement since hypothesis testing goal predictive regression.perspectives predictive regression discussed controversial many statisticians undoubtedly disagree many statements made. truth, causal modeling important done well can lead insights likely longer-run usefulness. However, present purposes, focus chapter regression chapters book prediction. critical understand predictive model created ignores traditional assumptions regression, using results causality established, can lead huge mistakes. may interest find predictor values greatest impact target variable, assumed manipulation impactful variables likely lead expected changes target. Correlation, causality, obtained predictive models.one assumption critical accurate predictive regression models dependent variable linear, additive function predictors. Othe algorithms,discussed later chapters, can less “automatically” deal complications non-linearity. regression, however, analyst explore possible non-linear relationships potential interactions among predictors. Failure can lead sub-optimal models. predictors transformed way, accuracy regression model improved without additional collection data.","code":""},{"path":"multiple-regression.html","id":"prediction-example-used-toyota-corollas","chapter":"7 Multiple regression","heading":"7.4.2 Prediction example: Used Toyota Corollas","text":"dataset prices used Toyota Corolla sale late summer 2004 Netherlands obtained Kaggle. (reference) 1436 records containing details 38 attributes, subset extracted 1,000 observations variables used example. variables included price (target variable) following predictors:Age car (months)Mileage (KM)Fuel type (Diesel, natural gas, gasoline)HorsepowerAutomatic transmission (Yes )Engine displacement (CC)Weight (KM)goal predict price used Toyota Corolla based specifications.KNIME workflow analysis shown Figure 7.2.\nFigure 7.2: Workflow OLS regression Toyota Corolla price data.\nregression results shown Table 7.2. measures show results training test data comparable. training data shows better results R2 root means square error measures slightly better test data mean absolute error mean absolute percent error.\nTable 7.2: Regression Toyota1000 data.\nscatterplot predicted price versus actual price test data indicates reasonable results. one clear outlier row 221 (Figure 7.3). presence outlier likely explains measures involving absolute value rather squared residuals worse test data. Squaring residuals (R^2 root mean square error)results greater effect performance measure.\nFigure 7.3: Predicted versus actual prices Toyota Corollas.\n\nFigure 7.4: KNIME workflow regresion analysis apartment prices.\n","code":""},{"path":"multiple-regression.html","id":"appendix-a-brief-history-of-regression","chapter":"7 Multiple regression","heading":"Appendix: A brief history of regression","text":"term regression basic idea behind came British scientist, Sir Francis Galton. interested heredity evolution; surprising since cousin Charles Darwin. (Stanton 2001)\nSir Francis Galton.\nGalton asked following question: “relationship heights children heights parents?” empirical scientist. , collected data pairs parents children carefully studied data. created chart children’s heights (adults) versus height parents. (Figure 7.5) fit line data line close possible observations - blue line shown chart. Galton fitted line, calculated intercept slope.slope intercept fitted line, .611 26.4 respectively, shown top chart.\nFigure 7.5: Height children vs. height parents.\nexpected, Galton found tall parents tended tall children, relationship perfect. one--one relationship parents’ heights height children follow red line Figure 7.5. Instead, evidence adult height children tall parents regressed reverted mean height adults, shown blue line chart. example, parent’s height 72 inches, expected height children 70 inches. also found true shorter parents. parent’s height 64 inches, expected height children 65 inches. , heights children also tended toward mean height adults.Galton use term regression idea reverting returning norm. People took idea drawing line data estimating equation, applied many types problems, called process regression, Galton, even though context meaning different. Karl Pearson, collaborator, lab assistant Galton, developed mathematics regression today.","code":""},{"path":"multiple-regression.html","id":"problems-2","chapter":"7 Multiple regression","heading":"Problems","text":"Problem 1","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"8 Logistic regression","heading":"8 Logistic regression","text":"Many analytics problems can expressed prediction two outcomes likely. target variable applications binary expressed “1” “0.” many questions binary outcome, :Buy/buy.Success/Failure.Heart attack/heart attack.Continue/continue.Fraud/fraud.Cancer/cancer.Vote yes/Yote ., course, just partial list since binary outcomes concern many different disciplines including human resources, marketing, medicine, education, political science, criminology, others.Logistic regression one technique frequently used create predictive models target binary. (techniques discussed subsequent chapters.) build model, data set obtained following structure shown Figure 8.1.\nFigure 8.1: Typical data structure.\nblue squares Figure 8.1 represent zero maroon squares represent one. cases may individuals, companies, types entities. predictors can continuous variables (e.g., age) categorical (e.g., sex). first might seem data set structure analyzed ordinary least squares regression. several reasons linear appropriate, including fact estimates target constrained 0 1. Furthermore, logistic regression require assumption normally distributed error term assumption error variance constant. (fact, error variance function predictors.)One way understanding logistic regression model think intermediate variable (explicitly observed) predictors. target variable yi (subscript represents ith case data set) binary, intermediate variable pi continuous can thought propensity target 1.0 versus 0. Therefore, pi continuous range zero one. pi modeled, predictions target can obtained using decision rule based threshold value: pi greater threshold, yi predicted 1; otherwise yi predicted 0.equation weighted linear combination predictors created. linear combination can produce continuous outcome estimates ranging -infinity +infinity. logit transformation pi \\(\\hat{y_i}\\) created results continuous variable linear function predictors:\\[\\begin{equation}\n    \\ \\mathit{}\\;p_{}\\; >\\; threshold,\\;  \\hat{y}_{} = 1;\\; else\\;  \\hat{y}_{} = 0, \\  \\mathit{}\\;p_{}\\; = \\frac{1}{1 + e^{-(log(odds\\;ratio)}}\n \n\\end{equation}\\]\nlog odds transformation probabilities 1 0. logistic regression model estimated linear function log [p(1)/p(0)] p( ) indicates probability.p(1) p(0) unobserved; coefficients logistic regression model estimated using technique works create p(1) p(0) maximizes conformance observed 1’s 0’s observations data set. Unlike ordinary least-squares regression, closed-form algebraic solution estimation problem, iterative maximum likelihood algorithm can used.effect logistic transformation illustrated Figure 8.2. variable x-axis continuous, y-axis variable constrained zero one due logistic transformation.\nFigure 8.2: logistic function.\nNote logistic regression technique can used cases. closely related technique called probit analysis works similar manner. Instead using logistic function S-shaped curve, probit analysis uses cumulative normal function. Investigations two techniques shown similar results. Also, hyperbolic tangent function can used properties similar logistic function except ranges -1 +1 instead 0 +1. course, techniques neural networks, decision trees, discriminant analysis can also used binary targets.","code":""},{"path":"logistic-regression.html","id":"example-with-a-single-predictor","chapter":"8 Logistic regression","heading":"8.1 Example with a single predictor","text":"Consider simple classification problem using synthetic data. example involves predicting purchase (“Yes” “”) customer function customer’s age. Twenty observations nine “Yes” values 11 “” values created demonstration10. listing first 10 rows data set shown Table 8.1.\nTable 8.1: First 10 rows simulated data\nlogistic regression run Purchase target Age predictor variable. equation estimated using logistic regression data Table 8.1 :\\[\\begin{equation}\n   log \\left[  \\frac{\\;p_{}\\;}{1-\\;p_{}\\;} \\right] = 11.75 - 0.286 \\cdot \\;Age_{}\\;\n\\tag{8.1}    \n\\end{equation}\\]Interpreting coefficients produced logistic regression model straightforward due non-linear relationship predictors probabilities. example, can noted since coefficient Age negative, increases Age result decreased likelihood purchasing product. value coefficient (-0.286) means one unit increase Age associated -0.286 decrease log odds purchase. intercept (11.75) usually much interest problems. intercept adjust model fit overall proportion “yeses” “noes.”Using equation (8.2), graph probability versus age (Figure 8.3) created, showing decrease probability age increases.\\[\\begin{equation}\n    \\;p_{}\\; = \\frac{1}{1 + e^{11.75 - 0.286 \\cdot \\;Age_{}\\;}}\n\\tag{8.2}   \n\\end{equation}\\]\nFigure 8.3: Probability purchase versus age.\nEquation [3] can also used create predictions comparing probability threshold value using following decision rules:      probability purchase > .5,\n      likely purchase=“Yes,”\n      else likely purchase=. Table 8.2 shows results applying decision rule.\nTable 8.2: Predictions using logistic model.\nresults showing performance predictive classification models typically displayed “classification matrix” (also known “confusion matrix”) shown Table 8.3.\nTable 8.3: Confusion matrix\n","code":""},{"path":"logistic-regression.html","id":"example-predictive-analytic-in-hr","chapter":"8 Logistic regression","heading":"8.2 Example: Predictive analytic in HR","text":"Employee retention important objective HR departments many organizations. According report Work Institute (“2020 Retention Report” 2020), 27 percent U.S. employees voluntarily left jobs 2020 total estimate cost $630 billion due factors cost replacement, loss productivity. study reported three-fourths turnover preventable.Using predictive analytics can enable employers identify employees risk leaving take preemptive corrective action. data set employee turnover obtained Kaggle (“Hr—Analytics-Employee-Turnover,” n.d.) . data set 14,999 rows 9 columns:Satisfaction: Level scored 0 1.Evaluation: Last evaluation rating scored 0 1.Projects: Number projects completed work.Hours: Average monthly hours workplace.Years: Number years spent company.Promotion: Whether employee promoted last five years.Department: Department employee worked.Salary: Relative level salary: low, medium, high.Left: Whether employee left workplace .KNIME workflow, shown Figure 8.4, created analyze HR data.\nFigure 8.4: KNIME workflow logistic regression employee turnover data.\ndescription node shown Table 8.4.\nTable 8.4: Description workflow nodes employee turnover logistic model.\nNode 1 data set 14,999 observations nine variables read using File Reader node. Promotion, Salary Left string variables; others numeric. data set explored using Data Explorer. variables appeared suitable analysis exception YearsAtCompany highly skewed shown histogram Figure 8.5.\nFigure 8.5: Histogram YearsAtCompany.\nSince skewed variable can lead poorer performance logistic regression, transformation YearsAtCompany computed using Math Formula (Node 3) original variable replaced subsequent nodes. reduced skewness YearsAtCompany 1.85 0.59.Predictive models typically better normalized predictors, following variables normalized (Node 4) range 0 1: Satisfaction, Evaluation, Projects, Hours, Years.Node 5 data split randomly two subsets: Training Test. split ratio 70/30 sample stratified Left proportions Left / leave Training Test partitions.SMOTE (Node 6) used oversample Left category. Prior oversampling, split Training partition 7,999 Left 2,499 leave. imbalance binary target variable usually cause logistic regression make predictions level larger number cases. applying SMOTE, levels (Left leaver) 7,999 cases. balanced sample used build logistic model, evaluation model performance done using Test set original level proportions. balanced sample used facilitate estimation logistic regression model (ChristianSalas-Eljatiba ValeskaYaitul 2018).k-fold cross validation (Node 7) used assess stability logistic model using Training data employing X-Partitioner X-Aggregator nodes KNIME k = 10. Training data divided randomly 10 approximately equal segments. , logistic regression run 10 times, time withholding 10% subset. Logistic regression requires two steps KNIME; Node 8 Logistic Regression Learner runs model outputs model coefficients; Node 9 Logistic Regression Predictor applied compute predictions row Training data. 10 held samples used assess predictive accuracy. results 10 runs obtained output Node 10 shown Table 8.5.\nTable 8.5: Results k-fold validation.\naverage accuracy 80.57%, maximum 81.63% minimum 78.86%. shows model reasonable accuracy quite stable performance across 10 folds. Recall analyses conducted using oversampled data.final model Node 10 (using last 10-fold partitions) passed evaluation nodes Training data. predictions Training data computed Node 10. ROC curve created Node 11 Scorer Node 12 created classification (confusion) matrix along several evaluation metrics. corresponding set analyses run Test data, shown Nodes 13, 14, 15.classification matrices Training Test data shown Tables 8.6 8.7.\nTable 8.6: Confusion matrix training data.\n\nTable 8.7: Confusion matrix test data.\nNote results Training data based oversampled data Test data results used original ratio Left versus leave. imbalanced data created less ideal results. Scorer Nodes (13 15) also provide summary descriptive statistics derived classification matrices. results partitions shown Table 8.8.\nTable 8.8: Performance metircs training test data.\noverall accuracies Training Test data sets close, slight edge Training data. expected since logistic regression optimized Training data. However, accuracy best measure situation due imbalance binary outcomes. fact, observations Test data predicted leave, accuracy .762. approach useful, course, since never identifies left company.Cohen’s kappa lower Test data Training data. likely due imbalance binary outcomes original data. known Kappa reaches maximum 1.0 balanced outcomes, lower kappa Test data surprising (Widmann, n.d.).area concern results Test data precision, much lower Test data Training data. due imbalance number cases Left versus number leave. imbalance, “easier” predict leave. context HR application, argued mistakenly predicting employee leave serious failing predict employee leave. sensitivity predicting likely lead 0.8 training data. Thus, vast majority likely turnover can identified. situation results model can used signal HR investigation identified likely leave.ROC curves obtained Training data Node 12 testing data Node 15. curves shown Figures 8.6 8.7.\nFigure 8.6: ROC curve training data. (AUC = 0.829)\n\nFigure 8.7: ROC curve training data. (AUC = 0.833)\nROC curves data sets quite similar, indicating comparable predictive performance Training Test data. areas ROC curves (AUC) 0.83, fairly high, AUC varies 0.0 1.0.\noverall assessment logistic model data useful identifying employees likely turnover predictor variables input model. noted earlier, however, employee predicted turnover, additional human review needed since precision model high.","code":""},{"path":"logistic-regression.html","id":"predictor-interpretation-and-importance","chapter":"8 Logistic regression","heading":"8.3 Predictor interpretation and importance","text":"questions predictive model : “can changes predictor variables interpreted? relative importance predictors employee turnover?” Answering might provide insight steps might taken reduce turnover.question predictor (feature) importance studied extensively multiple regression models. However, comparatively little published measuring predictive importance logistic regression (Azen Traxel 2009). approach based “dominance analysis” developed available Python R. Dominance analysis directly available KNIME. simpler approach discussed section (one limitations like methods).coefficients predictors used infer importance logistic regression coefficients log odds. Rather, usually desired impact probability changes predictor. complicated effect probability outcome linear, probabilities associated predictor depend upon values predictors. Thus, “importance” particular predictor varies based range predictor considered settings every predictor. , relative sizes coefficients logistic model indicate predictor importance, even predictor variables normalized.Likewise, p-values significance tests predictor interpreted measure importance practical sense. small p-value indicates variable low variance compared magnitude, variable still minor effect target variable.general, binary logistic models approach interpretation can fully describe relationship changes predictor value probability target variable (Long Frees 2006).\napproach examine effect probability outcome predictor varied minimum maximum. Since results changing one input variable depend upon values predictors, “baseline” set values predictors established. continuous predictors set respective means nominal values set modal values. results (Table 8.9) indicate “Number years spend company” important followed Satisfaction. Note still fully address question interpretation, may provide insight.\nTable 8.9: Range probabilities leaving predictors.\nAnother approach interpreting predictors chart changes probability predictor different levels another predictor. Figure 8.8 shows probability leaving company varies level satisfaction three salary levels. might expected, employees Low salary generally likely leave level satisfaction.type analysis useful examining impact two variables still fully address question variable importance.\nFigure 8.8: Probability leaving satisfaction different salary levels.\n","code":""},{"path":"logistic-regression.html","id":"regularized-logistic-regression","chapter":"8 Logistic regression","heading":"8.4 Regularized logistic regression","text":"Next, look subset data used Kaggle competition developed University Melbourne asked participants predict success research grant applications. subset used based involved data preparation process available R package “Applied Predictive Modeling” . subset data package created removing observations missing values, yielding total 5,503 observations 258 columns, including binary target column “Class” (successful versus unsuccessful).discussed chapter regression, KNIME includes integration algorithms H2O suite analytics programs. H2O algorithm “Generalized Linear Model (GLM)” used analyze grant application data. GLM program run following settings:Target Column => ClassPredictors => NumCI Day (257 columns)Ignore constant columns checkedThe random seed set 123.algorithm family set Binomial, Link logit.alpha parameter controls penalty functions LASSO ridge regression. alpha 1.0 produces LASSO 0.0 produces ridge regression. set 1.0 LASSO.lambda parameter controls amount regularization 0 (regularization) larger values regularization. option perform automatic search optimal value lambda parameter selected. GLM first fit model maximum regularization using high lambda value causes coefficients zero, sequentially reduces lambda minimum lambda (set .0001) overfitting occurs. best value lambda determined using validation subset data, set 15%. best lambda maximizes log-likelihood GLM model).workflow regularized logistic regression created using KNIME H2O (Figure 8.9) corresponding node descriptions Table 8.10. Note optimum value lambda found independently test data.\nFigure 8.9: Probability leaving satisfaction different salary levels.\n\nTable 8.10: Node descriptions regularized logistic regression.\nRegularization able simplify model considerably (74 fewer predictors) actually improved area ROC curve, accuracy, sensitivity (Table 8.11). Specificity slightly lower regularized model.\nTable 8.11: Comparison full regularized logistic regression models.\n","code":""},{"path":"logistic-regression.html","id":"probability-calibration","chapter":"8 Logistic regression","heading":"8.5 Probability calibration","text":"applications, important predict probability observation belongs specific classification outcome. Performance metrics accuracy, sensitivity, specificity focus overall classification one two outcomes. Typical models also used provide rank ordering cases highest lowest probability. metrics may correspond actual frequencies outcomes. words, probabilities calibrated “true” probabilities. algorithms, support vector machines, boosted trees, naïve Bayes may accurate terms classification, terms matching probabilities. Logistic regression shown produce well-calibrated probabilities. section assess calibration logistic regression present two calibration methods, Platt Scaling Isotonic Regression.Assessing calibration accuracy binary outcomes can done via calibration plot observed fraction positive outcomes versus mean probability obtained model. calibration plot success grant applications example previous section computed. Nodes added workflow shown Figure 8.9). output H2O Table node workflow submitted following series nodes (Figure 8.10 Table 8.12).\nFigure 8.10: Workflow creating calibration plot.\n\nTable 8.12: Node descriptions calibration workflow.\nlogistic regression fairly well calibrated, deviation mid-range probabilities (Figure 8.11).\nFigure 8.11: Calibration logistic regression probabilities.\n","code":""},{"path":"logistic-regression.html","id":"evaluation-of-logistic-regression","chapter":"8 Logistic regression","heading":"8.6 Evaluation of logistic regression","text":"Logistic regression one used algorithms predicting binary classes. easy run can handle large numbers observations efficiently. ordinary regression, number observations analysis much greater number predictors. Also, care taken avoid overfitting (true supervised models). logistic regression can used predict categorical targets three levels, models decision trees, neural nets, k nearest neighbors better choices problems. cases, multi-class problems can converted just two levels logistic regression can meaningfully applied.H2O GLM regularization model available KNIME demonstrated fairly large data set. Regularization reduced number predictors significantly maintaining overall model performance. estimated probabilities model generally aligned observed proportions.Logistic regression assumes linearity log-odds ratio predictor variables. Interactions can created, can cause computational problems. discussed earlier, interpreting coefficients predictors straightforward since coefficient reflects change log odds difficult intuit.Finally, data samples logistic regression can result failure converge solution. One issue potential complete separation data single feature. situation, weight can estimated feature question; essentially infinite.","code":""},{"path":"ensemble-models.html","id":"ensemble-models","chapter":"9 Ensemble models","heading":"9 Ensemble models","text":"Many different machine learning models developed new variants continue explored. leads questions , “algorithm best?” “algorithm best particular problem context data set?” might expected, simple, definitive answer questions. interesting comparison performance terms error five algorithms shown Figure 9.1 (Seni Elder 2010). None algorithms performed best data sets.\nFigure 9.1: Relative Performance 5 Algorithms 6 Datasets\nObservations model performance shown Figure 9.1 led idea combining predictions two models, process called ensemble learning. Ensemble models can lead lower prediction errors predictions diverse individual models independent aggregated make final estimate. Everyday examples effectiveness combining many independent estimates theme book Wisdom Crowds (Surowiecki 2004). famous example power combining estimates given .1907 Sir Francis Galton published article Nature combining individual estimates. attended West England Fat Stock Poultry Exhibition intrigued weight guessing contest. goal guess weight ox butchered dressed. Around 800 people entered contest wrote guesses tickets. person guessed closest butchered weight ox won prize.\ncontest Galton borrowed tickets found median value guesses. discovered median guess across entrants 1,207 pounds, 9 pounds actual weight 1,198 pounds. (mean value guesses exactly 1,198)","code":""},{"path":"ensemble-models.html","id":"creating-ensemble-models","chapter":"9 Ensemble models","heading":"9.1 Creating ensemble models","text":"several ways generate several different base learners can combined create ensemble model.Different algorithms can used, decision trees, logistic regression neural networks.Changing parameters single model, number branches decision tree number nodes neural networks.Using different subsets predictor variables.Using different subsets observations.different algorithms can used basis building ensemble models, chapter focus decision tree, classifications regression.Ensemble models use weaknesses decision trees create advantage. Tree models around long time, development ensemble models, use limited two reasons. First, decision trees unstable different samples. , considerably different decision trees can result data set changed slightly. , turn, due two characteristics tree models. () models produced globally optimum. split made, consideration given altering split based splits made tree. (b) two potential predictor variables, B, highly correlated, variable selected split, effectively makes variable B essentially unimportant. expected. However, slightly different sample selected, example randomly splitting data set training test subsets, variable B might selected first, making variable apparently less important. Thus, changing seed random split result quite different results.two main benefits ensemble models (Brownlee 2020):Reduced variation (reliability) predictions different data samples.Improved prediction accuracy.","code":""},{"path":"ensemble-models.html","id":"reduced-variation","chapter":"9 Ensemble models","heading":"9.1.1 Reduced variation","text":"Performance evaluation models typically based accuracy predictions. might measured mean square error case regression trees errors made classification trees. assessments important, also important consider variation performance different data samples analyzed. One way assess variation use k-fold partitioning data set. k samples taken submitted tree model, performance typically vary. range standard deviation variation provide indication likely performance unseen data. possible ensemble model might improve average performance across k samples compared single tree model yet smaller range performance values.","code":""},{"path":"ensemble-models.html","id":"improved-performance","chapter":"9 Ensemble models","heading":"9.1.2 Improved performance","text":"Ensemble models can perform better average single tree model errors averaged . creating multiple tree models, problems local optima can avoided. Since different samples data set likely lead different predictors variation performance, risk relying single model can avoided.","code":""},{"path":"ensemble-models.html","id":"parallel-and-sequential-learners","chapter":"9 Ensemble models","heading":"9.2 Parallel and sequential learners","text":"Tree ensemble models require two steps:Creating set distinct predictive tree models.Combining predictions set models produce overall prediction.Ensemble models can classified two types based set predictive models created: parallel learners sequential learners. Parallel learners generate set strong (complex) models independence models used average errors. Examples include Bagging Random Forests.sequential learners, consecutive weak models created, errors captured stage process. errors stage become target variables next stage. Examples sequential ensemble models include AdaBoost, Gradient Tree Boosting, XGBoost.","code":""},{"path":"ensemble-models.html","id":"bagging-bootstrap-aggregating","chapter":"9 Ensemble models","heading":"9.2.1 Bagging (Bootstrap Aggregating)","text":"Bagging multiple data sets bootstrap sampling aggregating results create predictions early parallel ensemble approach\n(L. Breiman 1966). bootstrap sampling n observations, k repeated samples size n selected replacement. Figure 9.2 shows diagram bagging approach. Repeated sampling means observations left given sample observations included . regression trees, average k predictions across becomes ensemble prediction. classification trees, majority vote taken obtain predicted class.\nFigure 9.2: Creating bagged ensemble\n","code":""},{"path":"ensemble-models.html","id":"random-forests","chapter":"9 Ensemble models","heading":"9.2.2 Random Forests","text":"random forest, another example parallel ensemble model, introduces another element sampling increase diversity models created. Instead using available predictors create tree, sample predictor variables taken. , two forms randomization involved: sampling observations replacement sampling features. diagram random forests appear similar bagging shown Figure 9.2, exception tree created randomized subset predictors.","code":""},{"path":"ensemble-models.html","id":"adaboost","chapter":"9 Ensemble models","heading":"9.2.3 AdaBoost","text":"Boosting process building large, iterative decision tree fitting sequence smaller decision trees, called layers. tree layer consists small number splits. fact, trees can tiny – splits. process boosting shown Figure 9.3.regression trees, process begins creating “weak” tree just splits. called “base” learner. Predictions made initial split residual error computed. residuals information model explain. residual error becomes target second “weak” tree. idea successive trees tend correct errors, resulting accurate model. trees added, existing trees changed revisited, making another example “greedy algorithm.”\nFigure 9.3: Creating boosted tree\nfinal prediction aggregation results. regression trees, one approach creating ensemble prediction simply average predictions multiple levels small intermediate trees. classification trees, majority vote taken produce prediction.early boosting algorithm, called AdaBoost, developed Freund Schapire (Freund Schapire 1996) used categorical target variables (although subsequent extensions continuous targets implemented).Two aspects AdaBoost model make powerful method. First, stage, weak model created errors prediction captured. errors stage become target next stage. Thus, model faced increasingly difficult predictions, prediction accuracy improves. second important aspect AdaBoost stage, errors prediction observations previous stage given weight compared observations accurate. initial stage, weights equal.end process, AdaBoost creates weighted ensemble predictions made , weights proportional accuracy stage.","code":""},{"path":"ensemble-models.html","id":"gradient-boosting-machines","chapter":"9 Ensemble models","heading":"9.2.4 Gradient Boosting Machines","text":"Gradient Boosting Machines (GBM) (Friedman 2001) build approach AdaBoost algorithm incorporating gradient descent optimization. models popular successfully applied several contexts. GBM models quite flexible implemented time--event, Poisson regression, multinomial classification problems. section focus predicting continuous binary target variable.implement model, weights must calculated predictor variable minimize prediction error, called “loss function” context GBM models. continuous responses, typical loss function based square actual minus predicted values. binary responses loss function negative log-likelihood (Natekin Knoll 2013).brute force approach form grid possible values (degree increments) predictor. unguided search process grossly inefficient use building tree model. Instead, calculus-based approach finding local minimum based error actual predicted values target variable. gradient (sort multivariable slopes derivatives) directs changes model parameters steepest slopes toward minimization.minimization process proceeds iteratively small steps. (step size can set user. Large steps reduce computer processing time may miss local minimum. Small steps likely find local minimum cost increased time.) number iterations required depends particular problem step size can range thousands . method guarantee global minimum, algorithms typically incorporate multiple runs data different starting values.Friedman, creator Gradient Boosting Machines, updated model calling Stochastic Gradient Boosting (Friedman 2002). model inserted step prior construction tree sequence took sample data. tree formed sample loss function calculated. next iteration, another random sample full data set selected, . reportedly increased accuracy predictions compared GBM.","code":""},{"path":"ensemble-models.html","id":"xgboost","chapter":"9 Ensemble models","heading":"9.2.5 XGBoost","text":"Tianqi Chen Carlos Guestrin (Chen Guestrin 1996) developed XGBoost (stands “eXtreme Gradient Boosting”) become one popular ensemble modeling techniques recent years, primarily due use Kaggle competitions (Adebayo 2020) considered “state art” many practitioners (Kunapuli 2021).XGBoost flexible customizable, open source runs many operating systems, provides options regularization reduce overfitting, many parameters tune algorithm specific applications. Furthermore, executes faster comparable models.","code":""},{"path":"ensemble-models.html","id":"example-of-ensemble-modeling-for-a-continuous-target","chapter":"9 Ensemble models","heading":"9.3 Example of ensemble modeling for a continuous target","text":"file ToyotaCorolla.csv contains 1,436 records used cars sale summer 2004 Netherlands. addition price automobile, details 30 attributes, weight, age, number kilometers, horsepower, optional accessories power steering, central locking, tow bars.\nFigure 9.4: Workflow comparison OLS Gradient Boosted Trees\nnodes workflow described Table 9.1,\nTable 9.1: Descriptions nodes OLS vs. GBT workflow\nresults comparative analysis shown Table 9.2\nTable 9.2: Comparative performance OLS GBT\nresults show Gradient Boosted Tree performed slightly better ordinary regression, especially terms Root Mean Square Error (RMSE).separate comparison run (workflow shown) 10-fold cross-validation OLS GBT performed. done compare stability models terms producing comparable accuracy. One advantages ensemble models reduced variation different samples data set. important since implies ensemble models perform expected new data sets. results stability assessment shown Table 9.3.\nTable 9.3: Comparison OLS GBT stability\nresults table show stability ensemble model better OLS model 10 replications terms R^2 RMSE. mean R^2 slightly better GBT mean RMSE 4% better GBT.summary, Toyota Corolla data set, overall performance terms accuracy nearly single OLS model versus GBT ensembles, GBT exhibited greater consistency performance. , course, necessarily case different data sets different ensemble models, tests needed actual situation.","code":""},{"path":"naive-bayes.html","id":"naive-bayes","chapter":"10 Naive Bayes","heading":"10 Naive Bayes","text":"","code":""},{"path":"naive-bayes.html","id":"a-thought-problem","chapter":"10 Naive Bayes","heading":"10.1 A thought problem","text":"police officer breathalyzer indicates false drunkenness 5% cases driver sober. However, breathalyzers never fail detect truly drunk person. Suppose given evening 1 1,000 drivers driving alcohol legal limit. traffic checkpoint stop set , drivers selected random, selected drivers required take breathalyzer test.\n\nlimit?\n\nlimit?\nAssume particular driver found legal limit alcohol according breathalyzer. Assume nothing else driver. probability driver really limit?Many people answered probability high 0.95, correct probability 0.02. can proper probability person really drunk estimated? calls Bayes Theorem.Bayes’ theorem formula describes update prior probability event additional evidence made available. Prior probabilities Bayesian perspective based known likelihoods historical data. example random checks drivers, prior 1/1000 .001 driver drunk.estimate probability identifying drunk driver, results breathalyzer can used update probability estimate. revised probability called posterior probability. determine posterior probability, Bayes theorem can used. goal find probability driver drunk given breathalyzer indicated /drunk, can represented :p(drunk|POS), “POS” means breathalyzer indicates driver drunk_p(drunk|POS) = [p(POS|drunk) X p(drunk)] / p(POS)p(POS) = p(POS|drunk) X p(drunk) + p(POS|Sober) X p(Sober)p(drunk) = 0.001p(Sober) = 0.999p(POS|drunk) = 1.00 (breathalyzer 100% accurate person actually drunk)p(POS|sober) = 0.05 (breathalyzer mistakenly reports sober driver drunk 5% time)Given data positive indication breathalyzer test randomly selected driver, probability person drunk?numerator Bayes formula = [p(POS|drunk) X p(drunk)] = [1.00 X 0.001] =0.001The denominator Bayes formula = p(POS|drunk) X p(drunk) + p(POS|Sober) X p(Sober) = 1.0 X 0.001 + 0.05 X 0.999 = 0.001 + .04995 = 0.05095Substituting numerator denominator Bayes theorem yields:p(drunk|POS) = 0.001 / 0.05095 = .0196The framework Bayes theorem can applied supervised analytics problem.","code":""},{"path":"naive-bayes.html","id":"bayes-theorem-applied-to-predictive-analytics","chapter":"10 Naive Bayes","heading":"10.2 Bayes Theorem applied to predictive analytics","text":"\n\nReverend Bayes.\n\nReverend Bayes.\ntest involved sending offer random sample 1,000 current customers. can cast Bayesian model. Using results test, company like predictive model use rest customers.test results 400 customers bought new service, prior probability purchase 0.40. illustrated Figure 10.1 grid representing 1,000 customers test.\nFigure 10.1: Overall results test market.\nprior probability applied entire subscriber base, company expect 400,000 positive responses. process contacting customers via mail, email, telephone offer new service cost, company wanted know way make contacting process efficient. , way increasing positive rate?.turned company data gender age (young, old) subscribers thus information available test market. Using gender, probability purchase can refined. turns 600 female customers test, 300 subscribed new service 400 males, 100 subscribed. probability refining using age, results shown Figure 10.2.\nFigure 10.2: Conditional results test market.\nsimply counting number customers shaded area, posterior probabilities segment calculated.prob (Subscribing | male, young) = 50/ (50+180) = .217prob (Subscribing | male, old) = 50/ (50+120) = .294prob (Subscribing |female, young) = 180/ (180 + 150) = .545prob (Subscribing | female, old) = 120/ (120+150) = .444So, small example shows Bayes model can used predicting classification new observations. classify new case, find observations sample exactly descriptive characteristics. set observations, count number positive negative outcomes apply counting scheme discussed .approach work many predictors. Many practical predictive modeling problems many predictors. , Bayesian idea works theory, always practice.practical example, assume want predict binary target class true false outcomes using 15 binary predictors. Assume needed least 50 observations one resulting cells make reasonable estimate true versus false values binary target. minimum number observations need 50 x 215 = 1.638.400 observations. Even may enough distribution observations may uniform many cells observations.“solution” problem use Naïve Bayes model. word solution quotes problem really solved. Instead, approximation, works well many practical situations, used. approximation based assumption predictor variables operate independently one another. , naive Bayes assumes presence specific feature unrelated presence feature. predictors operate independently, joint probabilities multiple variables can simply estimated product individual probabilities.","code":""},{"path":"naive-bayes.html","id":"illustration-of-naïve-bayes-with-a-toy-data-set","chapter":"10 Naive Bayes","heading":"10.3 Illustration of Naïve Bayes with a “toy” data set","text":"small data set consists 14 observations target variable “play tennis” weather characteristics thought affect decision play play. (“Play Tennis: Simple Dataset Decisions Playing Tennis,” n.d.) observations shown Table 10.1.\nTable 10.1: tennis data set.\nUsing data Table 10.1, following probabilities calculated:\nFigure 10.3: Probabilities Naïve Bayes model.\ncalculations shown Figure 10.3 simply obtained counting. example, obtain conditional probability Sunny given playing, note five observations Sunny conditions. five observations, three indicated Sunny, conditional probability 3/5 = .60. Similar calculations done probabilities table.obtain probabilities playing versus playing Outlook = Sunny, Temperature = Mild, Humidity = High, Wind = Strong, following calcuations made using naive Bayes model:value playing tennis:Prob(Outlook=Sunny Given Playing tennis = Yes) = 0.222 timesProb(Temperature=Mild Given Playing tennis = Yes) = 0.444 timesProb(Humidity=High Given Playing tennis = Yes) = 0.333 timesProb(Wind=Weak Given Playing tennis = Yes) = 0.333 timesProb(Playing tennis = Yes) = 0.644which equals = 0.222 X 0.444 X 0.333 X 0.333 X 0.643 = 0.0071==================================================================value playing tennis:Prob(Outlook=Sunny Given Playing tennis = Yes) XProb(Temperature=Mild Given Playing tennis = Yes) XProb(Humidity=High Given Playing tennis = Yes) XProb(Wind=Weak Given Playing tennis = Yes) XProb(Playing tennis = Yes)equals = 0.600 X 0.400 X 0.800 X 0.600 X 0.357 = 0.0412================================================================== *probability playing = 0.0071 / (0.0071 + 0.0412) = .1465Since probability playing less 0.50, prediction “play”Similar calculations completed 14 observations summary predictions Table 10.2. Thirteen 14 predictions correct using Naïve Bayes model.\nTable 10.2: Prediction accuracy tennis data set using naive Bayes.\n","code":""},{"path":"naive-bayes.html","id":"the-assumption-of-conditional-independence","chapter":"10 Naive Bayes","heading":"10.4 The assumption of conditional independence","text":"Referring Figure 10.4, probability getting three roll die, “red” spinner, heads flip coin? Since three experiments independent, probability simply 1/6 X 1/4 X 1/2 = 1/48 = .0208.\nFigure 10.4: Illustration independence.\nnaïve Bayes analysis assumes effects predictors target class supervised model.","code":""},{"path":"naive-bayes.html","id":"naïve-bayes-with-continuous-predictors","chapter":"10 Naive Bayes","heading":"10.5 Naïve Bayes with continuous predictors","text":"simplicity, previous examples categorical predictors, Naïve Bayes can used continuous predictors. two approaches can used continuous predictors. simple solution discretize continuous variables categories. However, sometimes subjective. instance, categorizing temperature, someone may select 80 degrees cutoff temperature can considered “High,” whereas another person (tropics!) may choose select 90 degrees border “Medium” “High.” subjectivity causes obvious loss information. can still used quick way get going applying naive Bayes classification.Another method represent continuous variables probability density function. Typically, normal Gaussian distribution used, software programs can use distributions. normal distribution convenient since continuous variable can represented using just mean standard deviation. software implementations Naïve Bayes offer choice distribution function, e.g., Poisson.way works demonstrated Figure 10.5. Consider continuous variable, V, predictor categorical variable Y either True False. Observations V data sample grouped according Y values. means standard deviations group computed used form two normal density functions shown Figure 10.5. conditional probabilities Prob(X|Target = False) Prob(X|Target = True) needed naïve Bayes model obtained density functions. method assumes normal distribution usefully represents variable V.\nFigure 10.5: Demonstration working continuous variable naïve Bayes.\n","code":""},{"path":"naive-bayes.html","id":"laplace-smoothing","chapter":"10 Naive Bayes","heading":"10.6 Laplace Smoothing","text":"naïve Bayes algorithm can problem certain situations, especially small sample sizes. problem happens particular value occur frequency greater zero level predictor. case, conditional probability becomes zero since conditional probabilities multiplied chain, causes posterior probabilities included level zero. (actually case tennis example illustrated earlier. condition playing tennis, overcast level weather never occurred.)avoid , Laplace Smoother (Kuhn Johnson 2016) used. several ways incorporate smoother simplest add one every count combination predictor values.","code":""},{"path":"naive-bayes.html","id":"example-using-naïve-bayes-with-churn-data","chapter":"10 Naive Bayes","heading":"10.7 Example using naïve Bayes with churn data","text":"churn data set analyzed using naive Bayes KNIME. KNIME workflow shown Figure 10.6. preprocessing churn data included SMOTE used balance target values training data.\nFigure 10.6: Workflow naïve Bayes using churn data.\nNode descriptions workflow Figure 10.6 Table 10.3.\nTable 10.3: Node descriptions naive Byes churn data.\nevaluation metrics results naïve Bayes shown Table 10.4. comparison, metrics basic decision tree well three ensemble models also shown. naïve Bayes model performed comparably. area ROC curve greater decision trees, lower ensemble models. Interestingly, naïve Bayes traded specificity (lower) sensitivity (higher). Overall, however, naïve Bayes contender classification, perform well complex models.\nTable 10.4: Comparative performance naïve Bayes ensemble models.\n","code":""},{"path":"naive-bayes.html","id":"spam-detection-using-naïve-bayes","chapter":"10 Naive Bayes","heading":"10.8 Spam detection using naïve Bayes","text":"Email provided convenient mode communication used throughout world millions people business personal messages. huge number unsolicited commercial messages people receive daily soon, however, best annoyance worst means deception even criminal activity. proliferation variety unsolicited email messages, now called spam junk mail, led development software programs detect screen emails. Spam filters developed sift email messages separate “ham” “spam.” challenge designing spam filters make algorithm selective enough identify spam flagging legitimate messages. estimated 45% global e-mail traffic spam. (“Spam Statistics Facts,” n.d.)Naïve Bayes used machine learning engine spam filters simplicity, speed, accuracy. Many enhancements basic naïve Bayes model made improve performance algorithms used k-nearest neighbors, support vector machines.(Karimovich Salimbayevich 2020) (Ma Thida 2020)\ndata set 5,556 messages labeled spam ham email messages (“SMS Spam Collection Data Set, UCI Machine Learning Repository,” n.d.) downloaded analyzed using KNIME. Example messages data set Table 10.5:\nTable 10.5: Examples spam ham email messages.\ntext file converted file consisting bag words.11 KNIME workflow Figure 10.7. node descriptions Table . workflow table created 5,572 rows (one message) 12,230 columns indicators terms. (details textual analysis covered detail chapter Text Analytics.)\nFigure 10.7: Pre-processing workflow KNIME SPAM/HAM example.\n\nTable 10.6: Node descriptions naive Bayes SPAM / HAM data workflow.\nbag words data preprocessing step submitted naive Bayes KNIME (Figure 10.8. Descriptions node used run naïve Bayes model Table @ref(tab:naiveBayesHam ).\nFigure 10.8: Naïve Bayes workflow KNIME SPAM/HAM example.\nresults naïve Bayes analysis spam data set show quite good accuracy (98%). shown confusion matrix created test","code":""},{"path":"deep-learning.html","id":"deep-learning","chapter":"11 Deep learning","heading":"11 Deep learning","text":"source images: https://appliedmachinelearning.blog/2018/12/26/tensorflow-tutorial--scratch-building--deep-learning-model--fashion-mnist-dataset-part-1/Earlier versions neural networks first perceptrons “shallow,” composed one input one output layer usually one two hidden layers . computers became powerful 1990s, feasibility neural networks two hidden layers became possible solving complex problems.Around year 2000 limitations number layers reached. Computer power issue. Instead, found back propagation algorithm hampered layers added resulting gradients becoming small (“vanishing gradients”) effecting training. cases, gradients become huge (“exploding gradients”) precision suffered. either situation, deep learning many hidden layers work.\nFigure 11.1: ReLU function\nNetworks three hidden layers considered deep learning models.\n, deep-learning networks distinguished commonplace single-two-hidden-layer neural networks depth. (Figure 11.2, (Vázquez 2018))\nFigure 11.2: Neural nets versus deep learning\nDeep learning getting lots attention lately good reason. ’s achieving results possible . [Sarker (2021)} lists 40 published applications deep learning, including:Detecting corona virus using x-ray images.Predicting air quality cities.Recommendation systems.Speech recognition.Smart parking system.Plant disease detection.","code":""},{"path":"k-nearest-neighbors.html","id":"k-nearest-neighbors","chapter":"12 k Nearest Neighbors","heading":"12 k Nearest Neighbors","text":"","code":""},{"path":"k-nearest-neighbors.html","id":"k-nearest-neighbors-and-memory-based-learning","chapter":"12 k Nearest Neighbors","heading":"12.1 k nearest neighbors and memory-based learning","text":"K nearest neighbors (kNN) another example supervised model, simple supervised model. may simplest, intuitive model used data mining. Despite simplicity model, can work quite well large data sets. also one top 10 data mining tools terms popularity usage. (“Top 10 Machine Learning Algorithms Know 2021,” n.d.)kNN example family algorithms known instance-based memory-based learning classify new objects similarity previously known objects. kNN require many assumptions input predictor data anything error distributions. Formally, non-parametric model. parametric models (multiple regression), must make assumptions distribution error term estimates made parameters distribution. considerations involved kNN.training phase used determine best value k (number “neighbors” used compute similarity), otherwise training needed. can directly applied data. data set, simply apply kNN. important disadvantage kNN model must run data available time new observation needs classified. can time consuming large data sets. logistic regression OLS, save model apply new data, much faster. Since “model” per se K-nearest neighbors, model essentially created time analysis needed.","code":""},{"path":"k-nearest-neighbors.html","id":"typical-applications","chapter":"12 k Nearest Neighbors","heading":"12.2 Typical applications","text":"look typical applications, list much like predictive models logistic regression decision trees, :Flagging fraudulent insurance claims.Predicting customer response promotional offers.Selecting effective treatment medical condition.Classifying free-text responses.Recommending next offer customer retail settings.Searching similar documents.Recommender systems.","code":""},{"path":"k-nearest-neighbors.html","id":"what-is-knn","chapter":"12 k Nearest Neighbors","heading":"12.3 What is kNN?","text":"KNN method classifying objects based similarity. called “lazy” algorithm, means use training data points generalization contrasted “eager” algorithms. differences described Table 9.1. words, explicit training phase, minimal. lazy algorithms – especially kNN – make decisions based entire data set. distinctions lazy eager learners shown .lazy learners\ndata stored, learned .\nClassifications made soon new observation received.\nmodel richer since rely single pre-specified model.\nTime classify new observations can considerable large data sets.\nLearning time non-existent (except determining optimal value k) need minor preprocessing.\ndata stored, learned .Classifications made soon new observation received.model richer since rely single pre-specified model.Time classify new observations can considerable large data sets.Learning time non-existent (except determining optimal value k) need minor preprocessing.eager learners\nmodel developed (learned) based training data.\nmodel used classify new observations received.\nmodel depends upon single function derived learning phase.\nClassification new observations fast.\nLearning time can considerable.\nmodel developed (learned) based training data.model used classify new observations received.model depends upon single function derived learning phase.Classification new observations fast.Learning time can considerable.algorithm works like . labeled data set known categorical target vector, Y (may binary multichotomous) matrix X p potential predictors features used. case considered point multidimensional feature space. allows computation “distances” among points (cases) based locations (values) feature space.KNN used classify new observation described p variables (X1, X2, …, Xp). , algorithm computes distance new observation every observation data set. Euclidean distance typically used, metrics absolute value differences, squared Euclidean distance, Jacard distance, Manhattan distance, cosine distance also used. cases important standardize X variables prior determining distances avoid variables largest numerical values dominating computed distances.distances new observation row data set computed ranked smallest largest. k smallest distances (“nearest neighbors”) selected. assumptions form relationship Y Xi’s made; parameters estimated. majority “vote” k neighbors taken category smallest distance selected predicted class. k – number nearest points neighbors consider making assignment category – usually odd number ties formed; even number set k, random choices Y class made ties.) Since majority used make predictions, kNN can applied targets two two possible outcomes.trade-implicit . minimal training phase used determine best k, deployment kNN efficient terms processing time memory requirements. time needed data points might take part determining prediction. Furthermore, data must stored available deployment.","code":""},{"path":"k-nearest-neighbors.html","id":"a-two-dimensional-graphic-example-of-knn","chapter":"12 k Nearest Neighbors","heading":"12.4 A two-dimensional graphic example of kNN","text":"simple example used illustrate kNN works. Figure 9.1 shows plot characteristics different types homes. two predictors area square feet number rooms. variables standardized zero mean unit standard deviation.Toward center chart “unknown” type home. area number rooms unknown standardized using mean standard deviation computed known data. distances unknown every data point original set represented arrows. three closest known observations circled, assuming example k = 3. Two closest homes flats one apartment. Using majority rule, unknown classified flat. (See Figure 12.1)\nFigure 12.1: Graphical illustration kNN.\n","code":""},{"path":"k-nearest-neighbors.html","id":"example-of-knn-diagnosing-heart-disease","chapter":"12 k Nearest Neighbors","heading":"12.5 Example of kNN: Diagnosing heart disease","text":"database part larger study factors associated heart disease collected Cleveland Clinic. data set contains test results 303 patients measured 13 attributes plus target variable indicating presence absence heart disease. data set used several published studies machine learning. variables listed Figure 12.2 along indication numeric categorical. categorical variables, levels shown well.\nFigure 12.2: Variables Cleveland Heart Disease study.\nKNIME used determine best value k finding accuracy values k 1 20. plot Accuracy versus k shown Figure 12.3, indicates best k 7.\nFigure 12.3: Accuracy versus k heart dataN.\nKNIME workflow shown Figure 12.4. Descriptions node workflow given Table 12.1.\nFigure 12.4: kNN workflow heart disease data.\n\nTable 12.1: Workflow nodes kNN heart data\n","code":""},{"path":"k-nearest-neighbors.html","id":"results","chapter":"12 k Nearest Neighbors","heading":"12.5.1 Results","text":"confusion matrix kNN model k = 7 Table 12.2.\nTable 12.2: Confusion matrix predicting heart disease kNN\nAccuracy measures kNN analysis Table 12.3\nTable 12.3: Accuracy metrics kNN heart disease data\n","code":""},{"path":"k-nearest-neighbors.html","id":"knn-for-continuous-targets","chapter":"12 k Nearest Neighbors","heading":"12.6 kNN for continuous targets","text":"kNN primarily method classification, can also used continuous target variables much like ordinary least squares (OLS) regression. KNIME include node kNN regression, small R Snippet created use package FNN. One advantage kNN regression non-linear relationships can easily captured. Ordinary regression requires transformations /adding predictors capture non-linearities.simple data set single predictor (X) continuous target(Y) created. scatterplot data shown Figure 12.5. Note relationship non-linear slightly concave upward.\nFigure 12.5: Simulated data non-linear relationship Y X.\nKNIME workflow created, shown Figure 12.6, compare kNN OLS. small demonstration data, test data subset created.\nFigure 12.6: KNIME workflow compare kNN regression OLS.\nKNIME node perform kNN regression, R code inserted Node 7 contains R Snippet. Note R code customized new problems.library(FNN)mydata <- .data.frame(knime.)TrainData <- .data.frame(mydata[,1])TrainTarget <- mydata[,2]TestData <- .data.frame(mydata[,1])YKNN = knn.reg(train = TrainData, test = TestData, y = TrainTarget, k = 3)knime.<- .data.frame(cbind(mydata,YKNN$pred))metrics two models :\nTable 12.4: Comparison kNN OLS non-linear data\nmetrics show kNN accurately captured relationship data. OLS improved creating polynomial model, course, point kNN regression require model specified priori.Plots predicted Y (y-axis) actual Y values (x-axis) show Figures 12.7 12.8 two models. Note OLS model capture non-linearity created downward concave result.\nFigure 12.7: OLS results.\n\nFigure 12.8: kNN results.\n","code":""},{"path":"k-nearest-neighbors.html","id":"knn-for-multiclass-target-variables","chapter":"12 k Nearest Neighbors","heading":"12.7 kNN for multiclass target variables","text":"KNN also effective target variables two classes. example data set obtained UCI Machine Learning Repository illustrate kNN multiclass target. data set 214 observations, 6-level categorical target, Type glass, nine continuous predictors. kNN workflow KNIME shown Figure @[fig:kNN_GlassWorkflow]. model run 50/50 split training test rows k set two, found result highest accuracy.\n(#fig:kNN_GlassWorkflow)KNIME workflow kNN analysis glass data.\nconfusion matrix predicting type glass test data set shown Table @ref[tab:GlassConfusionMatrix]. accuracy varies type glass overall accuracy just 73%. may seem good, predicting six classes small data set easy task model.\nTable 12.5: Confusion matrix glass data\n","code":""},{"path":"tree-models.html","id":"tree-models","chapter":"13 Tree models","heading":"13 Tree models","text":"Decision trees (aka tree-based models) commonly used data mining perform predictive analytics, typically single categorical dependent variable multiple predictor variables (can continuous categorical). two major types decision trees: classification trees regression trees. Classification trees discussed first section regression trees discussed second section. Decision tree algorithms “automatic” independent variables selected searching optimal splits using measure “purity” effect size.","code":""},{"path":"tree-models.html","id":"classification-trees","chapter":"13 Tree models","heading":"13.1 Classification trees","text":"example, results classification tree inform customer targeting marketing campaign. Consider hypothetical results tree based four predictor variables:customer’s income greater less $70k?old customer?customer college graduate?customer male female?classification tree based past data purchases might produce tree-like structure shown Figure 13.1.Notice branches terminate variables considered. splitting branch lead useful differences.\nFigure 13.1: Initial calculations regression tree.\nNote splitting income, next variable selected splitting differs depending income level. income less equal $70k, next split age. income greater $70k, split education. known interaction effect. Finally, note splits can either continuous variables age nominal variables gender. process starts root node, represents entire data set. process proceeds creating branches data split sub nodes. final splits result leaf terminal nodes.hypothetical tree shown Figure 13.1, series rules produced:Income ≤ $70,000 Age ≤ 30 probability purchase = 14%.Income ≤ $70,000 Age > 30 Female, probability purchase = 40%.Income ≤ $70,000 Age > 30 Male, probability purchase = 63%.Income > $70,000 college, probability purchase = 38%.Income > $70,000 College graduate, probability purchase = 70%.classification tree inferential technique suitable statistical hypothesis testing. One way think classification trees recursively split data smaller smaller branches increasingly “pure” terms target variable. find split, program examines input variables selects one stage effective according criteria algorithm.search process involved forming tree violates logical premises classical statistical testing since data used inform splits branches. However, many observations many variables cases well-defined theory exists, classification trees can help researcher “discover” relationships can tested later different sample.common uses classification trees :Market segmentation – identifying segments likely purchase.Stratification – dividing cases high/medium/low risk, example.Prediction – creating rules use predict future outcomes.Data reduction variable screening – screening many variables identify best prospects.Interaction detection – finding variables effects differ according levels variables.Category merging – recoding variables large numbers levels fewer categories without substantial lose predictive information. one attraction classification trees: technique works without much “thinking.”","code":""},{"path":"tree-models.html","id":"forming-classification-trees","chapter":"13 Tree models","heading":"13.2 Forming classification trees","text":"exponentially many possible classification trees given set attributes. number large continuous predictors can split many ways predictor can used tree built. Since usually impossible examine possible tree structures problem, algorithm used grow tree reasonably accurate instead optimal. Classification trees known “greedy algorithms” since use strategy proceeding stage stage split data made, algorithm go back making additional splits check previous splits. Thus, locally optimum selections made stage. result usually good model, one necessarily optimal.However, dangers costs associated approach. put long list variables program selects best predictors (best point splitting).","code":""},{"path":"tree-models.html","id":"varieties-of-classification-tree-algorithms","chapter":"13 Tree models","heading":"13.3 Varieties of classification tree algorithms","text":"wide variety different models techniques developed fall umbrella term classification trees. Models classification trees differ terms branches tree split main trunk, terms stopping rules, types variables can used, provided output, etc. Several algorithms available classification trees including CART, C5.0, CHAID. means give answers given problem. One reason programs use different criteria select parent nodes split, shown Table 13.1.\nTable 13.1: Characteristics three classification tree algorithms.\nClassification trees around long time, recently frequently discussed derogatory terms. One first models, AID, called substitute thinking. AID acronym “automatic interaction detection.”\nClassification trees remain controversial, researchers claim classification trees used. automatically combing data sets search relationships, models potential find spurious associations may appear plausible artifacts due randomness.much true use classification trees small samples develop training testing subsets. criticism due past applications small data sets used. era plentiful data, concerns longer important. Data mining, , large data sets.continuous predictor variables, possible splits considered. Thus, n distinct values predictor, n-1 potential splits considered. nominal predictor variables, number possible splits two groups depends upon number distinct categories. Table 13.2 gives examples number possible splits function distinct categories.\nTable 13.2: number possible binary splits number categories nominal predictor.\ngeneral, number possible splits function categories given Sterling Numbers Second Kind (“Wolfram MathWord: Stirling Numbers Second Kind,” n.d.).","code":""},{"path":"tree-models.html","id":"criteria-for-splitting-and-growing-a-tree","chapter":"13 Tree models","heading":"13.4 Criteria for splitting and growing a tree","text":"noted , various approaches developed selecting node split forming classification tree. Three approaches splitting Gini index, entropy, chi-square. can measure “purity” node.data set three possible predictors: Feature 1, Feature 2, Feature 3. , first split three predictors.","code":""},{"path":"tree-models.html","id":"the-gini-index","chapter":"13 Tree models","heading":"13.4.1 The Gini index","text":"Gini index “node purity,” \\(G_{}\\) computed using equation (13.1).\\[\\begin{equation}\n    \\ \\mathit{G_{}}\\; = 1 - (p_{0}^2\\; + p_{1}^2\\;)\n\\tag{13.1}\n\\end{equation}\\]\\(p_{0}\\) proportion cases node level “0” \\(p_{1}\\) proportion level “1.” Figure 13.2 shows Gini index \\(p_{0}\\) \\(p_{1}\\) varied. maximum value index .5 0 either cases equal 0 equal 1.\nFigure 13.2: Gini index function p0 p1.\nalgorithms consider splitting node two child nodes others, CHAID can create multi-category nodes. example, binary splits data set considered. overall Gini index binary split computed weighted average Gini values two possible branches.consider splitting node two, \\(G_{}\\) computed resulting nodes, labeled \\(G_{\\_Left}\\;\\) \\(G_{\\_Right}\\;\\). total Gini index calculated weighted average \\(G_{\\_Left}\\;\\) \\(G_{\\_Right}\\;\\), weights proportions cases left right nodes, given \\(w_{n\\_Left}\\) \\(w_{n\\_Right}\\) . , Gini index split calculated equation (13.2).\\[\\begin{equation}\n    \\ \\mathit{G_{\\_split}\\; = w_{n\\_Left} \\times  G_{\\_Left}\\; + w_{n\\_Right} \\times  G_{\\_Right}\\; }\\;\n\\tag{13.2}\n\\end{equation}\\]","code":""},{"path":"tree-models.html","id":"information-gain","chapter":"13 Tree models","heading":"13.4.2 Information Gain","text":"Information node measure impurity, higher values indicating greater impurity.\nexpected information node binary target variable, IInfo_A, computed using equation (13.3).\\[\\begin{equation}\n    \\ \\mathit{I_{Info\\_A}\\; = \\sum_{=1}^{2}   p_{}\\; \\times \\log_{2} p_{}\\;}\\;\n\\tag{13.3}\n\\end{equation}\\]\\(p_{}\\) proportion cases node level (either 0 1). Figure 13.3 shows expected information varies \\(p_{0}\\) \\(p_{1}\\). maximum value index 1.0 0 either cases = 0 cases = 1.\nFigure 13.3: Expected information function p0 p1.\nselect node split, information gain computed, sum information values parent node minus sum expected information values two child nodes.consider splitting node two, \\(I_{}\\) computed resulting nodes, labeled \\(I_{\\_Left}\\;\\) \\(I_{\\_Right}\\;\\). information contained two child nodes calculated weighted average \\(I_{\\_Left}\\;\\) \\(I_{\\_Right}\\;\\), weights proportions cases left right nodes, given \\(w_{n\\_Left}\\) \\(w_{n\\_Right}\\). , information gain split calculated equation (13.4).\\[\\begin{equation}\n    \\ \\mathit{I_{\\_split}\\; = w_{n\\_Left} \\times  I_{\\_Left}\\; + w_{n\\_Right} \\times  I_{\\_Right}\\; }\\;\n\\tag{13.4}\n\\end{equation}\\]","code":""},{"path":"tree-models.html","id":"chi-square-as-a-splitting-criterion","chapter":"13 Tree models","heading":"13.4.3 Chi-square as a splitting criterion","text":"approach splitting nodes uses chi-square statistic selecting parent node split. case preferred split based split increases chi-square value , since two nodes “pure” node terms 1’s 0’s sum chi-squares greater nodes balanced. formula used stage classification tree target binary (labeled “0” “1”) binary splits considered equation (13.5).\\[\\begin{equation}\n    \\ \\mathit{\\text{Chi-square}{_A}\\;  = \\sum_{=1}^{2}   \\frac{(a_i\\; - e_i\\;)^2 }{e_i}\\;}\\;\n\\tag{13.5}\n\\end{equation}\\]\\(a_{}\\) \\(e_{}\\) actual number expected 0’s node \\(a_{1}\\) \\(e_{1}\\) actual number expected 1’s node. Figure 13.4 shows shape chi-square value varies different proportions \\(p_{0}\\) \\(p_{1}\\).\nFigure 13.4: Chi-square function p0 p1.\nNote node increases “purity” terms \\(p_{0}\\) \\(p_{1}\\), chi-square increases. Therefore, parent node highest sum chi-squares two child nodes selected splitting.illustrate, consider “toy” data \nTable 13.3. data consists 20 observations, 9 zero responses 11 responses 1.\nTable 13.3: Toy data set\nGini index, information gain, chi-square computed candidate splits Features 1, 2, 3. three criteria (shown Table 13.4 led initial split Feature 3, always case.\nTable 13.4: Criteria initial splits..\n.\ntree far shown Figure Figure 13.5:\nFigure 13.5: Split #1\nContinuing just Gini index, next possible splits shown Table 13.5.\nTable 13.5: Criteria second splits.\nbest split stage split Node 3 Feature 2. , best split split Node 2 Feature 1. resulting tree shown 13.6. Note best split Node 2 based Feature_1 best split Node 3 based Feature 2. splits made example.\nFigure 13.6: Final tree\n","code":""},{"path":"tree-models.html","id":"overfitting","chapter":"13 Tree models","heading":"13.5 Overfitting","text":"Trees can grow complexity susceptible overfitting data. Bramer defines overfitting follows:classification algorithm said overfit training data generates classification tree … depends much irrelevant features training instances, result performs well training data relatively poorly unseen instances.\n\"Realistically, overfitting always occur greater lesser extent simply training set contain possible instances. becomes problem classification accuracy unseen instances significantly downgraded. (Bramer 2007)Figure 13.7 shows conceptually increasing size classification tree allowing modes usually increase accuracy training data, eventually increases error test unseen data. Overfitting training data fits model errors idiosyncrasies training data present data sets.\nFigure 13.7: overfitting can degrade accuracy.\ntwo general approaches avoid overfitting:Simply avoid growing large trees – providing stopping rule minimum number observations node maximum number splits.Grow large tree cut branches afterwards, known pruning. full tree grown (early stopping might additionally used), split examined determine brings reliable improvement.","code":""},{"path":"tree-models.html","id":"example-of-a-classification-tree","chapter":"13 Tree models","heading":"13.6 Example of a classification tree","text":"Customer churn occurs customer (player, subscriber, user, etc.) ceases relationship company. full cost customer churn includes lost revenue well marketing costs involved replacing customers new ones. Reducing customer churn key business goal nearly every online business almost always difficult expensive acquire new customer retain current paying customer.ability predict particular customer high risk churning, still time something , represents important potential increasing revenue profit.case telecom companies, customers may cancel many reasons, including poor service, availability specific hardware, price. Identifying potential churners quit can first step efforts lower churn rate. makes financial sense offer incentives potential churners customers going remain.Analytic techniques can used develop predictive models identify likely churners based customer characteristics behaviors.example, data set, TelcoChurn5000.csv, consisting 5,000 customers telecom provider available variables shown Table 13.6.\nTable 13.6: Variables TelcoChurn5000.csv data set.\nclassification tree developed KNIME using workflow shown Figure 13.8.\nFigure 13.8: KNIME workflow Churn data.\nnodes workflow described Table 13.7,\nTable 13.7: Descriptions nodes churn workflow.\nperformance results Training Test summarized Excel file shown Table 13.8.\nTable 13.8: Performance measures classification tree.\nclassification tree performed quite well data set, comparable\nperformance Training Test data shown Tables 13.9 13.10.\nTable 13.9: Confusion matrix Churn Training data.\n\nTable 13.10: Confusion matrix Churn Test data.\n","code":""},{"path":"tree-models.html","id":"regression-trees","chapter":"13 Tree models","heading":"13.7 Regression trees","text":"Decision trees can also used predict continuous target variables. applications known regression trees. analysis much like case categorical target variables: model produces series splits using selected predictor variables. regression trees, much like ordinary regression, can continuous nominal predictors. However, assumptions associated linear regression regarding error distributions , really relevant regression trees classical statistical technique.","code":""},{"path":"tree-models.html","id":"how-regression-trees-work","chapter":"13 Tree models","heading":"13.7.1 How regression trees work","text":"Regression trees operate successively dividing data set smaller smaller groups homogeneous respect target variable. groups nodes node lower tree homogeneous terms target variable nodes higher tree. model starts predictors examines possible predictor turn select best variable initial split.process building regression tree illustrated using simple data set (DemoRegressionTrees.csv) consisting 100 observations home prices continuous target variable area square feet (either 1,000 2,000) quality (either high average) predictors. first five last five rows shown Table 13.11.\nTable 13.11: Performance measures classification tree.\ntree building process uses criterion minimizing sum squared errors binary splits predictors made. initial sum squares (\\(SST_{}\\)) given equation (13.6).\\[\\begin{equation}\n    \\ \\mathit{\\text{SST}{_I}\\;  = \\sum_{=1}^{n} (y_{}\\; + \\bar{y}\\;)^2\\; = 502,301}\\;\n\\tag{13.6}\n\\end{equation}\\]n = number observations, \\(y_{}\\) = price observation \\(\\bar{y}\\;\\) = mean value prices.Next, splits considered area square feet quality. split made, sum squares branch split computed added together. sum squares splitting subtracted \\(SST_{}\\) split greatest reduction sum squares selected.sum squares binary splits given (13.7).\\[\\begin{equation}\n    \\ \\mathit{\\text{SST}_{binary}\\;  = \\sum_{=1}^{n_L} (y_{}\\; + \\bar{y}_L\\;)^2\\; + \\sum_{=1}^{n_R} (y_{}\\; + \\bar{y}_R\\;)^2\\;}\n    \\tag{13.7}\n\\end{equation}\\]\\(n_{j}\\) = number observations jth node\nsplit, \\(\\bar{y}_L\\;)\\;\\) = mean value prices jth node\nj = L left node j = R right node.results calculations shown Figure 13.9.\nFigure 13.9: Initial calculations regression tree.\nSplitting area produced greatest reduction sum squares therefore split made area.larger problem, recursive process used, predictor considered. potential split continuous predictor must calculated, variable k distinct values, k-1 splits considered.several different algorithms regression trees, commonly used approach CART method. depth tree can controlled programs. (J. F. Breiman L. Stone 1984)predictions made using terminal nodes bottom tree. prediction estimates target variable equal average value observations terminal node.Simple regression trees limitation, see example. limitation stems model structure estimates target made average value terminal node. Therefore, terminal nodes, different prediction values made. happens even though original target variable many, perhaps hundreds thousands different values target continuous.predictions said limited cardinality. Cardinality just number distinct values set predicted values. can limit R2 predictive accuracy regression trees.","code":""},{"path":"tree-models.html","id":"example-predicting-home-prices","chapter":"13 Tree models","heading":"13.7.2 Example: Predicting home prices","text":"Next, example based predicting home prices. data set 522 observations file RealEstatePrices.csv. variables data listed Table 13.12.\nTable 13.12: Performance measures classification tree.\nworkflow (Figure 13.10) created KNIME predict Sales Price Test data using regression trees , comparison, ordinary linear regression.\nFigure 13.10: KNIME workflow Real Estate Example.\nnodes workflow example described Table 13.13.\nTable 13.13: Node descriptions Real Estate workflow.\nresults two analyses shown Table 13.14. Ordinary least squares regression performed slightly better example. One reason may limitation predicted values using regression trees. many distinct predictive values number terminal nodes. original Selling Price variable 131 distinct values. predicted values regression tree, 30 distinct values, 157 distinct values predictions using ordinary regression.\nTable 13.14: Node descriptions Real Estate workflow.\nSince results regression tree ordinary regression differed, exploration combining two results conducted. predictions two models averaged compared actual Selling Price data. performance metrics averaged predictions shown Table 13.15.\nTable 13.15: Performance metrics averaged predictions.\nmetrics indicate improved accuracy. particular root mean squared error (RMSE) reduced 50,101 compared RMSE regression tree 56,473 52,811 OLS. RMSE averaged model 5% lower OLS mode. Whether improvement important, illustrate combining prediction estimates can result better performance. similar effect one discussed chapter ensemble models.","code":""},{"path":"tree-models.html","id":"strengths-and-weaknesses","chapter":"13 Tree models","heading":"13.8 Strengths and weaknesses","text":"Decision trees widely used data mining virtually every technique, strengths weaknesses.Strengths decision treesInterpretation usually straightforward easy demonstrate explain.underlying assumptions must met.results displayed tree-like structure, intuitively appealing. Rules generated transparent.Interactions among predictor variables can identified.Outliers missing values can handled without problems (algorithms).Non-linear relationships handled without problems.Predictor variable selection automatic.Binary, categorical, ordinal, interval target predictor variables can used.Weaknesses decision treesSlight changes data set can produce dramatically different results.Large datasets needed.Careful validation required avoid -fitting.data snooping process can misleading.bias toward selecting categorical predictors many levels.Considerable judgment experimentation may needed develop suitable result.","code":""},{"path":"neural-networks.html","id":"neural-networks","chapter":"14 Neural networks","heading":"14 Neural networks","text":"Artificial neural networks class extremely powerful techniques become quite popular recent years. reason can produce accurate predictions used supervised data mining applications.networks flexible algorithms can applied different types modeling including supervised unsupervised problems.Neural networks can used place conjunction logistic regression decision trees categorical dependent variable. Neural networks flexible – also work continuous dependent variable, can used regression-type setting.applications regression, logit, decision trees, techniques might used, neural nets can evolve much complex, flexible, potentially accurate models. downside models often difficult interpret explain.Neural nets especially effective many input variables, non-linear relationships target variable. ’s fascinating neural nets model structure needs specified terms number nodes hidden layers. analyst concerned non-linearities /interactions among predictors.sense, using neural nets, computer learns data. specific model specified regression models. Instead, process works like : “’s data, complicated net can . Develop predictive model.” statistical models rather powerful computer programs. Thus, assumptions made normality, linearity, etc. led concept “machine learning.”flexibility complexity neural net models source attractiveness neural nets well part challenges effectively using . Neural nets work best many observations training, validation, test subsets can formed.Neural nets can actually easy apply use modern software. many software programs available.\nresulting models using neural networks can quite complicated even though one sense just combination non-linear regression models. ’s combination many simple models makes artificial neural nets complicated.","code":""},{"path":"neural-networks.html","id":"what-are-artificial-neural-networks","chapter":"14 Neural networks","heading":"14.1 What are artificial neural networks?","text":"“artificial” adjective used models inspired attempts simulate biological neural systems.\nfirst neural networks originally developed data analysts, computer experts, statisticians. original research human brain activity led development computer models.artificial neural net works way, , although number elements even complicated neural networks nowhere near billions human brain. (human brain thought contain 100 billion neurons.) , artificial neural nets used data mining nowhere near proficient complicated human brain. Despite , much terminology persists original research done human brain, terminology used terms neurons, learning, nodes, activation functions, synapses machine learning neural networks.","code":""},{"path":"neural-networks.html","id":"human-neurons-to-mathematical-models","chapter":"14 Neural networks","heading":"14.1.1 Human neurons to mathematical models","text":"Figure 14.1 (Source: (Unal Başçiftci 2021)) simplified model typical human neuron. basic terms, neuron works follows. dendrites receive chemical electrical signals neurons. soma (nucleus) processes information dendrites creates output transmitted axon. axon connected via synapses neurons. many neurons combined network, result powerful capabilities human mind.\nFigure 14.1: Simplified model human neural net\n1943 McCulloch Pitts, two neurophysiologists Yale University, interested understanding anatomy functioning human brain (McCulloch Pitts 1943). proposed mathematical model explain human neurons worked make decisions create insights. hypothesized human brain works using millions relatively simple elements, essentially -switches.idea complicated set behaviors, evidenced human brain, can arise set relatively simple units enough acting concert sequence. McCulloch Pitts proposed neurons activated binary manner - either “fire” “-fire.” basic element model can stated mathematically :\\[\\begin{equation}\n  {S} = \\sum_{=1}^{n} I_{} W_{}   \n \\end{equation}\\]\\[y(S) =\n\\begin{cases}\n1, & \\text{} S\\geq T\\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\\(I_1\\), \\(I_2\\),…,\\(I_n\\) binary input values \\(W_1\\), \\(W_2\\),…, \\(W_n\\) weights associated input, \\(S\\) weighted sum inputs \\(T\\) threshold value neuron activation.","code":""},{"path":"neural-networks.html","id":"activation-functions","chapter":"14 Neural networks","heading":"14.1.2 Activation functions","text":"weighted sum submitted activation function, translates sum value based range function. (Figure 14.2). possible linear activation function, activation functions non-linear. Using linear activations essentially re-create ordinary regression using neural networks.\nFigure 14.2: Examples activation functions used neural nets\nNon-linear activation functions enable neural networks model complex relationships inputs outputs. fact, neural nets can approximate function desired degree accuracy. known “universal approximation theorem” (Nielsen 2019).12","code":""},{"path":"neural-networks.html","id":"the-road-to-machine-learning-with-neural-nets","chapter":"14 Neural networks","heading":"14.2 The road to machine learning with neural nets","text":"Beginning 1950s digital computers became available, computer scientists became aware perceptrons based work Yale University professors.Scientists began trying teach computers learn. One example problems solved early neural networks balance broom standing upright moving car controlling motions cart back forth. broom starts falling left, cart learns move left keep room upright. interesting, promises early work realized.Scientists began trying teach computers learn. One example problems solved early neural networks balance broom standing upright moving car controlling motions cart back forth. broom starts falling left, cart learns move left keep room upright. interesting, promises early work realized.excitement early 1950s gave way disillusionment late 1960s. disillusionment stemmed publication book Marvin Minsky Seymour Papert 1969 showed basic problems perceptrons (Minsky Papert 1969). example, perceptron model -called XOR (exclusive ) problem. (Table 14.1.) effect Minsky Papert’s paper funding research neural nets dried 10 years.\nTable 14.1: XOR problem\nearly 1980s, however, researchers devised way incorporate multiple layers perceptrons models multiple layering enabled models become extremely flexible. flurry research developed.13","code":""},{"path":"neural-networks.html","id":"example-of-a-neural-network","chapter":"14 Neural networks","heading":"14.3 Example of a neural network","text":"class Iris data set consists 150 observations 4 measured attributes (sepal length, sepal , petal length, petal.width) one type Iris target (setosa, versicolor, viginica). data divided randomly training set (60%) validation set ((40%). neural network fitted training data single hidden layer two nodes.resultant network shown 19 parameter estimates Figure 14.3. circles “1” represent constant terms.\nFigure 14.3: Neural net Iris data\nresults shown two confusion matrices, one training data (Table 14.2) one validation data(Table 14.3). errors made training data just two validation data. Reduced accuracy validation data expected since data used create model.\nTable 14.2: Neural net results Iris training data\n\nTable 14.3: Neural net results Iris validation data\n","code":""},{"path":"neural-networks.html","id":"training-a-neural-net","chapter":"14 Neural networks","heading":"14.4 Training a neural net","text":"several ways developed estimating weights neural net. don’t say estimation talking neural nets. say training. use training data adjust weights model.Probably common structure neural network -called feed-forward model. means network trained, input data flows model one direction toward output target. feedback built model. (confused estimation technique called back-propagation, discussed .)Training neural net analogous finding coefficients best fit regression model. One key difference, however, regression single best fitting linear model optimizes fit set training observations. equivalent method calculating best set weights neural network. Instead, optimization model routine used minimize error function, average squared error. doesn’t guarantee optimal result, instead looks good result. (possible get local optima.)Probably common training method called back propagation method. starts randomly assigning set weights model calculates value target. provides initial model, likely good., error initial model calculated subtracting predicted target value calculated neural net actual value target. error fed back model weights adjusted try minimize error. name back propagation suggested errors sent back network.adjustments made model weights determined strategy called gradient descent. gradient partial derivative function one input variable. gradient measure much direction output function changes small changes inputs. sizes changes gradient set learning rate. Setting learning rate high may cause algorithm miss optimum. Setting learning rate low likely lead optimum cost excessive computer time.learning process repeated many times repeated criterion reached, pre-set value computer processing time, specified maximum number iterations, error associated weights negligible.can slow process terms number iterations required, modern computers many analyses can completed matter seconds. However, problems hours. actual mechanisms quite sophisticated, developed period mathematicians computer scientists.","code":""},{"path":"neural-networks.html","id":"considerations-in-using-neural-nets","chapter":"14 Neural networks","heading":"14.5 Considerations in using neural nets","text":"","code":""},{"path":"neural-networks.html","id":"missing-data","chapter":"14 Neural networks","heading":"14.5.1 Missing data","text":"Neural nets handle missing data, imputation values must performed data values missing.","code":""},{"path":"neural-networks.html","id":"representative-data","chapter":"14 Neural networks","heading":"14.5.2 Representative data","text":"training, verification test data must representative underlying model. old computer science adage “garbage , garbage ” apply strongly neural modeling. training data representative, model’s worth best compromised. worst, may useless. worth spelling kind problems can corrupt training set:","code":""},{"path":"neural-networks.html","id":"all-eventualities-must-be-covered","chapter":"14 Neural networks","heading":"14.5.3 All eventualities must be covered","text":"neural network can learn cases present. people incomes $100,000 per year might bad credit risks training data include anyone incomes $40,000 per year, expect model make correct decisions previously unseen cases. Extrapolation dangerous model, types neural network may make particularly poor predictions circumstances.network learns easiest features can. classic (possibly apocryphal) illustration vision project designed automatically recognize tanks. network trained hundred pictures including tanks, hundred . achieves perfect 100% score. tested new data, proves hopeless. reason? pictures tanks taken dark, rainy days, pictures without sunny days. network learns distinguish (trivial matter ) differences overall light intensity. work, network need training cases including weather lighting conditions expected operate - mention types terrain, angles shot, distances.","code":""},{"path":"neural-networks.html","id":"unbalanced-data-sets","chapter":"14 Neural networks","heading":"14.5.4 Unbalanced data sets","text":"Since network minimizes overall error, proportion types data set critical. network trained data set 900 good cases 100 bad bias decision towards good cases, allows algorithm lower overall error (much heavily influenced good cases). representation good bad cases different real population, network’s decisions may wrong. good example disease diagnosis. Perhaps 90% patients routinely tested clear disease. network trained available data set 90/10 split. used diagnosis patients complaining specific problems, likelihood disease 50/50. network react -cautiously fail recognize disease unhealthy patients.contrast, trained “complaints” data, tested “routine” data, network may raise high number false positives. circumstances, data set may need crafted take account distribution data (e.g., replicate less numerous cases, remove numerous cases), network’s decisions modified inclusion loss matrix (Bishop, 1995). Often, best approach ensure even representation different cases, interpret network’s decisions accordingly.","code":""},{"path":"neural-networks.html","id":"the-overfitting-problem","chapter":"14 Neural networks","heading":"14.5.5 The overfitting problem","text":"data mining techniques neural net model trained separate set data tested validated separate data sets. particularly important using neural nets.Neural nets can predict well. , given enough flexibility several hidden layers large number nodes, neural net model can developed perfectly predict target training data.problem fitting like generalize well. words, take model fit perfectly training data, may predict validation testing data sets well . overlearning. sufficient iterations enough nodes, neural nets can even fit random data.illustrate , Iris data set used. time, however, predictor variables ordered randomly. meant predictors (sepal length, sepal , petal length, petal.width) target (setosa, versicolor, viginica) longer correctly matched time larger neural net specified two hidden layers, 10 nodes.\nTable 14.4: Neural net results training data: Randomized Iris data\nsecond confusion matrix, applied model validation data, showed model overfit. (Table 14.5) model accurately predict new data.\nTable 14.5: Neural net results validation data: Randomized Iris data\n","code":""},{"path":"neural-networks.html","id":"neural-network-example","chapter":"14 Neural networks","heading":"14.6 Neural network example","text":"German credit data used illustrate neural network. data set contains 1,000 observations 20 predictors binary target: “Credit risk.” study credit card defaults Taiwan available Machine Learning source Repository UCI (Gromping 2019b) detailed report corrections provided (Gromping 2019a). number “bad” credit ratings oversampled; actual prevalence “bad” credit 5%. “good” versus “bad” ratings based debtor’s assessment risk prior granting credit. also unequal costs errors example.account differences cost errors, cutoff threshold predicting neural net changed. structure matrix resultant threshold Figure 14.4. threshold computed 0.93 based costs revenues associated cell 2X2 table using approach developed (Elkan 2001).\nFigure 14.4: Threshold calculations\nvariables German credit data set shown Table 14.6.\nTable 14.6: Variables German credit data set.\nKNIME workflow example shown . (Figure 14.5\nFigure 14.5: Workflow neural net analysis German Credit data set\ndescription node shown Table 14.7.\nTable 14.7: Description workflow nodes German Credit neural net anslysis.\nresults neural nets analysis shown Figure 14.6 0.50 threshold 0.93 threshold. Note increasing threshold assign prediction “good” category 0.93 reduced overall performance model. number correct “good” predictions decreased, number correct “bad” predictions increased.\nFigure 14.6: Results neural net analysis\nnoted , number “bad” credit ratings oversampled 30% actual percentage “bad” credit 5%. , confusion matrices need rebalanced reflect correct percentages. rebalanced confusion matrices shown . (Figure 14.7) Rebalancing straightforward process adjusting cell matrices row margin totals (actual numbers “good” “bad” credit cases) match population.\nFigure 14.7: Workflow neural net analysis German Credit data set\nAlso shown Figure14.7 costs (revenues, represented negative costs) associated cell confusion matrix. multiplying cell predictions times corresponding cell cost matrix, overall net cost revenue can estimated. example, predictions using 0.93 threshold resulted revenue 8,771 compared 0.50 default threshold resulted revenue 4,900. , despite reduced accuracy achieved higher threshold, bank better foregoing good customers avoid making bad credit decisions.","code":""},{"path":"cluster-analysis.html","id":"cluster-analysis","chapter":"15 Cluster analysis","heading":"15 Cluster analysis","text":"Cluster analysis unsupervised set methods identifying groups observations according measure proximity, mean either similarity distance. Clustering can used find groups objects (records, people, items, documents) objects within group similar sense one another distinct objects groups. goal create within group homogeneity group heterogeneity.example, three clusters, denoted 10 objects, B 20 objects, C 15 objects, formed set 45 observations, 10 objects cluster similar one another according criterion. true clusters B C. However, also cases observations cluster quite distinct (using criterion) cluster B. likewise cases considering clusters C, clusters B C.Cluster analysis used many scientific applied fields since goal many studies simplify, condense, classify situations – take many data points somehow extract essential groups segments large number observations. [^Cluster analysis goes different names various disciplines includes numerical taxonomy, pattern recognition, typology, clumping.]assumptions needed made priori regarding number groups structure; goal usually one discovery. cases objective find “natural groups” within data structure. Discovering natural groups always easy huge number possible ways groups can formed.One approach might investigate possible groupings decide (using criterion) “best” approach. computationally unfeasible. example, consider 16 playing cards Figure 15.1. number possible clusters size 2 set 16 32,767. number possible clusters size 3 16 objects 7 million.\nFigure 15.1: 16 playing cards.\ncluster objects, necessary define criterion used group objects. Similarity distance must computed (Case 1 Case 2), (Case 1 Case 3) (Case 2 Case 3). Euclidean distance typically used measure dissimilarity. Euclidean distance can generalized number dimensions. distance measure provides quantitative index similarity records objects. common measure distance squared Euclidean distance. measures, discussed later.","code":""},{"path":"cluster-analysis.html","id":"approaches-to-forming-clusters","chapter":"15 Cluster analysis","heading":"15.1 Approaches to forming clusters","text":"","code":""},{"path":"cluster-analysis.html","id":"hierarchical-versus-partitioning-methods","chapter":"15 Cluster analysis","heading":"15.1.1 Hierarchical versus partitioning methods","text":"many different clustering algorithms. One distinction hierarchical partitioning (non-hierarchical) methods. hierarchical approach, clusters formed sequentially case grouped cases cluster, case never separated program proceeds. partitioning methods pre-set number clusters specified algorthim proceeds iteratively assign observations clusters similar within different across.","code":""},{"path":"cluster-analysis.html","id":"hierarchical-methods","chapter":"15 Cluster analysis","heading":"15.1.1.1 Hierarchical methods","text":"simple conceptual example hierarchical clustering shown Figure 15.2.\nFigure 15.2: hierarchical clustering process.\nNine objects () assumed set characteristics (shown) used compute similarity. [^type hierarchical method described called agglomerative approach, since algorithm proceeds observations individual clusters observations single cluster. Divisive methods also available work opposite way, objects single cluster.] start, nine objects separate clusters 36 [9X(9-1)/2] similarity measures. algorithm ranks 36 values similar least similar two objects similar (C D) joined single cluster. results eight clusters 28 [8X(8-1)/2] similarities among eight computed ranked least similar. time, note various ways compute similarities clusters just single object one. complication discussed later. process continues nine objects grouped single cluster.\nFigure 15.3: Example dendrogram.\n","code":""},{"path":"cluster-analysis.html","id":"partitioning-methods","chapter":"15 Cluster analysis","heading":"15.1.1.2 Partitioning methods","text":"popular partitioning algorithm widely available k–means method. k-means procedure performs disjoint (observation one one cluster) analysis basis Euclidean distances computed using k quantitative variables. analyst must specify number clusters advance.k-means works like :Choose initial centroids randomly specified number clusters.Assign observation nearest centroid based Euclidean distance, forming provisional set clusters.Compute new centoids based new cluster.Reassign observations nearest new centroids, based Euclidean distance.Repeat maximum number iterations reached largest change cluster centroid less prespecified minimum.k-means method form tree structure different numbers clusters hierarchical procedures. Objects grouped one number clusters necessarily grouped different number clusters. Typically, program run several different settings number clusters results evaluated desirable result (discussed detail later).example, start nine objects . , program run four times two five clusters specified.two clusters shown red boxes.Next asked program find three clusters. next slide, 3 cluster solution shown red boxes. previous two-cluster solution shown blue boxes. Notice structure changed three cluster solution subset superset two cluster solution. Next can obtain 4 cluster solution. can continue far like. notice clusters formed hierarchical manner. advantage approach program doesn’t get locked early structure might optimal.","code":""},{"path":"cluster-analysis.html","id":"hard-versus-soft-methods","chapter":"15 Cluster analysis","heading":"15.1.2 “Hard” versus “soft” methods","text":"Another distinction clustering methods “hard clustering” observation belongs single cluster “soft” (“fuzzy”) clustering observations can belong one cluster (Harmouch, n.d.). typical hard clustering method K-means.Table 15.1\nTable 15.1: Beer brands data set\nTable 15.2\nTable 15.2: Correlation matrix beer brands data set\nTable 15.3\nTable 15.3: Eigenvalues beer brands data set\n","code":""},{"path":"cluster-analysis.html","id":"applying-hierarchical-clusters","chapter":"15 Cluster analysis","heading":"15.1.3 Applying hierarchical clusters","text":"Hierarchical clustering single technique rather set related techniques. software tools limit choices small number choices others offer almost bewildering list different approaches. (KNIME intermediate number choices provided, using R Python node, many types hierarchical clustering can used.)questions facing analysts using hierachical clustering different software tools include: (Everitt 2011)agglomerative divisive measures used?measure distance (similarity) used?\nAlternatives include Euclidean, cosine, Minkowski, Manhattan, Jaccard, Tanimoto.\nvariables weighted variables influence determining clusters weights chosen variables equal influence?\nAlternatives include Euclidean, cosine, Minkowski, Manhattan, Jaccard, Tanimoto.variables weighted variables influence determining clusters weights chosen variables equal influence?measure inter-cluster distance used?\nAlternatives include single linkage, complete linkage, centroid linkage, average linkage, Ward’s method.\nAlternatives include single linkage, complete linkage, centroid linkage, average linkage, Ward’s method.frequently recommended variables standardized prior computing distances. usually good idea, “…substitute careful thought context individual problem. Specifying appropriate dissimilarity measure far important obtaining success clustering choice clustering\nalgorithm. aspect problem emphasized less clustering literature algorithms , since depends domain knowledge specifics less amenable general research.” (Hastie 2009)","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
