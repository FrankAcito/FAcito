[["index.html", "Analytics with KNIME and R Cover page", " Analytics with KNIME and R F Acito 2021-11-16 Cover page This is a draft of an online book on analytics using KNIME. R will also be used to supplement KNIME with specialized tasks. At this point, the book has not been extensively reviewed and is being provided as is. Revisions are in progress. Frank Acito "],["intro.html", "Chapter 1 Introduction 1.1 What is analytics? 1.2 Some trends in analytics 1.3 The analytics process model", " Chapter 1 Introduction Two developments over the past few years have created great opportunities to use analytics to support and enhance decision making in areas such as business, health care and medicine, government, and not-for-profit organizations but four trends have converged to create emerging disciplines with labels such as data science or analytics. The three trends are: An explosion in the amount, variety, and velocity of data. Data as a driver of strategy. Greater management sophistication (and expectations) regarding the use of data to support decision making. Better, faster, and cheaper hardware and software. In recent years terms such as business analytics, predictive analytics, data mining, machine learning, big data, and others have been used to describe various aspects of dealing with data.1 At first many of these terms were thought to represent a re-branding of the discipline of statistics. But statistics is only part of the story. The terminology associated with analytics changes over time. One way of looking at this is with Google Trends. Google Trends only goes as far back as 2004, but this nevertheless provides some interesting comparisons. (Figure 1.1) Figure 1.1: Google search trends on analytics-related terms The y-axis in Google trends) ranges from 0 to 100, with 100 representing the peak number of searches for a particular term, so only relative popularity of the terms is shown.. In 2004, the terms data mining and business intelligence were at their peaks in terms of searches. By 2020 machine learning, data science, and big data tended upward. Each of these and related terms have specific definitions, but it is easy to become confused about this area. It is likely that the existence of so many terms is partly a function of the newness of this area. The emergence of nearly 250 graduate programs in Analytics, Business Analytics, and Data Science since 2007 is another indicator the popularity of this field. [^Source: analytics.ncsu.edu] 1.1 What is analytics? The term analytics has been defined in many ways. An interesting paragraph from Gartners Information Technology Glossary provides a useful perspective. Analytics has emerged as a catch-all term for a variety of different business intelligence (BI)- and application-related initiatives. For some, it is the process of analyzing information from a particular domain, such as website analytics. For others, it is applying the breadth of BI capabilities to a specific content area (for example, sales, service, supply chain). In particular, BI vendors use the analytics moniker to differentiate their products from the competition. Increasingly, analytics is used to describe statistical and mathematical data analysis that clusters, segments, scores and predicts what scenarios are most likely to happen. Whatever the use cases, analytics has moved deeper into the business vernacular. Analytics has garnered a burgeoning interest from business and IT professionals looking to exploit huge mounds of internally generated and externally available data. (Gartner, n.d.) 1.2 Some trends in analytics 1.2.1 Broadening of application areas Some of the first applications of analytics were focused on management of risk from a financial viewpoint. The FICA scores on credit from the Fair Isaacs Corporation began in the 1950s was one of the first applications. Today there are applications and opportunities in virtually all functional areas. For example, an emerging trend is the use of analytics in human resources where applications are being developed to deal with turnover issues, benefits optimization and management, succession planning, and even training. 1.2.2 Generalization of the notion of data For many years data analysis was applied to structured quantitative data. The data is clearly formatted with defined fields. In recent years advances have been made in applying analytics to unstructured data from text, images, video, and sounds. Examples include media data, surveillance data, weather data, and more. The Internet of Things refers to use of embedded sensors that collect data on physical objects - such as machines, automobiles, and home appliances. The sensors are used to collect and the data from things so provide safety and performance monitoring, productivity improvement, guidance, and other functions. Today analytics includes marketing mix optimization, real time customer offer engines, predictive campaign analytics, web analytics, and customer profitability management, profitability modeling and optimization that goes beyond activity- based costing to allocate revenues and enable resource allocations to maximize profits. Related to the many new types of data is the trend toward using external data in combination with internal data. Organizations have routinely collected and stored extensive information about finances, production, employees, and sales. By combining internal data with data from governments, social media, mobile devices, and many other sources richer analyses and better insights can be formed. 1.2.3 A trend from slicing and dicing data to more advanced techniques There is a trend from reporting, slicing, dicing, and drilling down in databases toward the increased use of data mining, machine learning, and more of what are generally called advanced analytics. Advanced applications are not necessarily more complicated, sophisticated, or difficult  but they can be. Applications can be mapped to a continuum that ranges from queries of data bases, to pivoting, slicing and dicing, and drilling down into historical data. Moving one more step to the right is descriptive analytics, computing means, ranges, standard deviations, medians, correlations, and other summary statistics. Next is data mining and predictive modeling, which includes techniques ranging from multiple regression, to logit analysis, neural nets, decision trees, discriminant analysis, and support vector machines. These techniques build models which make predictions and inferences about the future or can help to identify new patterns in data that were hitherto unseen. Another step to right is simulation. Predictive models are employed to create alternate possible outcomes, perhaps thousands of different ones in a Monte Carlo type of random process. Using different sets of assumptions or scenarios, explorations of possible future outcomes can be performed. If the models are complete enough and the data reliable enough, the ultimate kind of analytics can be conducted which is optimization. Here the models are used to determine the best actions or decisions according to some criterion (such as maximizing profit or minimizing cost) subject to constraints on budgets, time, and/or other resources. At this point the models become prescriptive rather than just predictive or descriptive. 1.2.4 More advanced data visualization Many software programs are available that can easily produce basic visual representations of data such as bar charts, scatterplots, pie charts, and line graphs. Such visualizations are only a small sliver of what is possible. Advanced data visualization is concerned with the last link in the communication of results  from the computer to the human mind. Techniques are now used to show dynamic content and animations, allow real-time querying, alerts, and combine multiple visualizations into interactive dashboards. 1.3 The analytics process model It is important to conceptualize business analytics as a process. It is not a one-shot effort. The business analytics process can be represented as a series of steps. These steps are conducted sequentially, but there almost always involves iteration, since the results of one step may require that previous steps are re-examined and perhaps redefined. There are several process frameworks that have been published including CRISP (Cross Industry Standard Process for Data Mining) (The Crisp-DM User Guide, n.d.), SEMMA (Sample, Explore, Modify, Model and Assess) (SEMMA from SAS, n.d.), KDD (KDD and Data Mining, n.d.) (Knowledge Discovery in Databases), Microsofts TDSP (What Is TDSP? n.d.) (Team Data Science Process), and others. While the CRISP framework has not been updated since it was developed, it has remained the most popular model according to a KDnuggets survey in 2014 CRISP. [CRISP-DM, Still the Top Methodology for Analytics, Data Mining, or Data Science Projects (n.d.)} Many analysts have developed their own custom models, but the CRISP framework remains a viable approach, especially due to its emphasis on business understanding as the first step in a project. (Figure 1.2) Figure 1.2: The CRISP model. This book focuses on the process of building analytics models. While exploratory and prescriptive models could be considered, the book is mainly concerned with the prediction. Descriptive models are often created as a prelude to constructing a predictive model, since it is important to describe the data to inform model building. Also, while prescriptive models are becoming more common, it is usually necessary to have a predictive model to inform the prescriptive recommendations. Furthermore, the line between predictive and descriptive models is frequently blurred when it comes to deployment. Deploying a predictive model to production means that the output or outputs of the predictive model become scores used to support decision making. In fact, predictive models alone, without deployment of some form, are usually considered to be of little value to organizations. Despite the widespread use of the CRISP model, I propose a revised modeling process. The revised model (Figure 1.3) groups data preparation, exploratory analytics, and feature engineering into a single step. Based on my experience with creating models, it seems more logical to group these tasks since they are closely related and usually involve iteration. I added communication to the deployment step, since creating a clear understanding of the model is important to achieving buy-in by management.. Finally, I added a step labeled Performance monitoring, since a deployed model should be periodically evaluated to make sure that it is performing as intended. If it is not performing well, the back arrow signals that the process starts anew. In practice the various steps are likely to be iterative, which is indicated by the dual arrows linking each step. Results from one step may requiring revisiting or revising a previous step or even more than one step. (Figure 1.3) Figure 1.3: The process based on modifications of the CRISP model. References "],["business-understanding-and-problem-definition.html", "Chapter 2 Business understanding and problem definition 2.1 Expert views 2.2 Understanding the business 2.3 Identifying stakeholders 2.4 Structured versus unstructured problems 2.5 Framing the problem 2.6 Summary Appendix: Some tools for problem definition Problems", " Chapter 2 Business understanding and problem definition Before collecting data or thinking about which technique to use, it is critical to understand the business problem. While it may seem obvious that the business problem should be clearly stated, it is frequently reported that the biggest reason for failures of analytics projects is a poor understanding of the problem. It perhaps goes without saying that in order to define a problem that will result in adding value to an organization, it must be preceded by a good understanding the business. The elements of this phase of the analytics process include understanding the business, identifying the stakeholders, recognizing the type of problem, and finally framing the problem. There is a temptation to take a request from a client or manager without careful thought and to go directly to gathering and analyzing data. If this path is taken, it is very likely that the most important business question will not be answered. This causes wasted time and resources. The benefits of a careful effort to understand the business and define the problem include: The work of the analyst becomes more efficient because fewer dead-ends are encountered. Data collection will be more efficient, with more likely focus on getting the right data and ignoring variables that are not relevant. Rather than just getting all the data that can be found, a more directed search for variables can occur. This also provides stronger justification for asking the client for the additiontal data. Second, business analytics is about adding value. The problem should be well structured so that the results can be implemented and that they will add value. With the right problem structure getting a useful solution is more likely. Scattershot model building is likely to lead to poor predictions and, importantly, little guidance as to what is wrong and how to improve the model. Third, by having a clearly defined problem that can be communicated to everyone involved, two traps of providing information that is already known or information that is so unusual as to be unbelievable are avoided. 2.1 Expert views Business understanding and defining the problem are among the most important tasks in analytics projects. These quotes from a variety of experts attest to the importance of this phase of the analytics process. The difference between great and mediocre data science is not about math or engineering: it is about asking the right question(s). No amount of technical competence or statistical rigor can make up for having solved a useless problem. (Cady 2017) Einstein is quoted as having said that if he had one hour to save the world he would spend fifty-five minutes defining the problem and only five minutes finding the solution. The most serious mistakes are not being made as a result of wrong answers. The truly dangerous thing is asking the wrong questions. Peter Drucker Successful analytic teams spend more time understanding the business problem and less time wading through lakes of data. (Taylor 2017) What we are finding is that in a lot of companies, there are great data scientists and great business people but what is missing is business people who know enough data analytics to say, here is the problem I would like you to help me with. And then they can take the outcome from the data scientists and see how they can best leverage it. That is where we must get to in the next couple of years if we want to take advantage of the digital technologies. (Ittner 2019) The following case study demonstrates the importance of delving deeper into a situation and not simply focusing on the approach asked for by the client.2 One of the common applications of analytics to is help predict and understand churn for telecommunications providers. Subscribers to cell phone plans are notorious for cancelling contracts and signing up with another carrier. People churn because they are unhappy with service, or they want a new phone, or they get a good price from another carrier. There are all sorts of reasons that people leave. This can be very expensive for the provider. It is usually far cheaper for the provider to take steps to keep a customer than to go find a new one. A large cell phone carrier in the United States called in a consultant to help with this problem. The carrier was experiencing considerable churn and asked the business analyst to find a way to predict who was most likely to leave the company and to determine what might be done to prevent that customer from leaving. This seemed like a reasonable request and one that could be addressed with analytics. The cell phone company had lots of data on its customers and a history of those customers that stayed and those that left. The consultant decided to look into the problem before digging into the data. What the consultant found was that the churn was occurring right around the time that the two-year contract was up. He asked managers in the company for a detailed explanation of how customers were contacted, how they were asked to sign up for a new contract, and what sort of communications were being sent to the customers. It turned out that that about two months prior to a customers contract being up, the company sent a reminder letter saying that the contract was coming up for renewal and asking that the customer sign up for another two years. This same letter was sent to all customers. Imagine you are a consumer with a cell phone company and you get a letter from the company saying that your contract ends in 60 days. What behavior does this trigger? Well, for many customers, it got them thinking about shopping around. It turned out that the very efforts to get customers to sign up for another contract were actually motivating many of them to leave the company. Many people dont know when their contracts are up, and if left alone, they would continue with the company for years without switching. The companys marketing department was triggering the churn. These letters were stopped and the churn went down. If the consultant had just gone directly to analyzing data, he might have totally missed this root cause of the problem. In fact he would be solving the wrong problem. The point is management wanted a predictive model for churn. The consultant could have done this, but the real solution would not be found. Understanding the real problem is therefore extremely important. 2.2 Understanding the business At the start, the goal of a project can be stated in vague terms. It is important to meet with stakeholders (those with an interest in the results of the project) to refine the objectives into operational terms and to obtain buy-in from those stakeholders. Instead, those individuals, the stakeholders, involved with the project should be considered. Stakeholders can include IT, executives in the customer department (marketing, accounting, finance, etc.), those who may be expected to use the resulting model, and those who might be affected by the project. The following tasks are needed during the business understanding phase (adapted from the CRISP model ): Determine business objectives; that is, what is the basic goal of the project? In business situations it might be to gain new customers, increase the loyalty of current customers, or to reduce lost due to fraud. Translate the business objectives into the technical metrics that can result from the analysis. For example, if customer loyalty is the problem, the goal of the analysis could be to identify those customers likely to stop using the companys products or services. This objective might be further refined by stating a needed lead time prior to customer loss so that remedial action could be taken. Develop a project plan  list the stages, required resources, risks, time duration, contingencies, and evaluation metrics. 2.3 Identifying stakeholders Stakeholders  the people in the organization who care about the problem - can be part of the solution, can potentially derail the solution, can provide the resources needed to arrive at a solution, or can be in a position to take action on the results. Some questions to ask about stakeholders: Which executives have a stake in the outcome of the analysis? Have these executives been briefed on the problem and the solution approach? Can the key executives provide the necessary resources to implement recommendations? Do the key executives support analytic approaches to making decisions? Are there people with vested interests in a particular solution? Is there a plan for regular communication and feedback with interim results and progress? Do key stakeholders have certain styles of using information and making decisions?3 (Based on (Davenport and Kim 2013).) 2.4 Structured versus unstructured problems The types of problems that could be addressed with analytics are wide-ranging: How can the Red Cross increase blood donations? Should a consumer products company increase ad spending for a certain brand or lower the price to stimulate sales? How do you increase the number of people who agree to be organ donors? How can a university increase its student retention rates? How much of a discount should a resort offer for booking tickets online 6 months early? Should a hotel offer a surprise discount on rooms at the time of check-in or should they offer it as people are leaving? How can a casino increase the lifetime value of a customer? Should a particular person be approved for a loan? Is this credit card transaction fraudulent? While these may seem to be very specific questions, attempting to structure the question to guide an analytics project would likely reveal that the questions are not well structured. Many challenging analytics problems are unstructured and difficult. Professionals are rewarded for their ability to solve difficult problems, not simply following rote procedures or relaying memorized information. When faced with an unstructured problem, it is even more important to work on carefully defining it. The following, based on (Jonassen 1997), shows some of the characteristics which distinguish well-structured versus unstructured problems. (Figure2.1) Figure 2.1: Structured versus unstructured problems. 2.5 Framing the problem The term framing means the process of describing and interpreting a situation: framing focuses attention. The problem frame helps define the importance of a problem and also sets the direction for solving it. Depending upon how a problem or question is framed can lead to very different answers. For example, these two questions ask about the number 10 in different ways which will lead to different answers: What is the sum of 5 plus 5? What two numbers add up to 10? When a tentative analytics problem is identified, different frames will lead to different analytics approaches. For example, an insurance company may want to reduce the cost of fraudulent claims. Analytics could be used to address this problem, but depending on how the problem is framed will lead to different target variables and probably different analytic techniques. Consider the following different goals associated with reducing fraud: Identify cases that had the highest propensity of fraud? Identify current customers likely to commit fraud in the near future? Identify customers apply for a policy are likely to commit fraud and rejecting those applicants? Identify cases where there is the highest likelihood to recover monies? Identify case with largest potential dollar amount of fraud? Identify cases that will maximize the hourly return of investigators assigned to deal with fraudulent cases? The importance of problem framing is illustrated in this old story (Shoemaker and Russo 2001). There is an old story about a Franciscan priest and a Jesuit priest who were both heavy smokers and somewhat troubled by their human fragility, especially about smoking when praying to the Lord. The Franciscan met with his prefect and asked, Father, would it be permitted to smoke while praying to the Lord? The answer was a resounding, No! The Jesuit also sought counsel, but asked his question differently. Father, when in moments of weakness I smoke, would it be permitted to say a prayer to the Lord? The answer was, Yes, of course my son. In some cases, the first attempt at a problem statement focuses on symptoms, which may not address the root problem. For example, a symptom might be: We are losing market share rapidly. The problem definition might then be: How do we regain market share? (Hauch 2018) But this may not direct the analyst team to the real issue. One suggestion to get at the root of a problem through framing is to use The 40-20-10-5 rule. For all those problems that are difficult to define quickly, you can apply this rule that goes through 4 basic steps. State your problem in 40 words. Cut it down to 20, then to 10 and end up with a 5 words problem statement. If you can not keep it simple, probably you have not reached the roots of it yet. Another data scientist summarized key considerations to guide a team through a problem framing exercise. (Arnuld 2020) See if you can solve the problem without using ML. Sometimes a simple heuristic is good enough. Be clear about what you want and state it without using ML. 3. Write your desired outcome in simple English What do you expect the model to output? What kind of ML problem you are solving e.g., if supervised then what kind: classification or regression. If classification then what kind? binary or multi-class? etc. What is a successful model in this case? What is a failed model in this case? Write all the cases. Not being able to succeed is not the only failure case. Quantify your success. How will you measure it in your model? Try to keep the model simple. A simple models result can justify if you need a complex model. Complex models are slower to train and difficult to understand. Build a simple model and deploy it. The biggest gain from ML tends to be the first launch. You can improve later and launch another version Example of solving a simpler problem Often, engineers make things more complicated than necessary. Consider the example of the NASA space pen. During the 1960s, NASA focused a major program on developing a pen that would write in zero gravity, while the Soviet space program used the much simpler and already-invented pencil. I dont know for sure because I wasnt there, but I strongly suspect this came about because of a miss in the problem statement. The Americans were working on this problem: How do we get a pen to write in zero gravity? The Soviets were working on a different problem: How do we write in zero gravity? It seems like such a small, subtle difference, but it had a huge impact on what work followed. (Flinchbaugh 2009) 2.6 Summary Preparing an effective, actionable definition of the problem prior to machine learning is universally recognized as being important. However, specifying exactly how to create such as problem definition cannot be codified in a simple process. As with many complex concepts, it is likely that you will know it when you see it. But, getting to that point requires creativity and hard work. Appendix: Some tools for problem definition Because of the critical importance of problem definition, there are many tools and strategies that have been developed to get started. A few of these are listed here. Right to left thinking One approach that can be very helpful is to turn the business objective into a decision to be made. For example, if the problem is that a company needs more customers, this could lead to a goal of getting new customers or working on keeping current customers. These goals will lead to very different data needs and analyses. If the goal is keeping current customers, then a model that predicts which specific customers are likely to leave can lead to actions to reduce the probability of those customers leaving. If on the other hand, the goal is to get new customers, then the model can be used to indicate which potential individuals are most likely to respond favorably to an offer. Reversing the problem Clearly identify the problem or challenge, and write it down. Brainstorm the reverse problem to generate reverse solution ideas. Allow the brainstorm ideas to flow freely. Do not reject anything at this stage. Instead of asking, How do I solve or prevent this problem? ask, How could I possibly cause the problem? Instead of asking How do I achieve these results? ask, How could I possibly achieve the opposite effect? Once you have brainstormed all the ideas to solve the reverse problem, now reverse these into solution ideas for the original problem or challenge. Evaluate these solution ideas to determine if a potential solution is suggested or at least the attributes of a potential solution? (DeRusha and WOlfson, n.d.) Open the problem with whys Consider the following: If I asked you to build a bridge for me, you could go off and build a bridge. Or you could come back to me with another question: Why do you need a bridge? I would likely tell you that I need a bridge to get to the other side of a river. Aha! This response opens up the frame of possible solutions. There are clearly many ways to get across a river besides using a bridge. You could dig a tunnel, take a ferry, paddle a canoe, use a zip line, or fly a hot-air balloon, to name a few. You can open the frame even farther by asking why I want to get to the other side of the river. Imagine that I told you that I work on the other side. This, again, provides valuable information and broadens the range of possible solutions even more. There are probably viable ways for me to earn a living without ever going across the river. Source: (Seelig 2013) Challenge assumptions Every problem  no matter how apparently simple it may be  comes with a long list of assumptions attached. Many of these assumptions may be inaccurate and could make your problem statement inadequate or even misguided. The first step to get rid of bad assumptions is to make them explicit. Write a list and expose as many assumptions as you can  especially those that may seem the most obvious and untouchable. That, in itself, brings more clarity to the problem at hand. Essentially, you need to learn how to think like a philosopher. But go further and test each assumption for validity: think in ways that they might not be valid and their consequences. What you will find may surprise you: that many of those bad assumptions are self-imposed  with just a bit of scrutiny you are able to safely drop them. Be a skeptic. (Clissold 2021) Example of challenging assumptions Consider the interesting example of the mathematician Abraham Wald and the British Air Ministry. During World War II, the British Air Ministry engaged Mr. Wald to study the damage suffered by airplanes engaged in combat missions and use those results to recommend appropriate places for armor reinforcement. All returning planes were assessed for damage and the data was collected and analyzed. Patterns of damage soon became apparent, and the officers of the Royal Air Force concluded that the planes should be reinforced based on those patterns. (Ellenberg 2016) Mr. Wald took a different approach. He reasoned that the damaged planes which made it back werent the real problem. Additional armor was needed most on planes which didnt make it back. While he didnt know for certain where the damage was on the lost planes, he could reason that it was different from that of the planes which safely returned. His task was to see the problem which wasnt easily observed. Chunking Chunk up Chunking up is about taking a broader view. Helicopter up to 30,000 feet. Survey the landscape to see the whole system. Ask Why things happen to find higher-level purpose. Ask what is this an instance of to find a more general classification. Use inductive reasoning to go from specific detail to general theories and explanations. Chunk down Chunking down is about going into detail to find smaller and more specific elements of the system. Ask How things happen to find lower-level detail. Ask What, specifically to probe for more information. Ask Give me an example to get specific instances of a class. Use deductive reasoning to go from general theories and ideas to specific cases and instances. Chunk up and down Chunking up and down go well together as a way of looking differently at the same situation. Chunk up from the existing situation to find a general or broader view. Then chunk down somewhere else. Problems Problem 1 Virtually all data base systems today include capabilities to retrieve records and to create and manipulate tables efficiently. Queries return data from a database that fulfills specific constraints and criteria set by the user. OLAP uses a process of preaggregation to form data cubes, which enables retrieval more quickly, often with less computational power. Neither database queries nor OLAP, however, are considered data mining tools. Data mining addresses fundamentally different questions by constructing models of the data. Consider the following business questions. Take a few minutes and twist each question into an analytic modeling question. Who are my best customers? What geographic areas do sales come from? How long do customers stay with my company? What do my customers buy, specifically? What share of the customer wallet do I have? Which suppliers are most reliable? What are the characteristics of the most reliable suppliers? Which salespeople provide the most profit? What is the perceived quality of my products? Which products are most profitable? Problem 2 A bank is interested in running a major promotion to increase its share of the new car loan market. Assuming the promotion is successful, the number of applications for new car loans will increase substantially. Therefore, it was suggested that a predictive analytics model be developed to speed up the process of loan acceptance or rejection, with the specific goal of quickly and automatically identifying applicants that are likely to default on the loan. A database of 5,500 automobile loans is available from the banks information system, covering loans made from July of 1999 through June of 2010. Variables for each loan include: Whether or not the loan resulted in default Amount of loan Percentage of loan to value of automobile Age of applicant Gender of applicant Current debt (monthly payments) Monthly household income Years in current residence Years in previous residence Number of dependents Marital status Credit score Date of the loan Duration of the loan in months What issues or questions would you have regarding this data set and prior to developing a predictive model? References "],["introduction-to-knime.html", "Chapter 3 Introduction to KNIME 3.1 The KNIME Workbench 3.2 Learning to use KNIME 3.3 KNIME extensions and integrations 3.4 KNIME workflow example #1: Predicting heart disease 3.5 KNIME workflow example #2: Preparation of hospital data 3.6 Summary Problems", " Chapter 3 Introduction to KNIME The KNIME platform is presented along with links to resources including tutorials and documentation. A step-by-step example of using KNIME is shown to demonstrate how KNIME can be used to analyze a small data set. KNIME is a comprehensive tool for analytics and data mining which uses an intuitive drag and drop workflow canvas. While KNIME can be (and is) used by professional data analysts, it is an excellent low code platform for learning predictive analytics and data mining. KNIME workflows provide a graphic representation of the steps taken in an analysis, which makes the analysis self-documenting. This makes it easy to communicate your analyses with others and to see how to reproduce your analyses. KNIME was developed by a group at the University of Konstanz in Germany beginning in 2004 and the first release was in 2006. As noted on its web site, KNIME is committed to open source: Unlike other open-source products, KNIME Analytics Platform is not a cut-down version and there are no artificial limitations on execution environment or data size: If you have enough local or cloud-based space and compute power, you can run projects with billions of rows, as many KNIME users currently do. ref: https://www.knime.com/knime-open-source-story There is a commercial server version of KNIME which is needed for deploying KNIME on the web, but this is not needed for learning. Everything else is available with the open-source version. There are several features which make KNIME stand out from its competitors: KNIME is free to use on your machine. KNIME runs on Windows, Mac, and Linux machines. KNIME has over 4,000 nodes for data source connections, transformations, machine learning, and visualization. While KNIME includes a broad array of data processing and analysis capabilities, it is fully extensible by creating custom nodes using Python or R. Many of the capabilities of two other analytic platforms, H2O and WEKA, are also integrated into KNIME and work seamlessly in the drag and drop workflow. A variety of data file types can be used including csv, Excel, and, using an easily installed extension, databases. Data can be exported to Excel, Tableau, Spotfire, Power BI, and other reporting platforms. There is a large active community of users that can answer questions and provide help. Many ready-to-use workflows are available which can be easily installed in your own work environment by dragging and dropping from the KNIME site. Extensive documentation, learning modules, videos, and training events are available. 3.1 The KNIME Workbench The main sections of the KNIME Workbench are shown in Figure 3.1. A brief description of each section is provided below, but for more detail see (KNIME 2020). Figure 3.1: The KNIME Workbench 3.1.1 Elements of the KNIME Workbench KNIME Explorer This provides links to the available workflows on your machine as well as those available from KNIME servers including workflow examples from KNIME and the KNIME community. Workflow Coach This is a handy tool to help you build an analytic workflow. For any node selected in your workflow, suggested next nodes are listed. The suggestions are based on usage statistics from the KNIME Community. Node Repository Listed in this area are the analysis nodes installed on your machine. Nodes are available to read and write files, explore and transform data, run basic and advanced analytics, and create visualizations. A core set of nodes is included when you install KNIME, but thousands of additional nodes are available and easily installed. The nodes are organized by categories, but you can also use the search box on the top of the node repository to find nodes. Workflow Editor This is the canvas for editing the currently active workflow. The workflow is created by dragging nodes from the Node Repository and linking them appropriately. Outline This area shows a small overview of the current workflow. Console This shows the processing taking place when executing a workflow. It also provides warnings and error messages. Node Description For each node selected in a workflow, a detailed description is provided including the general function of the node, available settings, and input and/or output ports. 3.2 Learning to use KNIME Learning to use KNIME takes some time, but extensive, free written and video resources are available. A Getting Started Guide is available at Getting started with KNIME A series of self-paced courses are online and free. The courses include exercises and solutions. KNIME Self-Paced Courses There are four levels of self-paced courses: Level 1 courses KNIME Analytics Platform for Data Scientists: Basics KNIME Analytics Platform for Data Wranglers: Basics Level 2 courses KNIME Analytics Platform for Data Scientists: Advanced KNIME Analytics Platform for Data Wranglers: Advanced Level 3 course KNIME Server Course: Productionizing and Collaboration Level 4 courses Introduction to Big Data with KNIME Analytics Platform Introduction to Text Processing This is a link to all the documentation which can be read or downloaded. KNIME Documentation Individual documents for specific topics are available as follows: KNIME QuickStart Guide KNIME Analytics Platform Installation Guide KNIME Workbench Guide Extensions and Integrations Guide KNIME Flow Control Guide KNIME Components Guide KNIME Integrated Deployment Guide KNIME File Handling Guide A free bootcamp for KNIME is available from UDEMY with 50 video instructional lectures running over four hours. The course starts with installation and setup and proceeds to demonstrate practical applications in machine learning. Bootcamp for KNIME Analytics Platform 3.3 KNIME extensions and integrations In addition to the nodes that are developed and maintained by KNIME, there are many nodes that have been developed by community developers and KNIME Partners. One very nice feature of KNIME is its integration with open source tools. A few examples are listed below. R and Python scripts can be run seamlessly in a KNIME workflow. Code can also be run from Jupyter notebooks. H20 provides high-performance algorithms such as Random Forests, Gradient Boosted Tree, and many others. The H20 open source tools can be integrated into a since workflow. One feature of H20 is an available AutoML Learner which runs and compares the performance several predictor models, including optimization of parameter settings. KNIME provides integration with the Python Keras Deep Learning framework. Keras (which makes TensorFlow more approachable) contains several implementations of neural networks including convolutional and recurrent nets, which can be used for applications such as image recognition. 3.4 KNIME workflow example #1: Predicting heart disease The following example is included to illustrate how KNIME can be used to build a model. Do not be concerned if some of the details are unfamiliar at this point; these details will be covered in later chapters of the book. The example involves a cardiovascular disease data set. Mortality rates for cardiovascular diseases high and increasing, especially in developing regions of the world. In the United States 1 in 4 deaths each year are from heart disease. If the presence of heart disease can be predicted using a physicians examination and laboratory tests, this would be a valuable diagnostic tool. Such a prediction is difficult because there are several contributory variables, including diabetes, high blood pressure, high cholesterol, abnormal pulse rate, and other factors. A data set from the Cleveland Clinic is available from the UCI Machine Learning Repository (Aha 1988). The data set has 2974 rows, 13 predictor values, and 1 target (heart disease versus no heart disease). Since the target variable has just two levels, logistic regression was run in KNIME. (Details on logistic regression and other models are provided in later chapters.) A step-by-step illustration of running KNIME is shown in the next six figures. The first few rows of the data set are shown in Figure 3.2. The KNIME node CSV Reader was used to input the data. (Many other data formats can be read by KNIME as well.) Figure 3.2: Read data from a .CSV file. Good practice in building machine learning models is to create a model using part of the data and test the model using a separate section of the data. Accordingly, the KNIME node Partitioning was used to create training and test data subsets. 70% of the sample was used for training and 30% for testing. (Figure 3.3) The 70/30 ratio is typical, but not the only split that can be used. The data was split randomly; to make the splitting repeatable, a seed was set (arbitrarily at 123). Also, the random selection was done in a stratified manner so that the ratio of heart disease versus no heart disease would be equal (or nearly equal) in both the training and test subsets. The Partitioning node has two outputs: the upper output is the training data and the lower output is the test data subset. Figure 3.3: Create training and test data sets Performing a logistic regression in KNIME involves two nodes. The first node, shown in Figure 3.4, learns the logistic model. Note that the upper output from the Partitioning node (the training data) was used to build the model. There are three outputs from the Logistic Regression Learning: the upper output (shown as a black square) contains the model itself; the middle output arrow contains the coefficients from the model; and the lower output arrow has information about the modeling process (not used in this example). Figure 3.4: Run logistic regression on training data The Logistic Regression Predictor (Figure 3.5) two inputs: the square input takes the model built in the Learner node. The lower arrow input takes the test data from the Partitioning node. The Logistic Regression Predictor uses these inputs to create predictions for the test data which is output in the single arrow on the right of the node. Figure 3.5: Predict heart disease using the logistic regression model on the test data. The Scorer node (Figure 3.6)in KNIME is designed to evaluate the performance of models where the target variable is binary, such as is the case in this example. The input to this node is a data table with the actual and predicted values for the test data set. Using this data, a cross-tabulation called a confusion matrix can be derived. Figure 3.6: Score the logistic regression results on the test data. The 2 X 2 confusion matrix is shown in Figure 3.7. This table shows both accurate and inaccurate predictions. In this example, the prediction accuracy was under 80%, which is not sufficient for clinical diagnoses. Perhaps a different model with more data could be developed to improve accuracy. Especially of concern in the results shown in the confusion matrix is the 11 cases where heart disease was present, but the model predicted not present. Adjustments to the prediction mechanism could be made to err more on the side of predicting present when the fact is not present, since this is arguably less serious than the other type of error. Figure 3.7: The confusion matrix and accuracy measures for predicting heart disease Additional performance measures are also provided by the Scorer node and these will be discussed in the chapter on model evaluation. 3.5 KNIME workflow example #2: Preparation of hospital data The length of stay for patients in hospitals is an important indicator of the efficiency of hospital management. Extended stays can increase the likelihood of infection and complications. Long stays in the hospital negatively impact a patients experience and result in higher health care costs. This example uses a 2015 data set on over 2.3 million patient hospitalizations in New York State. Thirty-four variables are included in the data set, but this analysis will focus on the following: Age Group (0 to 17, 18 to 29, 30 to 49, 50 to 69, 70 or Older) Gender (M,F, U) Length of Stay (in days) Type of admission (Elective, Urgent) Severity of illness( Minor, Moderate, Major, Extreme) Total Charges (the amount billed by the hospital) Total Cost (the expense incurred by the hospital) The steps taken in preparing the data are shown in Table 3.1. Table 3.1: Nodes for preparing hospital data Node Label Description 1 File Reader Read Hospital_Inpatient_Discharges.csv file. 2 Column Filter Include only: Age Group, Gender, Length of Stay, Type of Admission, APR Severity of Illness Description,Total Charges,Total Costs. Remove other columns. 3 String Manipulation remove , from Total Charges 4 String Manipulation remove , from Total Costs 5 String Manipulation remove + from Length of Stay 6 String to Number Change Length of Stay, Total Charges, and Total Costs to numeric type. 7 Row Filter Exclude 48 rows with Gender = U 8 Data Explorer Explore the data set 9 CSV Writer Write the modified file to SubsetOfHospitalData.csv The table from the Data Explorer node for the continuous variables is shown in Figure 3.8 and the table for nominal variables is shown in Figure 3.9. Figure 3.8: Summary of continuous variables in the hospital data subset Figure 3.9: Summary of continuous variables in the hospital data subset 3.6 Summary KNIME is the main platform that will be used throughout this text for most of the analyses. It is important to download and install KNIME prior to continuing with the next chapters and to become familiar with KNIME by working some of the problems. Problems Problem 1 Create a new KNIME workflow Open KNIME. In the KNIME Explorer area: Right click on LOCAL (Local Workspace). Click New KNIME Workflow. Enter the name of the Workflow. Click FINISH. In the Node Repository: Click on &gt; IO. Click on &gt; Read. Drag File Reader to the Workflow. Right click on the File Reader in the Workflow. Select Configure. Browse to select iris.csv data set.5 Click OK. Right click on File Reader and select Execute. Find the Statistics node in the Node Repository. Drag the Statistics node to the Workflow and place it to the right of the File Reader. Using the mouse, connect the output port of the File Reader to the input port of the Statistics node. Right click on the Statistics node and select Execute. There are three output ports on the Statistics node. Table with numeric values. Table with all nominal value histograms. Table with all nominal values and their counts. Questions: What is the mean value of sepal length? Do any of the continuous variables have skewness values which indicate that the median rather than the mean should be used as the measure of central tendency? Explain. Is the Skewness value for the variable amount consistent with the appearance of the histogram? Explain. Save and close the workflow. Problem 2 This problem requires you to perform an analysis of the file sales.csv from the KNIME Getting Started Guide. The following steps will guide you through this introduction to KNIME. Download the data set sales_data.csv. Open KNIME and create a new workflow named Problem_KNIME_1. Drag and drop sales_data.csv on to the KNIME workflow. Right click on the CSV Reader and select Execute. Right click on the output arrow on the CSV Reader and select File Table to examine the file. Close the File Table. In the Node Repository, search for the Column Filter node. Drag the Column Filter node to the workflow and connect its input port to the output port of the CSV Reader. Right click on the Column Filter and choose Configure. Move all of the variables to the Exclude area using the &lt;&lt; icon. Then use the &gt; icon to move Country, product, and amount to the Include area. Click OK. Execute the Column Filter node. Right click on the output arrow on the Column Filter and select Filtered Table to examine the file so far. In the Node Repository, search for the Row Filter node. Drag the Row Filter node to the workflow and connect its input port to the output port of the Column Filter. Right click on the Row Filter and choose Configure. In the Column to test area select Country using the drop down. Make sure use pattern matching is selected and use the drop down to select unknown. On the right side of the configuration dialog, select Exclude rows by attribute value. This will remove the observation where Country is unknown. Click OK. Execute the Row Filter. In the Node Repository, search for the Data Explorer node. Drag the Data Explorer node to the workflow and connect its input port to the output port of the Row Filter. (Leave the configuration to the default.) Execute the Data Explorer. Right click on the Data Explorer and select Interactive View: Data Explorer View. (This may take a few seconds to process.) The variable amount appears in the Numeric page. Click on the plus sign in the red circle to obtain a histogram of amount. Switch to the Nominal view in Data Explorer. The products and countries should be shown. V. Close the Data Explorer. Questions: How many variables are in the Filtered Table of the sales_data.csv file? How many observations are in the Filtered Table? Is the Skewness value for the variable amount consistent with the appearance of the histogram? Explain. Save and close the Workflow. Problem 3 This problem builds on the KNIME workflow example on preparing hospital data. The data set produced in the example is available in the file SubsetOfHospitalData.csv. Create a new workflow named Problem_KNIME_3. Read the data from the file SubsetOfHospitalData.csv. Add a Data Explorer node to the output of the Data Explorer. Execute the node. Add a GroupBy node to obtain the mean and median summaries of total charges by age group. Execute the node. (Note that you will have to increase the setting on Maximum unique values per group to one million or more.) Execute the node. Add a GroupBy node to obtain the mean and median summaries of total charges by APR Severity of Illness Description. (Note that you will have to increase the setting on Maximum unique values per group to one million or more.) Execute the node. Questions: Decide whether the better measure of central tendency of total charges should be mean or median. Does it matter which measure is better? Why or why not? Using your selected measure, Which age group has the highest charges? Again, using your selected measure, which APR Severity of Illness group has the highest total charges? Save and close the workflow. References "],["data-preparation.html", "Chapter 4 Data preparation 4.1 Introduction 4.2 Obtaining the needed data 4.3 Data cleaning 4.4 Feature engineering", " Chapter 4 Data preparation 4.1 Introduction Data preparation includes the processes of obtaining the data, cleaning it, feature engineering, and exploratory analysis. Data analysts report that 60 to 90 percent of the time on a project is often consumed by these processes. There are many possible ways to perform the data preparation processes and many steps might be taken before the data is ready for analysis. It is very important to document whatever steps are taken to join, clean, transform, or otherwise prepare the data so that reference can be made when new data is to be analyzed. *[A common problem when working with spreadsheets is that after making several modifications to the data, it is difficult to reproduce the steps unless they are somehow recorded. This is an added benefit of using KNIME since the workflows provide a graphical record of all steps taken.] 4.2 Obtaining the needed data One aspect of the analytics process that is surprising to many newcomers is that a companys or organizations data warehouse will not provide the data that is needed in the correct form. The data must be assembled, cleaned and custom fitted to the analytics problem.6 This process is most efficient and effective when the analyst has good knowledge of the business itself or has a domain expert on the project team. Data from a companys internal systems may need to be integrated with census data. Or weather data. Or traffic data. Or calendar data indicating holidays and weekends, etc. Data base tools such as joins involve linking records between two or more data bases by matching what is known as a key field. External data from the web, government, and commercial sources may be needed and integrated into a usable structure. Most predictive and data mining software requires that the data to be analyzed is in a well-formed tabular structure or flat file. Variables should be in the columns and observations in the rows. If the data source has variables in the rows and observations in the columns, the data table must be transposed. It is also important that the proper type of data is available. For example, if a predictive model is to be developed, it is necessary to have both predictors and outcome variables. If a binary outcome is to be predicted, then there must be enough instances of both binary outcomes. For example, if a predictive model to identify fraud requires observations which are both fraudulent and not fraudulent. As the data is being assembled one may find that it is not going to be appropriate or sufficient to address the problem identified in the first step. Then, the business problem might have to be revised or new ways explored to obtain the required data. 4.3 Data cleaning After obtaining the data it might include inaccurate, incomplete, or inconsistent values; there might be duplicate records, spelling errors, or simply errors from the original data capture. Trying to analyze data that has such errors is bound to lead to incorrect or misleading results. Data cleaning (aka data scrubbing or data cleansing) prior to model building is extremely important. It is not as simple as deleting records with apparent records. Instead, it is important maximize accuracy while not deleting information. The goal is to make sure that the data set submitted to the modeling process is of the highest quality. Some of the steps in the data cleaning process include: Removing unneeded columns (variables) and/or rows. Removing duplicate observations. Removing variables that have constant values for all observations. Removing data that is out of scope and/or not within the required time frame. Finding and correcting impossible or non-sensible values where logical constraints can be specified. Checking conditions across variable that must be satisfied. (For example, age in years should equal current year minus birth year, population density should equal total population divided by area, and so on.) Removing duplicate observations. Removing variables that have constant values for all observations. Identifying and dealing with outliers. Identifying and dealing with missing values. Finding misspellings, out-of-range, or impossible values. Checking to make sure that the range of each variable is correct. (For example, ensuring that dates, geographic regions, and other variables fall into the proper range.) Making sure that the formats and units of measurement are consistent. Are the variables measured in the same way over time or have the data definitions been changed? Are the variables from different data sets available at the same level of granularity? ensuring that regular expressions such as phone numbers, zip codes, social security numbers, etc. are in a consistent and correct format. This is an imposing list and there may be additional steps that might be needed. Also, some of the issues may not be relevant to a particular project. There is no simple process to follow since it depends on the specific variables in the data. KNIME provides several nodes that can be used for cleaning the data. (Figure 4.1) Figure 4.1: Some of the KNIME nodes useful for data cleaning. 4.3.1 Missing values The presence of missing values is one of the most difficult problems in analytics. In an ideal situation there would be valid values for every observation. However, it is very likely that some missing values will be present for some of the variables in most real data sets. The first thing to do is to try to find the value for the missing information. While it would be great if these values could be found, in practice it is unlikely. Missing values can be indicated in variety of ways in a data set, so analysts need to be on the look out for such indicators. Some of the ways that missing values are indicated in data sets include the following: Null 99, 999, -999, etc. -1 ? . NA None It is important to recognize how the original values were coded and to recode the values to a common indicator. This is especially true for numeric variable, where the missing value code could distort statistics such as the mean and variance. Missing values occur for many reasons. Missing values could be generated due to structural reasons. For example, a variable might ask if a person is homeowner or not. If yes, then a question might ask for the approximate value of the home. If no, the value of the home is missing, as it should be. In survey research, it is very common for respondents to refuse to answer questions about income. Respondents may simply not know the answer to a question, resulting in a missing value. 4.3.1.0.1 Types of missing values A classification scheme for missing values developed by (Rubin 1976) stipulated three types of missing values: MCAR - Missing completely at random. MAR - Missing at random. MNAR - Not missing at random. MCAR is the most favorable situation, since it means that the values which are missing have no relation with either target or predictor values. The observations with missing values will have similar distributions as those without missing values. The missing values in the rows of a data set could reflect random errors of data collection. MAR means that there are likely systematic differences between the observations with versus those without missing values. The assumption is that is related to some of the observed predictor variables. For example, missing values for income might relate to income and age, with those higher in income and older more likely to omit income. MNAR refers to situations where the likelihood of a value to be missing is not related to other variables but instead is related to the value itself. For example, in a data set on credit worthiness, a question on personal bankruptcies may be left blank by persons who did declare bankruptcy. This is the most pernicious type of missingness. It is very difficult to determine whether missing values are MNAR versus MAR or MCAR. Domain knowledge can be helpful to identify when MNAR is present. 4.3.1.1 Handling missing values If the proportion of observations with missing information is small in a large data set (e.g., less than 1%), those observations can probably be deleted. This is known as listwise deletion. Before dropping observations, however, it is important to check whether the missing values are concentrated in just a couple of variables or if many of most of the observations have at least one missing value. In the latter case, it is possible, even with a small percentage of missing values, that a data set with a million observations could be reduced to only thousands using listwise deletion. Another approach consider is to delete columns with large percentages of missing data. For example, if 75% of observations on a particular variable are missing, then the column might be deleted. This, of course, should not be done if the situation is deemed NMAR. 4.3.1.2 Imputation methods Imputation refers to the substitution of a value to replace the missing values in a data set. (If MNAR is suspected, the imputation techniques discussed in the following should not be used since the results of analyses can be biased and misleading.) There are many ways of doing so: For numeric variables: Substitute a constant value for all missing values in numeric variables. Substitute missing values with mean or median of the non-missing values. Regress non-missing values of a variable on other variables in the data set and then use the regression equation to predict the most likely values in the observations with the missing values. Use k nearest neighbors to find observations with similar values on the non-missing variables and then use those observations to impute the missing values, for example, by taking the average of the non-missing values from k nearest neighbors. For categorical variables: Substitute for the missing values the mode of the values from the non-missing values. Explicitly consider the missing values as simply another level of the categorical variable. Use a model to predict category level for the missing values from non-missing observations. One such technique to consider is decision trees. Missing values for each variable in a data set can be identified using the KNIME Statistics node. The KNIME node Missing Value handles missing values for both numeric and string variables. The node can replace all numeric and string variables in the same manner or the type of imputation can be set differently for each variable. The approach to handling missing values depends on the type of variable. The following options are available in the Missing Value node for numeric variables: For any type of variable with missing values: Do nothing. Remove the row with the missing value. Impute a fixed value  replace missing values with a user specified constant for strings or numeric variables. For numeric variables with missing values: Replace values with a summary statistic  the maximum, minimum, mean, median, or rounded mean. For time series data with missing values: Linear Interpolation  estimate a missing value via a straight-line trend either increasing or decreasing. Average Interpolation  take the average of a number of previous and next non-missing values. Moving average  calculate the mean of a specified number of look-ahead and look-behind values. For string or nominal variables: Most frequent value  replace the missing values with the mode of the variable. Additional methods for missing value imputation are available in R such as: MICE (Multivariate Imputation by Chained Equations) (mice 2021) Amelia (A Program for Missing Data) (Amelia 2021) VIM (Visualization and Imputation of Missing Values) (VIM 2021) mlr (Machine learning in R) (mlr 2021) 4.3.2 Outliers Outliers are defined as values of a variable that are an abnormal distance from other values in a data set - extremely high or extremely low. Whether a value is an outlier is not always easy to say as it may depend on domain knowledge. Extreme outliers can cause problems for many algorithms and can distort both supervised and unsupervised models. There are many graphical and statistical techniques that can be used to detect outliers. Outliers are sometimes due to data entry errors, e.g., when an extra zero or two is added to value by mistake. In other cases, a pattern of outlying values can be identified which may suggest how to deal with them. In general, it is not a good idea to simply remove outliers without further investigation. If we are very sure that an outlier is an error, the observation can be removed if the fraction of cases with outliers is very small. Graphical approach to detecting outliers Statistical methods One statistical method is to standardize a variable by subtracting the mean and dividing by the standard deviation. Then, assuming normality, values outside plus or minus two or three standard deviations can be evaluated as possible outliers. Another popular approach is to calculate Tukeys upper and lower fences which are shown graphically in histograms (Tukey 1977). The formulas are based on the first (Q1) and third (Q3) quartiles. Using this formula, k can be set by the analyst. The advantage of calculating the fences rather than using histograms is that the analyst can define what outliers are by setting k. For deviations from normality, k is typically set at 1.5. For extreme outliers, the value of k can be set at 3.0 Lower fence = Q1 - k(Q3 - Q1) Upper fence = Q3 + k(Q3 - Q1) The KNIME node Numeric Outliers can be used to detect and potentially deal with outliers. This node allows the user to specify k. To identify extreme outliers a typical practice is to set k equal to 3. The advantage of calculating the fences rather than using histograms is that the analyst can define what outliers are by setting k. The value of k depends on the purpose of the test. For deviations from normality, k is typically set at 1.5, although others tests for normality are available and normality of the predictor values is not required. Multivariate outliers To this point the discussion has focused on univariate outliers. Even if no univariate outliers are found, it is possible that outliers are present that can be only detected using multivariate methods. For example, the following scatterplot shows an outlier in red. Box plots are created on the X and Y variables. No outliers are identified. To identify multivariate outliers, the Mahalanobis Distance can be used. A detailed discussion of how to do this is in (Cansiz 2020). This article uses R. KNIME has a component called outliers 1 column that detects outliers using Mahalanobis distance that requires Python integration. An R code snippet can be developed for Mahalanobis distance using the R package mvoutlier (mvoutlier 2021). 4.3.2.1 Handling outliers Identifying potential outliers in a variable is usually straightforward, but deciding what to do about outliers is not simple. Outlying values could be legitimate and correct, so routinely deleting observations is not a recommended practice. In fact, in some cases detecting outliers is actually the object of analysis in applications such as intrusion detection in computer systems, determining fraud in financial activity, detecting unusual patterns in medical data, and other areas. (Aggarwal 2017) The KNIME node Number Outliers detects and provides several options to treat any identified outliers. The treatment options are: Remove each row containing an outlier (although this should not be done without careful consideration).7 Replace each outlier with a missing value indicator. Replace each outlier with the closest value within the permitted interval. Sometimes a log transformation will reduce variability in a predictor and lessen or remove outliers. The Math node in KNIME can be used to create transformations. Another approach is to bin a continuous variable by dividing it into groups. This creates an ordinal variable where the values in each bin can be controlled. The KNIME node Numeric Binner can be used define and create bins. The numeric variate is transformed into a string column. Of course, detail is lost is binning, but this is frequently a useful technique. 4.4 Feature engineering Feature engineering refers to steps taken to improve the accuracy of predictive models though data transformations, constructions of new variables from available variables, or acquiring additional variables to enhance model performance. Feature engineering, despite its technical connotation is really more of an art than a science that is learned through experience. However, there are still useful suggestions for starting the process. Features and independent variables are basically the same thing. Independent variables (and related terms such as predictors, inputs, and so on) are used by statisticians while the term features is more prevalent in the context of machine learning.8 4.4.1 Data transformations To identify the need for transforming predictor variables, descriptive statistics and data visualizations are important tools. Histograms, box plots, scatter plots, and other basic visualization techniques can be routinely applied to numeric predictors. 4.4.1.1 Handling skewed data After outliers, missing values, and other anomalies have been dealt with, the distributions of variables should be checked for extreme skewness. While predictor values in machine learning models do not have to be perfectly symmetrical, skewed variables can distort or bias model performance. The models can be overly sensitive to the tails of the variable distributions. Detecting skewness is usually straightforward using histograms. Skewness can be reduced by using transformations: For positive or left skew, try the following on a variable x: log(k + x) sqrt(k + x) where k is a large enough positive number to insure that the log and square root are applied only to numbers &gt; 0. For negative or right skew, try the following on a variable x: xn, where n is positive number. sqrt(k - x) where k is greater than the largest number in x. For the positive skew in the following example, the square root transformation created a more symmetric distribution the log function. For the negative skew, the square root of (k-x) worked best, Selecting the transformation is usually a trial- and-error process. 4.4.1.2 Transformations to achieve linearity For some machine learning methods, such as ordinary regression and logistic regression it is important that the relationship between the predictors and the target variable is a straight line or linear. Other predictive models, such as neural nets and decision trees are not as reliant on linearity, but transformations to linearity will not hurt for such models. To check for linearity a separate scatterplot of the target variable versus each continuous predictor usually diagnostic. If non-linearity is found, then transformations can be tried on either the target variable or the predictors. In my experience, it is better to try transformations on the predictors and leave the target variable as is. Transforming the target variable can sometimes cause other problems with modeling. With predictive models the objective is almost always to make predictions in the units of the original variable. Therefore, if the target variable is transformed using the log (a common suggestion in many texts), at the final stage of the analysis the inverse of the log must be applied to the result. This is not always a straightforward step. Performance measures such as R2 and root mean squared error cannot be compared between models where the target has been transformed versus not transformed. In general, its better to attempt to achieve linearity using the predictors. The following chart shows typical transformations for different non-linear relationships. (Figure 4.2) Figure 4.2: Transformations to achieve linearity. A demonstration of how these transformations work to straighten relationships is shown here. Figure 4.3: Examples of transformations. Figure 4.4: Examples of transformations. Figure 4.5: Examples of transformations. Figure 4.6: Examples of transformations. Deriving new variables New variables can be created used two or more variables already in the data set. This requires more than statistical and computer skills. It requires understanding of the business problem. For example, if you are predicting whether a person will cancel a cell phone contract or not, the data may have the number of calls made from the phone. A new measure might be the number of calls per day. Or the number of days when no call is made. Or the number of calls made divided by the number of calls received. 4.4.2 Data exploration Once the data set has been cleaned, the next step is to thoroughly explore the data. Some of this may have been done in the previous step, but here is where descriptive statistics and graphical representations are most useful. Exploratory data analysis is an approach/philosophy for data analysis that employs a variety of graphical and some descriptive statistical techniques to provide insight into a data set. Exploratory analysis is open-ended, since the goal is to generate clues about what is happening which can then open new avenues of investigation (hypothesis generation rather than the more formal hypothesis testing featured prominently in many books on statistics). This analysis can also support the selection of appropriate modeling techniques, uncover underlying structure in the data, and further assess how clean the data is. Exploratory analysis is not a highly linear, rigid process. Different people will literally find things differently. The software used for exploratory analysis should ideally be both agile and functional. Commonly used graphs include: Scatter plots Bar plots Histograms Box and whisker plots Density plots Bubble charts Commonly used descriptive statistics include: Mean, median, mode Variance, standard deviation Range, Max, Min Correlations and covariances Frequencies and percentages Percentiles and quartiles References "],["principal-components-analytics.html", "Chapter 5 Principal components analytics 5.1 Approaches to dimension reduction 5.2 Description 5.3 The PCA model", " Chapter 5 Principal components analytics One of the characteristics of data mining and predictive analytics problems is that there are lots of observations and lots of variables. Instead of having perhaps 1,000 or 2,000 rows of data, we might have tens of thousands or even millions of rows of data. Theres another dimension to this plethora of data, however. And that is the fact that we might have many variables. When I say many variables, I mean potentially a huge number of variables, perhaps one or 200 or even thousands. For example, one study predicting the onset of personal bankruptcy found only 2,244 cases of actual bankruptcy versus millions not reporting bankruptcy among users of credit cards. To predict these, the analysts began with 255 features to which they added missing value indicators and pairwise interactions that resulted in a set of over 67,000 potential predictors. This might not seem like a problem  we frequently hear that more data is better. In fact, many of the data mining algorithms and techniques that we talk about are specifically designed to comb through variables and pick the ones that are most predictive or most effective in terms of answering or business problem. It turns out in practice having too many variables, especially variables of the wrong kind, could actually reduce the effectiveness of the business analytics project. Several simulations and experiments with analytic techniques have shown that some of them are fairly sensitive to having irrelevant variables tossed in the mix. So if you were to throw in a random variable. This might actually cause the predictive ability of the model to degrade. The exact effect depends upon what kind of technique youre using, but many of the techniques suffer from the same problem. Having a large number of variables in a predictive model can increase computer processing time. This is probably not a problem when developing a model, but it can be a factor when the goal is to employ a model in real time where near-instantaneous results are desired. 5.1 Approaches to dimension reduction The most basic (and possibly most important approach) to dimension reduction is to apply domain knowledge. Using an understanding of the particular problem situation and the information and knowledge gained from experts, only variables potentially relevant to the analysis can be specified. Naturally, you have to be careful not to succumb to preconceived notions, so in using the domain knowledge approach it is sometimes advisable to err on the side of too many versus too few variables when building a predictive model. In some cases you want to drop variables, not just re-express them in a smaller subset. One approach is to create an index of some sort to rank each variable individually in terms of its simple relationship with the criterion variable. This can be done with simple correlations. This, of course, does not work in situations where multiple variables work together in ways not captured by examining one variable at a time. 5.2 Description Principal components analysis (PCA) is an unsupervised model, which means that there is no target or dependent variable that you are trying to estimate or predict. The aim of principal components analysis is to reduce the dimensionality of data set from p variates to a smaller number of k variates so that most of the information in the original data set is preserved. It is used in many disciplines including psychology, biology chemistry, education, astronomy,and business. It is one of the oldest multivariate techniques, with early developments by (Pearson 1902) and (Hotelling 1933)) Because of this, it is not necessary to create training and validation subsets of the data. There is really no need to validate a principal components analysis since it is a purely descriptive summary of the data. The basic objective of PCA is re-specify variables using a new set of axes that are mutually uncorrelated or at right angles. If as many principal components are extracted from a set of variables as there are variables, then all of the variance is captured. Components are extracted in order of the variance captured, so that the first component has as much variance as possible along one dimension, the second component has as much variance along a second dimension which is uncorrelated with the first, etc. Typically, PCA is performed on the covariance or correlation matrix; the results will not be the same in both cases. Typical steps in performing PCA are: Opening an n X p data set. Selecting variables to submit to PCA. Computing the correlation (or covariance) matrix. Extracting p components. Determining k , the number of components to retain. Obtaining a matrix n X k of scores on the components to be used in further analyses. There are some issues with PCA: No strong criteria to test the solution against exist. Deciding on the number of components is somewhat arbitrary. Virtually any data set (even bad data) can be submitted to PCA and (meaningless) results will be obtained. 5.3 The PCA model The mathematics of principal components are fairly straightforward, but most presentations in texts and articles are often opaque with terms such as eigenvalues, orthogonality, etc. There are many excellent expositions of the details, e.g., (Shlens 2003). Instead of repeating this development here, I will provide what is hopefully an intuitive explanation of PCA. To begin, PCA is only meaningful with data sets in which two or more variables are correlated or covary. Typical applications of PCA involve data sets with many variables, some of which are nearly redundant. PCA is not usually the final objective of a project, although there are times when this is the case. Instead, PCA is a dimension reduction tool to make further analyses more effective and efficient. PCA is sometimes a precursor to ordinary regression, logistic regression, cluster analysis, and other techniques. So, given the assumption that there are correlated variables in a data set, the question is how to extract much or most of the information from the data set using a reduced set of variables. The process can be intuitively explained by asking the question, Which one component accounts for the most variance in the data set? A principal component is nothing but a special weighted average of the observed variables. To being, consider the toy data set shown in Table 5.1. Table 5.1: Toy data set for PCA demo Observation X1 X2 X3 1 1 7 3 2 7 1 21 3 1 5 3 4 4 2 12 5 3 4 34 6 2 6 6 7 5 5 15 8 6 4 18 9 9 8 27 10 13 44 35 11 15 7 12 12 16 10 1 The correlation matrix for the toy data set is: Observation X1 X2 X3 Observation 1.000 0.848 0.429 0.168 X1 0.848 1.000 0.456 0.165 X2 0.429 0.456 1.000 0.435 X3 0.168 0.165 0.435 1.000 To keep things simple, we can standardize the data so that the means of each variable X1, X2, and X3 are all 0.0 and the respective standard deviations are 1.0. This is consistent with performing PCA on the correlation matrix rather than the covariance matrix. The standardized variables are shown in Table 5.2. Table 5.2: Standardized data set for PCA demo Observation X1sd X2sd X3sd 1 -1.096 -0.139 -1.067 2 0.031 -0.663 0.459 3 -1.096 -0.314 -1.067 4 -0.532 -0.576 -0.304 5 -0.720 -0.401 1.562 6 -0.908 -0.226 -0.813 7 -0.344 -0.314 -0.049 8 -0.157 -0.401 0.205 9 0.407 -0.051 0.968 10 1.159 3.099 1.647 11 1.534 -0.139 -0.304 12 1.722 0.124 -1.237 Mean 0.000 0.000 0.000 Standard deviation 1.000 1.000 1.000 Next, a weighted average of the observations is taken using a vector a, which has three values (one for each variable). To being, random numbers are entered for the elements of a as shown in Table 5.3. Table 5.3: Random weights 1 - 3 Variable Weight 1 0.103 2 0.938 3 -0.331 Sum of squares 1.000 The weights are applied to each row as shown in the following equation. The variance of this weighted sum becomes the eigenvalue first principal component. The weights are random at this point. The weights will be adjusted to maximize the variance of the first component, subject to the constraint that the sum of squared values of the weights is 1.0. This has to be done because otherwise the variance will increase indefinitely. Table 5.4 shows the results so far. Table 5.4: Variance of first component with random weights Observation X1sd X2sd X3sd Weighted sum 1 -1.100 -0.140 -1.07 0.11 2 0.030 -0.660 0.46 -0.77 3 -1.100 -0.310 -1.07 -0.05 4 -0.530 -0.580 -0.3 -0.49 5 -0.720 -0.400 1.56 -0.97 6 -0.910 -0.230 -0.81 -0.04 7 -0.340 -0.310 -0.05 -0.31 8 -0.160 -0.400 0.2 -0.46 9 0.410 -0.050 0.97 -0.33 10 1.160 3.100 1.65 2.48 11 1.530 -0.140 -0.3 0.13 12 1.720 0.120 -1.24 0.70  Weights 0.103 0.938 -0.331 Variance = 0.81 The calculation for the first observation using the random weights is: \\[\\begin{equation} \\begin{split} Component1, obs 1 = &amp; (0.103) \\; \\; \\; \\times (-1.10) + \\\\ &amp; (0.938) \\; \\; \\; \\times (-0.14) + \\\\ &amp; (-0.331) \\times (-1.07) \\\\ &amp; = 0.11 \\end{split} \\tag{5.1} \\end{equation}\\] Next, the variance of the first component was maximized (with the constraint noted), resulting in the following estimates for the first principal component. (Table 5.5) Table 5.5: First component and variance (eigenvalue) Observation X1sd X2sd X3sd Weighted sum 1 -1.100 -0.14 -1.07 -1.24 2 0.030 -0.66 0.46 -0.18 3 -1.100 -0.31 -1.07 -1.36 4 -0.530 -0.58 -0.3 -0.83 5 -0.720 -0.40 1.56 0.17 6 -0.910 -0.23 -0.81 -1.06 7 -0.340 -0.31 -0.05 -0.42 8 -0.160 -0.40 0.2 -0.24 9 0.410 -0.05 0.97 0.69 10 1.160 3.10 1.65 3.53 11 1.530 -0.14 -0.3 0.58 12 1.720 0.12 -1.24 0.36  Weights 0.539 0.66 0.524 Variance = 1.72 The variance of the first component is 1.72. Since there are three variables in the original data, each with a variance of 1.00, the total variance in the original data is 3.00. So, the first component accounts for 1.72/3.00 = .573 = 57.3% of the variance. Next, the second principal component was computed by again maximizing the variance of that component subject to the constraint that the sum of squared values of the weights is 1.0 and that the second component is at right angles (i.e., orthogonal to the first component. The second component accounts for 0.83/3.00 = .270 = 27.0% of the variance. The total accounted for by the first components = (1.72 + .83)/3.00 = .850 = 85.0% of the variance (Table 5.6). Table 5.6: Second component and variance (eigenvalue) Observation X1sd X2sd X3sd Weighted sum 1 -1.100 -0.140 -1.07 0.02 2 0.030 -0.660 0.46 -0.32 3 -1.100 -0.310 -1.07 0.02 4 -0.530 -0.580 -0.3 -0.15 5 -0.720 -0.400 1.56 -1.63 6 -0.910 -0.230 -0.81 -0.04 7 -0.340 -0.310 -0.05 -0.21 8 -0.160 -0.400 0.2 -0.26 9 0.410 -0.050 0.97 -0.42 10 1.160 3.100 1.65 -0.35 11 1.530 -0.140 -0.3 1.27 12 1.720 0.120 -1.24 2.08  Weights 0.688 0.014 -0.726 Variance = 0.83 The third component (not shown) would account for only 15.0% of the variance. This toy example demonstrated that most of the variance in the data would only require two of the three components, reducing the dimensionality of the data set. Furthermore, the three components are uncorrelated: (Table 5.7). Table 5.7: Correlation matrix of components Component PC1 PC2 PC3 PC1 1 0 0 PC2 0 1 0 PC3 0 0 1 References "],["evaluating-predictive-models.html", "Chapter 6 Evaluating predictive models 6.1 Introduction 6.2 Training, Testing, and Validation samples 6.3 Evaluating continuous versus discrete targets", " Chapter 6 Evaluating predictive models In this chapter various metrics for evaluating predictive models, both for discrete targets and continuous targets are discussed. The content here will be used throughout the remainder of the text as different algorithms are presented. 6.1 Introduction In predictive analytics applications it is common practice to try multiple different models to assess predictions using a training sample and then to select the best of model in terms of some criterion. There are, however, many things that can go wrong. The data may be bad, the underlying model used may be wrong, or the right predictors have not been included. This can result in false positives and false negatives in binary classification models. One of the outcomes is usually the one were concerned with or the one with a favorable result. If the model accurately predicts this outcome, then its a true positive. If the model accurately predicts the outcome we are not interested in, then its a true negative. While looking at many different models is normally frowned upon in statistics, with large data sets where prediction is the objective, it is common practice. Even if a single model is used, such as logistic regression, there are different formulations to be tried, e.g., different predictor variables, different transformations of the predictors, and so on. Because of the practice of trying different models, we need a means to compare the models. 6.2 Training, Testing, and Validation samples Since the process of building a predictive model usually involves trying several (or many) different models to determine which performs best on a given data set, it is important to have a separate data set that was not used to build the model. The strategy for validating a predictive model depends on the number of observations available. The question is whether to create two subsets (training and validation) or three (training, testing, and validation). There should be a sufficient number of observations such that a training set can be created with at least 50 - 100 observations per predictor variable being considered. If the data set has enough observations, then a common approach is to create three subsets: training, test, and validation.9 A model is created using the training set. Then, the performance of the model is assessed using the test set. If the model doesnt perform well with the test set, the process returns to the training subset and retrained using different settings on the algorithm or a totally different algorithm. different different models or setting assumptions or different techniques. This process may be repeated several times. The relative proportions for the three subsets is somewhat arbitrary, but typical ratios are 60/20/20, 50/30/20, 50/25/25 for the training, test, and validation sets respectively. KNIME and other software tools provide convenient ways to construct training, testing, and validation sets. These subsets are created via a random sampling process as a way of avoiding bias. One approach where the number of observations is limited is to create just training and validation sets. There is no firm rule for what the split should be, but typical ratios are 50/50, 60/40, 70/30, 80/20 for training and validation sets respectively. However, with only two subsets of data, using the validation set to inform the model building process will not provide a proper test of the ability of the model to predict new data. As example, some procedures, such as stepwise regression, use the validation set to evaluate the accuracy of each iteration. Where only two subsets are used, the validation be held separate from the model building process. When a final model has been developed, the validation set can be used to evaluate performance on new data. Testing can be done using only the training data via k-fold validation. With k-fold performance assessment, the training data is randomly divided into k groups, where k is typically five or ten. The analysis is run k times, with each of the k subsets assigned to the role of test data and the remaining data used to build the model. The goal is to create a model that will perform on unseen data, which is held out in the validation set. The process is illustrated in Figure 6.1 for k = 5. The training data is randomly divided into five equally sized subsets. Then, the first subset is held out and the remaining four subsets used to build a model. The model is assessed on the held out first subset. This process is repeated five times. The model performance is then averaged across the five iterations. Figure 6.1: K-fold validation. KNIME and other software tools provide convenient ways to construct training, testing, and validation sets. These subsets are created via a random sampling process as a way of avoiding bias. KNIME also has nodes to perform k-fold validation. 6.3 Evaluating continuous versus discrete targets There are two main types of supervised models for which evaluation measures are needed. First, prediction models involve continuous dependent or target variables. This is the case typically with multiple regression, although there are other techniques which make predictions of continuous variables as well. The second main type is performance evaluation for classification. which involves assignment of case to a group. The second main type is performance evaluation for classification, which involves assignment of case to a group. 6.3.1 Evaluating performance with continuous targets The objective is to find out how well the model predicts new data. So, prediction metrics should be computed on a new data set, e.g., the validation and/or test set or with k-fold validation methods. Sometimes it is useful to compare the performance with the training data with the holdout or new data to assess whether the model overfit the training data. Typical measures used to evaluate continuous targets are listed below where y contains the actual target values and \\(\\hat{y}\\) contains is the predicted values using the validation data. R-squared ${R}^2 can be interpreted as the variance explained by a model. As such, the value ranges between 0 and 1.0. ${R}^2 is probably the most familiar measure for continuous targets because of its use in multiple regression. The formula for ${R}^2 is: \\[\\begin{equation} {R}^{2} = 1 - \\frac { \\sum_{i=1}^{n} (\\ y_i -\\hat{y_i} )^{2} } { \\sum_{i=1}^{n} (\\ y_i -\\bar{y_i} )^{2} } \\end{equation}\\] MAE The mean absolute error is measured in the same units as the data. Because it does not square the errors, as with other measures, it is less sensitive to the occasional very large errors. The formula for MAE is: \\[\\begin{equation} MAE = \\frac{1}{n} \\sum_{i=1}^{n} |{y_i}-\\hat{y_i}| \\tag{6.1} \\end{equation}\\] MAPE The MAPE, or mean absolute percent error, is one of the more attractive measures because the report as a percentage makes intuitive sense. However, it has some limitations. First of all, if any of the actual values are zero, then the error becomes apparently infinite because there would be division by zero. Second, it has been found to favor models that are systematically predicting values that are too low rather than too high. Thats because for estimates that are too low, the percentage error cannot exceed 100%. But for predictions that are too high, there is no upper limit to the percentage error. Finally, if there are both positive and negative values in the estimates, MAPE should not be used. The formula for MAPE is: \\[\\begin{equation} MAPE = \\frac{1}{n} \\sum_{i=1}^{n} {\\left|\\frac{{y_i}-\\hat{y_i}}{y_i}\\right|} \\tag{6.2} \\end{equation}\\] RMSE This is the square root of the mean of the squared error. This metric captures how far the predicted values are from the actual values in the same units are the target variable. One issue with the root mean squared error is that it is affected more by the occasional large error than MAE or MAPE because of the squaring of errors. Whether this is a problem or not depends to some extent on the cost or seriousness of large errors in the situation being modeled. If the costs are roughly linear with respect to the error, then the statistic might not be the best one to use. The formula for RMSE is: \\[\\begin{equation} RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} ({y_i}-\\hat{y_i}){^2}} \\tag{6.3} \\end{equation}\\] 6.3.2 Evaluating performance with classification models This discussion is focused on assessing performance where the target variable takes on two nominal levels (i.e., binary). The binary case is not the only situation that might be encountered, however. In some cases where the target has three or more categorical levels, the assessment can be reduced to binary for evaluation purposes. This is done by selecting one of multiple levels as the positive or 1 level and assigning the rest of the levels to the negative or 0 level. The basic structure for evaluating binary models is the contingency table or classification matrix (sometimes called the confusion matrix). It is a two by two table with the actual classifications on the rows and the predicted classifications on the columns.10 2 X 2 Classification Matrix Figure 6.2 shows a general two by two contingency table with the actual in the rows and the predicted in the columns. The predictions can be either positive or negative; these can be thought of as buy or not buy, try or not try, churn or not churn in the case of telecommunications, error or not an error, or even fraudulent or not fraudulent. Figure 6.2: A labeled 2 X 2 table. Each of the cells can be labeled as True Positive and True Negative (correct) or false positive and false negative (incorrect). To begin the evaluation, the naive rule can be used as basic benchmark. This rule classifies all records as belonging to the most prevalent actual level. The hope is that the model will perform better than the naive model. There are several measures that are frequently used to assess model performance using a 2 X 2 table. These are: Accuracy = (True positive + True negative) / Total cases; this is also known as the fraction correct. Sensitivity = (True positive) / (True positive + False negative); this is also known as recall, hit rate, or the true positive rate. Specificity = (True negative) / ( True negative + False positive); this is also known as true negative rate. Precision= (True positive) / (True positive + False positive); this is also known as positive predictive value. Negative predictive value = (True negative) / (True negative + False negative). F-score = 2 /[(sensitivity-1) + (precision-1)] It is important to select a metric appropriate to the situation being modeled. Accuracy is a good measure when the number of false positives and false negatives are approximately equal and the cost or seriousness of false positives and false negatives are similar. Consider the hypothetical data results shown in Figure 6.3. Figure 6.3: Evaluation metrics for two situations. The results on the left are for two different models for testing for Covid-19. The results on the right are for two models for SPAM detection. All four of the 2 X 2 tables have the same accuracy, yet the models are not equally favorable. Accuracy is not a good measure for these two situations since the consequences of false positives and false negatives are not the same. For the Covid tests model 1 is preferred to model 2 because only 10 actual Covid cases were mistakenly classified as Not Covid. Sensitivity (0.998) is the best measure of performance since it is arguably more important to avoid classifying cases with Covid as not having Covid. If a person without Covid is classified as having Covid, this of course causes stress on the individual, but this can be remedied by conducting further tests. On the other hand, for the SPAM tests the best metric is Precision and model 2 has the higher precision (0.998). Classifying cases where SPAM exists as not being SPAM is more important than filtering messages that are Not SPAM as SPAM. In the latter case, critical messages might never be seen by the email user. In general, if false positives are not as serious as false negatives, sensitivity is a good metric. If false positive are more serious than false negatives, precision is a good metric. 6.3.2.1 Evaluating models with class imbalance There are situations where the outcome of interest in a binary prediction model occurs infrequently. This causes an imbalance in the outcomes. For example, say we are developing a model to predict fraud and we have 50,000 cases where only 500 cases are fraudulent and 49,500 are not. There are many domains where imbalance occurs, including: Churn prediction at telecoms most users do not churn. Predicting insurance fraud - most claims are not fraudulent. Diagnoses of rare medical conditions Online advertising click-through rates - most viewers will not click. Pharmaceutical research - most potential modules useful for further development. Many classification models will not handle this situation very well. One likely outcome in such situations is that the model will simply predict all cases to not be fraudulent. After all, for the example cited, if all 50,000 cases are classified as non-fraudulent, then accuracy equals 49,500/50,000 or 99%. accuracy! There are several ways to handle situations of severe class imbalance. One way is to use stratified sampling on the training set to force a balance in the two outcomes. In the case of the fraudulent data case, assume a training set of 40,000 cases and a validation set of 10,000 cases are formed stratified by the presence of fraud. In the training sample, it is expected that there will be 400 cases of fraud in the training set and 39,600 cases with non-fraud. A random sample of 400 is taken from the 39,600 non-fraud cases. This creates a balanced data set with 400 cases each of fraud and non-fraud. A model is then created using the balanced data set. The evaluation metrics from this model are not very useful, however. Instead, the model should then be applied to the 10,000 cases in the validation set and then evaluation metrics computed. A disadvantage of this approach is that most of the cases in the training set are not used to create the model. Another approach is to oversample the rare cases in the data set. In the training data set in the fraud situation, the 400 fraud cases are repeatedly copied 99 times so that there are 39,600 cases of fraud and non-fraud in the set. The validation set is left alone. A more sophisticated approach is to oversample using similar cases. Using the characteristics of each case that is in the minority, a set of similar cases is selected. This is implemented in the R package smotefamily (SMOTE 2019) and the SMOTE node in KNIME. Other approaches to dealing with imbalance include the following. Lower the cutoff to increase the number of predictions of the minority class. Adjust the prior probabilities (e.g., in nave Bayes or discriminant analysis). Weight the minority class more heavily, where case weighting is available in the algorithm; this is essentially like oversampling. Use costs to differentially weight specific types of errors. Examples of using some of the these approaches will be discussed in the chapters on classification prediction models. 6.3.2.2 ROC curves Another useful method for assessing the accuracy of predictive binary classification models is the ROC (receiver operating characteristic) curve. ROC curves show the tradeoff between sensitivity and (1-specificity) or the tradeoff between the true positive rate and the false positive rate for a predictive model. These curves are especially useful in comparing the performance of two or more models. The ROC curve in Figure 6.4 shows the line of random performance in red. The blue line represents a model that achieves much better than random performance. The curve is generated by varying the cutoff threshold for assigning cases to one of the binary outcomes and computing the Sensitivity and 1 - Specificity and plotting the values. The cutoffs for 0.0 to 1.0 which result in the curve are shown in the figure. The closer the ROC curve is to the upper left corner (1.0, 1.0), the more accurate the model. A measure of the quality of the model is the area under the curve or AUC, which in this case is approximately 0.72. The maximum possible area for a model that predicts perfectly is 1.0; the area under the red line for a model that only predicts randomly line is 0.50. Figure 6.4: Example of an ROC curve. The concept of a tradeoff between true positives and false positives in a 2 X 2 table is illustrated in Figure 6.5. Figure 6.5: Tradeoff between true positives and false positives. If a threshold or cutoff is selected that achieves .85 true positives (in the chart on the left), then the model represented by the blue line will result in about .45 false positives. To lower the percentage of false positives to about .20 (in the chart in the center), the tradeoff shows that true positives are identified only about .65 of the time. The only way to achieve higher true positives and fewer false positives is to create a model that results in an ROC curve above and to the left, which is shown in panel on the right. This model achieved a true positive rate of .85 with only a .10 false positive rate. References "],["multiple-regression.html", "Chapter 7 Multiple regression 7.1 Introduction 7.2 Regression techniques 7.3 Regression for explanation 7.4 Regression for prediction Appendix: A brief history of regression Problems", " Chapter 7 Multiple regression 7.1 Introduction This chapter covers one of the most versatile and useful techniques, ordinary least squares regression. Included in this chapter is a discussion of the distinction between explanatory and predictive modeling. Several examples of predictive modeling using regression are included. Linear regression is arguably the most well-known of the many algorithms used in predictive analytics. There are several reasons for this. First, it is a logical, linear model which has many uses and is conceptually attractive. Linear relationships are easy to think about. Second, the technique itself is relatively easy to program, so there are many, many software tools available. It can easily be programmed in any language with just a few statements. Third, it is very flexible  it can be applied to many types of problems, even those that at first might not seem to be linear regression candidates. Many phenomena can be cast, at least approximately, into a linear model. Fourth, regression has multiple, distinct uses. It is often used to build models that explain how one or more independent variables affect a continuous, dependent variable. It is also used for control, in the sense that regression models can help identify cases that are in error or problematic. Finally, the focus in this course will be on regression for prediction. Fifth, regression has a long and elaborate history of development. It is a huge topic. There are many books and courses devoted to the subject. Its been under development for more than 100 years. The basic idea is quite simple, but there are many exceptions, special cases, and assumptions that might not fit a particular situation. In ordinary regression, the target variable is continuous, but over the years many modifications to the basic model have led to new regression-type models such as Cox regression, Poisson regression, logistic regression, and others. 7.2 Regression techniques There exist several algorithms that perform different forms of regression that were developed to better align with the objectives of the analysis and characteristics of the data. Ordinary least squares, the topic of this chapter applies to problems where there is a single continuous target variable and one or more predictor variables, which can be continuous or categorical (or a mixture of both). The models covered in this chapter are: Ordinary linear regression Forward selection of features Backward selection of features Stepwise selection of features Lasso regression Note that regression-type problems can also be addressed using other techniques such as neural networks, support vector machines, and others. These will be discussed in other chapters 7.3 Regression for explanation Regression is used differently in different disciplines. In economics, psychology, sociology and other fields, regression is mostly used with the goal of developing causal explanations.(Shmueli 2010) It is assumed that most readers of this text have been introduced to multiple regression. The context for regression applications in most statistics texts is on building models with the objective of making causal inferences and explanation. A theoretical model is posed, predictor variables are identified, and observational data is collected (cross-sectional, longitudinal, or both). Hypotheses based on the theoretical model are tested using regression and related techniques. The aims of such studies include assessing both the statistical significance and magnitude of independent variables. This is a challenging task and accomplishing this with any degree of confidence requires considerations about several strict assumptions. Violations of any of these assumptions can cast doubt on the validity of the conclusions made from the analysis so three questions should be asked about each: (1) Is the assumption met in the current situation? (2) If not, how serious are the consequences of violation of the assumption? (3) If the assumption is violated and critical to the analysis, can remedial techniques be used to alleviate the consequences? The assumptions need to be met to confidently deduce statistical significance of the predictors and the overall regression model itself, the interpretability of the effect of each predictor on the target, and confidence intervals on estimates made by the model. The Gauss-Markov theorem states that for an additive linear model ordinary lease squares regression produces unbiased estimates that have the lowest variance of all possible linear estimators. This is usually given the acronym BLUE for Best Linear Unbiased Estimators. The four Gauss-Markov assumptions are: The dependent variable is a linear, additive function of a set of predictors plus an error term. The error term has a conditional mean of zero. The variance of the error term is constant for all values of the predictors (homoskedasticity). The error term is independently distributed (no autocorrelation). To this list, some or all the following assumptions are sometimes added: Predictor variables are not correlated with the error term. There is no perfect collinearity among the predictors. The error term is normally distributed (and relatedly, there are no outliers or observations with undue influence). The number of observations must be greater than the number of predictor variables (usually 5 to 10 times as many observations as predictors.) All the predictor variables have non-zero variability, i.e., are not constants. Violation of these assumption can result in biased estimates of the parameters of the regression model. The estimated size and variance of predictor coefficients can be biased upward or downward. In some cases, this can render the results totally misleading. Example: Omitted variable bias There is considerable controversy on the question of whether increased spending on public education leads to better student outcomes. For example, the Heritage Foundation published a report that concluded: Federal and state policymakers should resist proposals to increase funding for public education. Historical trends and other evidence suggest that simply increasing funding for public elementary and secondary Education has not led to corresponding improvement in academic achievement.(insert reference) One approach to investigating this question is to look at expenditures per pupil in all 50 states plus the District of Columbia and measuring student performance. The performance criterion in the data set is the average combined SAT scores. The first 10 observations are shown in Table 7.1. The hypothesis is that students in states in the that spend more per pupil on primary and secondary schools should be better prepared for college and thus perform better on the SAT college prep test. SAT scores were regressed on expenditures (in 1,000s). The coefficient on expense is significant (p &lt; .001) and negative (-22.28), which suggests that increasing the spending on education results in lower performance on the SATs. If this is true, this finding has important implications for school funding. Those who argue against spending more on education may be right. It turns out that an important variable was omitted from the regression: the percentage of students taking the SAT by state. There was considerable variation in this measure, as shown in the chart in Figure 7.1. In Connecticut, more than 80% of students took the SAT while in Mississippi only 4% took the SAT. If the regression is run with expenditure per pupil and percentage taking the SAT, the conclusion is different. The coefficient on expense is positive (8.60) and significant (p = 0.046). The second analysis controlled percentage taking the SAT. This means that the effect of expenditures is estimated removing the effect of percentage taking the SAT. By including one or more control variables in a regression, their effects are separated from the variable of interest. The control variable itself is usually not of primary interest to the analyst. The point here is that omitting a key variable can lead to incorrect conclusions in regression. Explanatory models are more difficult and demanding than using regression for prediction. Figure 7.1: Percent taking SAT by state and DC. Table 7.1: Expenditures on education and SAT scores. State + DC Expenditures per pupil Combined SAT score Alabama 3627 991 Alaska 8330 920 Arizona 4309 932 Arkansas 3700 1005 California 4491 897 Colorado 5064 959 Connecticut 7602 897 Delaware 5865 892 District of Columbia 9259 840 Florida 5276 882 7.4 Regression for prediction Predictive modeling is the process of using a statistical model such as regression to make predictions on new or future observations of the predictor variables. Image recognition, natural language processing, and many business problems there is frequently an emphasis on prediction rather than explanation. Breiman (L. Breiman 2001) described what he considered two cultures in statistics. He called the most prevalent approach the data modeling culture. Applications typically involve relatively small numbers of observations and a smaller number of predictors, and the most important aim was causal inference. This approach is consistent with the explanatory use of regression. A second approach (or culture) Breiman called the algorithmic culture, which focuses almost purely on predictive accuracy rather than statistical tests and interpretation of model parameters. These applications typically involved huge data sets, sometimes with millions of observations as well as many potential predictors. (In some cases, the number of potential predictors was even larger than the number of observations.) This new paradigm began in the 1980s and gained in popularity from the 1990s to the present. This is the domain of data mining, predictive analytics, machine learning, and big data. Many have been critical of this newer approach, beginning with comments on Breimans by Cox (Cox 2001), Efron (Efron 2001), and other leading classical statisticians. Despite the criticism there have been many successful applications of pure prediction algorithms (Efron 2020), in business, medicine, biology, and other fields. The potential for misleading or even dangerous predictions using these models remains and it is the responsibility of the analyst to carefully acknowledge both limitations as well as benefits of such models. 7.4.1 Revisiting regression assumptions When regression is used for prediction rather than explanation, the role of assumptions is changed. The primary objective is prediction accuracy assessed on new data. This is usually assessed by creating training and test subsets of a data set with many observations. The training data is used to develop the predictive model and the test set is used to represent the new data.11 In fact, some of the assumptions may be violated, but if the model predicts well, it can be useful. Allison (2014) noted that outside of academia, regression is primarily used for prediction. He further notes that there are important differences between using regression for explanation versus prediction. Specifically, he writes that omitted variable bias is much less of an issue when the goal is prediction. Multicollinearity (if it is not extreme) can be tolerated in predictive model since the coefficients on individual variables are not of primary concern. Measurement error in the predictor variables leads to bias in the estimates of regression coefficients, but again the estimates on predictor coefficients is not as critical. (Of course, with predictive applications, high degrees of measurement error can make predictions less accurate). Other differences between the two uses of regression include the following: Out-of-sample prediction accuracy is more important than R2 with the original data set. However, high R2 in the hold-out sample or samples is important for prediction. With causal modeling, even low R2 values using the original data set are not as much of a concern. The hypothesis tests on the predictors in such applications are more important. Normality of the error term is not a requirement since hypothesis testing is not the goal of predictive regression. The perspectives on predictive regression discussed above are controversial and many statisticians would undoubtedly disagree with many of the statements made. In truth, causal modeling is important and if done well can lead to insights that are likely to have longer-run usefulness. However, for present purposes, the focus in this chapter on regression and the other chapters in the book will be on prediction. It is critical to understand that when a predictive model is created that ignores the traditional assumptions of regression, using the results as if causality has been established, it can lead to huge mistakes. While it may be of interest to find which predictor values had the greatest impact on the target variable, it cannot be assumed that manipulation of these impactful variables is likely to lead to expected changes in the target. Correlation, not causality, is obtained with predictive models. The one assumption that is critical for accurate predictive regression models is that the dependent variable is a linear, additive function of the predictors. Other algorithms, discussed in later chapters, can more or less automatically deal with complications such as non-linearity. With regression, however, it is up to the analyst to explore possible non-linear relationships and potential interactions among predictors. Failure to do so can lead to sub-optimal models. If some of the same predictors are transformed in some way, the accuracy of the regression model could be improved without additional collection of data. 7.4.2 Prediction example: Used Toyota Corollas A dataset with the prices of used Toyota Corolla for sale during the late summer of 2004 in The Netherlands was obtained from Kaggle. (reference) It has 1436 records containing details on 38 attributes, but only a subset was extracted with 1,000 observations and variables is used for this example. The variables included are price (the target variable) and the following predictors: Age of the car (in months) Mileage (in KM) Fuel type (Diesel, natural gas, or gasoline) Horsepower Automatic transmission (Yes or No) Engine displacement (in CC) Weight (in KM) The goal is to predict the price of a used Toyota Corolla based on its specifications. The KNIME workflow for this analysis shown in Figure 7.2. Figure 7.2: Workflow for OLS regression with Toyota Corolla price data. The regression results are shown in Table 7.2. The measures show that the results for the training and test data are comparable. The training data shows better results for R2 and root means square error while the measures are slightly better with the test data for mean absolute error and mean absolute percent error. Table 7.2: Regression with Toyota1000 data. Measure Training data Test data R^2 0.871 0.825 Mean absolute error 1028.513 1005.412 Root mean squared error 1385.478 1455.483 Mean absolute percentage error 0.094 0.091 A scatterplot of predicted price versus actual price for the test data indicates reasonable results. There is one clear outlier at row 221 (Figure 7.3). The presence of this outlier most likely explains why the measures involving the absolute value rather than the squared residuals are worse for the test data. Squaring the residuals (for R^2 and root mean square error)results in a greater effect on the performance measure. Figure 7.3: Predicted versus actual prices of Toyota Corollas. Figure 7.4: KNIME workflow for regression analysis of apartment prices. Appendix: A brief history of regression The term regression and the basic idea behind it came from a British scientist, Sir Francis Galton. He was interested in heredity and evolution; this is not surprising since he was a cousin of Charles Darwin. (Stanton 2001) Sir Francis Galton. Galton asked the following question: What is the relationship between the heights of children and heights of their parents? He was an empirical scientist. So, he collected data on pairs of parents and children and carefully studied the data. He created a chart with the childrens heights (as adults) versus the height of their parents. (Figure 7.5) He fit a line to the data such that the line was as close as possible to the observations - the blue line shown in the chart. Once Galton fitted the line, he calculated the intercept and slope. The slope and intercept of the fitted line, .611 and 26.4 respectively, are shown at the top of the chart. Figure 7.5: Height of children vs. height of parents. As expected, Galton found that tall parents had tended to have tall children, but the relationship was not perfect. A one-to-one relationship between parents heights and the height of their children would follow the red line in Figure 7.5. Instead, there was evidence that the adult height of children of tall parents regressed or reverted to the mean height of all adults, shown as the blue line in the chart. For example, if a parents height was about 72 inches, the expected height of the children would be 70 inches. This was also found to be true for shorter parents. If a parents height was about 64 inches, the expected height of the children would be 65 inches. So, the heights of children also tended toward the mean height of all adults. Galton use the term regression as this idea of reverting or returning to a norm. People then took this idea of drawing a line through data and estimating the equation, applied it to many other types of problems, and called the process regression, after Galton, even though the context and meaning were different. Karl Pearson, a collaborator, and lab assistant to Galton, developed the mathematics of regression that we have today. Problems Problem 1 References "],["logistic-regression.html", "Chapter 8 Logistic regression 8.1 Example with a single predictor 8.2 Example: Predictive analytic in HR 8.3 Predictor interpretation and importance 8.4 Regularized logistic regression 8.5 Probability calibration 8.6 Evaluation of logistic regression", " Chapter 8 Logistic regression Many analytics problems can be expressed as the prediction of which of two outcomes is more likely. The target variable in such applications is binary and expressed as 1 or 0. There are many questions with a binary outcome, such as: Buy/Not buy. Success/Failure. Heart attack/No heart attack. Continue/Not continue. Fraud/No fraud. Cancer/No cancer. Vote yes/Yote no. This, of course, is just a partial list since binary outcomes are of concern in many different disciplines including human resources, marketing, medicine, education, political science, criminology, and others. Logistic regression is one technique that is frequently used to create predictive models where the target is binary. (There are other techniques which will be discussed in subsequent chapters.) To build such a model, a data set is obtained with the following structure shown in Figure 8.1. Figure 8.1: Typical data structure. The blue squares in Figure 8.1 represent a zero and the maroon squares represent a one. The cases may be individuals, companies, or other types of entities. The predictors can be continuous variables (e.g., age) or categorical (e.g., sex). At first it might seem that a data set with this structure could be analyzed with ordinary least squares regression. There are several reasons why linear is not appropriate, including the fact that the estimates of the target are not constrained to be between 0 and 1. Furthermore, logistic regression does not require the assumption of a normally distributed error term nor the assumption that the error variance is constant. (In fact, the error variance is a function of the predictors.) One way of understanding the logistic regression model is to think of an intermediate variable (which is not explicitly observed) and the predictors. While the target variable yi (where the subscript represents the ith case in the data set) is binary, the intermediate variable pi is continuous and can be thought of as a propensity for the target to be 1.0 versus 0. Therefore, pi is continuous in the range of zero to one. Once pi is modeled, then predictions of the target can be obtained by using a decision rule based on a threshold value: if pi is greater than the threshold, then yi is predicted to be 1; otherwise yi is predicted to be 0. An equation that is a weighted linear combination of the predictors is created. The linear combination can produce a continuous outcome with estimates ranging from -infinity to +infinity. A logit transformation of pi to \\(\\hat{y_i}\\) is created that results in a continuous variable as a linear function of the predictors: \\[\\begin{equation} \\ \\mathit{If}\\;p_{i}\\; &gt;\\; threshold,\\; \\hat{y}_{i} = 1;\\; else\\; \\hat{y}_{i} = 0, \\ \\mathit{where}\\;p_{i}\\; = \\frac{1}{1 + e^{-(log(odds\\;ratio)}} \\end{equation}\\] The log odds is a transformation of the probabilities of 1 and 0. In logistic regression the model is estimated as a linear function of log [p(1)/p(0)] where p( ) indicates probability. The p(1) and p(0) are unobserved; the coefficients of the logistic regression model are estimated using a technique that works to create p(1) and p(0) that maximizes conformance with the observed 1s and 0s for the observations in the data set. Unlike ordinary least-squares regression, there is no closed-form algebraic solution to this estimation problem, so an iterative maximum likelihood algorithm can be used. The effect of the logistic transformation is illustrated in Figure 8.2. While the variable on the x-axis is continuous, the y-axis variable is constrained to be between zero and one due to the logistic transformation. Figure 8.2: The logistic function. Note that logistic regression is not the only technique that can be used in these cases. There is a very closely related technique called probit analysis which works in a very similar manner. Instead of using a logistic function for the S-shaped curve, probit analysis uses a cumulative normal function. Investigations of the two techniques have shown very similar results. Also, the hyperbolic tangent function can be used which has properties similar to the logistic function except that it ranges between -1 and +1 instead of 0 and +1. Of course, techniques such as neural networks, decision trees, or discriminant analysis can also be used for with binary targets. 8.1 Example with a single predictor Consider a very simple classification problem using synthetic data. The example involves predicting purchase (Yes or No) by a customer as a function of customers age. Twenty observations with nine Yes values and 11 No values were created for demonstration12. A listing of the first 10 rows of the data set is shown in Table 8.1. Table 8.1: First 10 rows of simulated data Purchase Age No 53 No 51 Yes 43 Yes 36 Yes 33 Yes 30 Yes 41 No 54 Yes 39 Yes 23 A logistic regression was run with Purchase as the target and Age as the predictor variable. The equation estimated using logistic regression on the data from Table 8.1 is: \\[\\begin{equation} log \\left[ \\frac{\\;p_{i}\\;}{1-\\;p_{i}\\;} \\right] = 11.75 - 0.286 \\cdot \\;Age_{i}\\; \\tag{8.1} \\end{equation}\\] Interpreting the coefficients produced in the logistic regression model is not straightforward due to the non-linear relationship between predictors and probabilities. For this example, it can be noted that since the coefficient on Age is negative, increases in Age result in decreased likelihood of purchasing the product. The value of this coefficient (-0.286) means that a one unit increase in Age is associated with a -0.286 decrease in the log odds of purchase. The intercept (11.75) is usually not of much interest in such problems. The intercept is there to adjust the model to fit the overall proportion of yeses and noes. Using equation (8.2), a graph of probability versus age (Figure 8.3) was created, showing a decrease in probability as age increases. \\[\\begin{equation} \\;p_{i}\\; = \\frac{1}{1 + e^{11.75 - 0.286 \\cdot \\;Age_{i}\\;}} \\tag{8.2} \\end{equation}\\] Figure 8.3: Probability of purchase versus age. Equation [3] can also be used to create predictions by comparing each probability to the threshold value using the following decision rules: If probability of purchase &gt; .5, then Most likely purchase=Yes, else Most likely purchase=No. Table 8.2 shows the results of from applying the decision rule. Table 8.2: Predictions using the logistic model. Purchase Age Logit Probability.of.purchase Most.Likely.Purchase No 53 -3.415 0.032 No No 51 -2.843 0.055 No Yes 43 -0.555 0.366 No* Yes 36 1.448 0.811 Yes Yes 33 2.306 0.910 Yes Yes 30 3.164 0.960 Yes Yes 41 0.018 0.506 Yes No 54 -3.702 0.024 No Yes 39 0.590 0.645 Yes Yes 23 5.167 0.994 Yes No 40 0.304 0.577 Yes* No 48 -1.985 0.122 No No 58 -4.846 0.008 No No 52 -3.129 0.042 No No 46 -1.413 0.197 No No 34 2.020 0.883 Yes* No 47 -1.699 0.156 No No 44 -0.841 0.303 No Yes 42 -0.269 0.435 No* Yes 25 4.595 0.990 Yes Note: * Indicates prediction error. The results showing the performance of predictive classification models are typically displayed as a classification matrix (also known as a confusion matrix) which is shown in Table 8.3. Table 8.3: Confusion matrix Predicted Actual No Yes Totals No 9 2 11 Yes 2 7 9 Totals 11 9 20 8.2 Example: Predictive analytic in HR Employee retention is an important objective for HR departments in many organizations. According to a report from the Work Institute (2020 Retention Report 2020), more than 27 percent of U.S. employees voluntarily left their jobs in 2020 with a total estimate cost of $630 billion due to factors such as cost of replacement, loss of productivity. This same study reported that more than three-fourths of the turnover was preventable. Using predictive analytics can enable employers to identify employees at risk of leaving and to take preemptive corrective action. A data set on employee turnover was obtained from Kaggle (HrAnalytics-Employee-Turnover, n.d.) . This data set has 14,999 rows and 9 columns: Satisfaction: Level scored 0 to 1. Evaluation: Last evaluation rating scored 0 to 1. Projects: Number of projects completed while at work. Hours: Average monthly hours at workplace. Years: Number of years spent in the company. Promotion: Whether the employee was promoted in the last five years. Department: Department in which the employee worked. Salary: Relative level of salary: low, medium, high. Left: Whether the employee left the workplace or not. A KNIME workflow, shown in Figure 8.4, was created to analyze the HR data. Figure 8.4: KNIME workflow for logistic regression on employee turnover data. A description of each node is shown Table 8.4. Table 8.4: Description of workflow nodes for employee turnover logistic model. Node Label Description 1 File Reader Read Employee_turnover.csv 2 Data Explorer Create summary statistics and histograms for each variable. All variables with the exception of YearsAtCompany are okay for analysis. YearsAtCompany is highly skewed, so logarithmic transformation is indicated. 3 Math Formula Compute ln(YearsAtCompany) 4 Normalizer Rescale continuous variables to range of 0 to 1. 5 Partitioning A 70/30 (Training/Test) random split of the data set stratified on Left is formed. Set random seed = 123. The Test portion is set aside for validation. 6 SMOTE SMOTE (Synthetic Minority Over-sampling Technique) oversamples the class where employees Left in order to create an equal number of those who Left and those who Did not leave. By balancing the target variable a better logistic model is formed. 7 X-Partitioner The beginning loop of a 10-fold cross validation is created. 8 Logistic Regression Learner Run logistic regression with Left as the target. 9 Logistic Regression Predictor Predict the response from the logistic regression model using the Training data. 10 X-Aggregator The end of the cross validation loop. 11 ROC Curve Create ROC curve for the Training data. 12 Scorer Compute performance statistics and classification (confusion) matrix for the Training data. 13 Logistic Regression Predictor Predict the response from the logistic regression model using the Test data. 14 ROC Curve Create ROC curve for the Test data. 15 Scorer Compute performance statistics and classification (confusion) matrix for the Test data. In Node 1 the data set with 14,999 observations and nine variables was read using the File Reader node. Promotion, Salary and Left are string variables; the others are numeric. The data set was explored using Data Explorer. All of the variables appeared to be suitable for analysis with the exception of YearsAtCompany which was highly skewed as shown in the histogram below in Figure 8.5. Figure 8.5: Histogram of YearsAtCompany. Since a skewed variable can lead to poorer performance with logistic regression, a transformation of YearsAtCompany was computed using a Math Formula (Node 3) and the original variable replaced in the subsequent nodes. This reduced the skewness of YearsAtCompany from 1.85 to 0.59. Predictive models typically do better with normalized predictors, so the following variables were normalized (Node 4) to a range of 0 to 1: Satisfaction, Evaluation, Projects, Hours, and Years. In Node 5 the data was split randomly into two subsets: Training and Test. The split ratio was 70/30 and the sample was stratified on Left so that the proportions of Left / Did not leave would be about the same in both the Training and Test partitions. SMOTE (Node 6) was used to oversample the Left category. Prior to oversampling, the split in the Training partition was 7,999 Left and 2,499 Did not leave. Such an imbalance in a binary target variable will usually cause the logistic regression to make most predictions to the level with the larger number of cases. After applying SMOTE, both levels (Left and Did not leaver) had 7,999 cases. This balanced sample was used to build the logistic model, but the evaluation of the model performance was done using the Test set with the original level proportions. The balanced sample was used to facilitate the estimation of the logistic regression model (ChristianSalas-Eljatiba and ValeskaYaitul 2018). A k-fold cross validation (Node 7) was used to assess the stability of the logistic model using the Training data by employing the X-Partitioner and X-Aggregator nodes in KNIME with k = 10. The Training data is divided randomly into 10 approximately equal segments. Then, the logistic regression was run 10 times, each time withholding a 10% subset. Logistic regression requires two steps in KNIME; in Node 8 the Logistic Regression Learner runs the model and outputs the model coefficients; in Node 9 the Logistic Regression Predictor is applied to compute predictions for each row of the Training data. The 10 held out samples were used to assess the predictive accuracy. The results of the 10 runs were obtained from the output of Node 10 and are shown in Table 8.5. Table 8.5: Results of k-fold validation. Fold Error in % Accuracy in % Size of test set Error count fold 0 18.6 81.4 1600 354 fold 1 19.0 81.0 1600 323 fold 2 19.7 80.3 1600 358 fold 3 18.4 81.6 1600 379 fold 4 19.9 80.1 1600 341 fold 5 20.7 79.3 1600 323 fold 6 19.0 81.0 1600 354 fold 7 18.7 81.3 1600 349 fold 8 20.3 79.7 1599 331 fold 9 21.1 78.9 1599 335 The average accuracy was 80.57%, with a maximum of 81.63% and a minimum of 78.86%. This shows that the model had reasonable accuracy and was quite stable in performance across the 10 folds. Recall that these analyses were conducted using the oversampled data. The final model from Node 10 (using the last of the 10-fold partitions) was passed to the evaluation nodes for the Training data. The predictions for the Training data computed in Node 10. An ROC curve was created in Node 11 and the Scorer Node 12 created a classification (or confusion) matrix along with several evaluation metrics. A corresponding set of analyses was run for the Test data, shown in Nodes 13, 14, and 15. The classification matrices for the Training and Test data are shown in the Tables 8.6 and 8.7. Table 8.6: Confusion matrix for training data. Predicted Actual Left company Did not leave Total Left company 6,909 1,090 7,999 Did not leave 2,035 5,964 7,989 Total 8,944 7,054 15,900 Table 8.7: Confusion matrix for test data. Predicted Actual Left company Did not leave Total Left company 919 152 1,071 Did not leave 879 2,559 3,429 Total 1,789 2,711 4,500 Note that the results for the Training data are based on the oversampled data while the Test data results used the original ratio of Left versus Did not leave. The imbalanced data created less than ideal results. The Scorer Nodes (13 and 15) also provide summary descriptive statistics derived from the classification matrices. The results for both partitions are shown in Table 8.8. Table 8.8: Performance metrics for training and test data. Metric Training data Test data Accuracy 0.805 0.773 Cohens kappa 0.609 0.491 Precision 0.772 0.514 Sensitivity 0.864 0.858 Specificity 0.746 0.749 F-measure 0.834 0.643 The overall accuracies for the Training and Test data sets were close, with a slight edge for the Training data. This is to be expected since the logistic regression was optimized for the Training data. However, accuracy is not the best measure in this situation due to the imbalance in the binary outcomes. In fact, if all observations in the Test data a predicted Did not leave, the accuracy would be .762. This approach would not be useful, of course, since it never identifies any of those that left the company. Cohens kappa was lower in the Test data than with the Training data. This is likely due to the imbalance in binary outcomes in the original data. It is known that Kappa reaches its maximum of 1.0 only for balanced outcomes, so the lower kappa in the Test data is not surprising (Widmann, n.d.). An area of concern with the results for the Test data is precision, which is much lower for the Test data than for the Training data. This too is due to the imbalance in the number of cases which Left versus the number that Did not leave. Because of this imbalance, it is easier to predict Did not leave. In the context of this HR application, it could be argued that mistakenly predicting an employee will leave is not as serious as failing to predict that an employee would leave. The sensitivity for predicting who is likely to lead was more than 0.8 with the training data. Thus, the vast majority of those likely to turnover can be identified. In such a situation the results of the model can only be used to signal HR that a further investigation of those identified as likely to leave. ROC curves were obtained for the Training data in Node 12 and the testing data in Node 15. The curves are shown in Figures 8.6 and 8.7. Figure 8.6: ROC curve for the training data. (AUC = 0.829) Figure 8.7: ROC curve for the training data. (AUC = 0.833) The ROC curves for both data sets are quite similar, indicating comparable predictive performance with the Training and Test data. The areas under the ROC curves (AUC) for both are about 0.83, which is fairly high, as the AUC varies from 0.0 to 1.0. The overall assessment of the logistic model with this data is that it would be useful in identifying employees likely to turnover when the predictor variables are input to the model. As noted earlier, however, when an employee is predicted to turnover, additional human review is needed since the precision of the model was not high. 8.3 Predictor interpretation and importance Some questions about this predictive model are: How can the changes in the predictor variables be interpreted? What is the relative importance of each of the predictors of employee turnover? Answering this might provide insight into what steps might be taken to reduce turnover. The question of predictor (or feature) importance has been studied extensively with multiple regression models. However, comparatively little has been published about measuring predictive importance in logistic regression (Azen and Traxel 2009). An approach based on dominance analysis has been developed and is available in Python and R. Dominance analysis is not directly available in KNIME. A simpler approach will be discussed in this section (but one that has limitations like any of the other methods). The coefficients on the predictors cannot be used to infer importance in logistic regression because these coefficients are for the log of the odds. Rather, what is usually desired is the impact on the probability of changes in each predictor. This is further complicated because the effect on probability of the outcome is not linear, and the probabilities associated with a predictor depend upon the values for the other predictors. Thus, the importance of a particular predictor varies based on the range of the predictor being considered and the settings of every other predictor. So, the relative sizes of the coefficients in a logistic model cannot indicate predictor importance, even if the predictor variables are normalized. Likewise, the p-values for significance tests on each predictor cannot be interpreted as a measure of importance in a practical sense. A small p-value indicates that the variable has a low variance compared with its magnitude, but the variable could still have a very minor effect on the target variable. In general, for binary logistic models no approach to interpretation can fully describe the relationship between changes in a predictor value and the probability of the target variable (Long and Frees 2006). An approach is to examine the effect on the probability of the outcome as each predictor is varied from its minimum to maximum. Since the results of changing one input variable depend upon the values of the other predictors, a baseline set of values for the predictors was established. Each of the continuous predictors was set to their respective means and the nominal values were set to the modal values. The results (Table 8.9) indicate that Number of years spend in the company was most important followed by Satisfaction. Note that this still does not fully address the question of interpretation, but it may provide some insight. Table 8.9: Range of probabilities of leaving by predictors. Predictor Range Last evaluation rating scored 0.12 Department in which the employee worked 0.23 Average monthly hours at workplace 0.33 Whether the employee promoted in last five years 0.39 Relative level of salary: low 0.47 Number of projects completed while at work 0.51 Satisfaction 0.75 Number of years spent in the company 0.77 Another approach to interpreting predictors is to chart changes in probability by a predictor at different levels of another predictor. Figure 8.8 shows how the probability of leaving the company varies with level of satisfaction at three salary levels. As might be expected, employees with Low salary were generally more likely to leave at any level of satisfaction. This type of analysis is useful when examining the impact of two variables but still does not fully address the question of variable importance. Figure 8.8: Probability of leaving by satisfaction for different salary levels. 8.4 Regularized logistic regression Next, we will look at a subset of the data used in a Kaggle competition developed by the University of Melbourne which asked participants to predict the success of research grant applications. The subset used is based on the involved data preparation process available in the R package Applied Predictive Modeling . A subset of the data in that package was created by removing observations with missing values, yielding a total of 5,503 observations on 258 columns, including the binary target column Class (successful versus unsuccessful). As discussed in the chapter on regression, KNIME includes an integration of algorithms from the H2O suite of analytics programs. The H2O algorithm Generalized Linear Model (GLM) was used to analyze the grant application data. The GLM program was run with the following settings: Target Column =&gt; Class Predictors =&gt; NumCI through Day (257 columns) The Ignore constant columns was checked The random seed was set to 123. The algorithm family was set to Binomial, the Link to logit. The alpha parameter controls the penalty functions for LASSO and ridge regression. An alpha of 1.0 produces LASSO and 0.0 produces ridge regression. This was set to 1.0 for LASSO. The lambda parameter controls the amount of regularization from 0 (no regularization) and to larger values for more regularization. The option to perform an automatic search for the optimal value of the lambda parameter was selected. GLM will first fit a model with maximum regularization using a high lambda value that causes all coefficients to be zero, and then sequentially reduces lambda until the minimum lambda (set to .0001) or until overfitting occurs. The best value for lambda is determined by using the validation subset of the data, which was set to 15%. The best lambda is that which maximizes the log-likelihood in the GLM model). A workflow for the regularized logistic regression was created using KNIME and H2O (Figure 8.9) with corresponding node descriptions in Table 8.10. Note that the optimum value of lambda is found independently of the test data. Figure 8.9: Probability of leaving by satisfaction for different salary levels. Table 8.10: Node descriptions for regularized logistic regression. Node Label Description 1 File Reader Read grantdata1.csv. 2 H2O Local Connect Allows running H2O models in KNIME workflow. 3 Table to H2O Convert KNIME data to H2O frame. 4 H2O Partitioning An 80/20 (Training/Test) random split of the data set stratified on Class, random seed = 123. The Test portion is set aside for validation. 5 H2O Generalized Linear Model Learner GLM set up to run logistic regression with regulation. 6 H2O Predictor (Classification) Create predictions and probabilities. 7 H2O to Table Convert H2O frame to KNIME data. Regularization was able to simplify the model considerably (with 74 fewer predictors) and actually improved the area under the ROC curve, accuracy, and sensitivity (Table 8.11). Specificity was slightly lower with the regularized model. Table 8.11: Comparison of full and regularized logistic regression models. Model and number of predictors Metric Full (257) Regularized (184) AUC 0.897 0.903 Accuracy 0.823 0.828 Sensitivity 0.814 0.833 Specificity 0.831 0.824 8.5 Probability calibration In some applications, it is important to predict the probability that an observation belongs to a specific classification outcome. Performance metrics such as accuracy, sensitivity, and specificity focus on the overall classification to one of two outcomes. Typical models are also used to provide a rank ordering of cases from highest to lowest probability. Such metrics may not correspond to the actual frequencies of outcomes. In other words, the probabilities are not calibrated to the true probabilities. Some algorithms, such as support vector machines, boosted trees, and nave Bayes may be accurate in terms of classification, but not in terms of matching probabilities. Logistic regression has been shown to produce well-calibrated probabilities. In this section we will assess the calibration of logistic regression and present two calibration methods, Platt Scaling and Isotonic Regression. Assessing calibration accuracy with binary outcomes can be done via a calibration plot of the observed fraction of positive outcomes versus the mean probability obtained from a model. A calibration plot for the success of grant applications example from the previous section was computed. Nodes were added to the workflow shown in Figure 8.9). The output of the H2O to Table node from that workflow was submitted to the following series of nodes (Figure 8.10 and Table 8.12). Figure 8.10: Workflow for creating calibration plot. Table 8.12: Node descriptions for calibration workflow. Node Label Description 10 Column Filter Select columns needed for calibration: ROWNUM, actual class, predicted probability of success. 11 Numeric Binner Create bins of probabilities from 0 to 1 in increments of .1. 12 Pivoting Create pivot table of bins, count of successful and unsuccessful 13 Column Expressions Calculate fraction of success 14 GroupBy Calculate mean probability by bin 15 Joiner Join fraction of success and mean probability 16 Excel Writer Output results to Excel for charting The logistic regression was fairly well calibrated, with some deviation in the mid-range of probabilities (Figure 8.11). Figure 8.11: Calibration of logistic regression probabilities. 8.6 Evaluation of logistic regression Logistic regression is one of the most used algorithms for predicting binary classes. It is easy to run and can handle large numbers of observations efficiently. As with ordinary regression, the number of observations in an analysis should be much greater than the number of predictors. Also, care should be taken to avoid overfitting (as is true of most supervised models). While logistic regression can be used to predict categorical targets with three or more levels, other models such as decision trees, neural nets, k nearest neighbors are better choices for such problems. In some cases, multi-class problems can be converted to just two levels so logistic regression can be meaningfully applied. The H2O GLM regularization model available in KNIME was demonstrated with a fairly large data set. Regularization reduced the number of predictors significantly while maintaining overall model performance. The estimated probabilities from the model were generally aligned with the observed proportions. Logistic regression assumes linearity between the log-odds ratio and the predictor variables. Interactions can be created, but this can cause computational problems. As discussed earlier, interpreting the coefficients on predictors is not straightforward since each coefficient reflects the change in log odds which is difficult to intuit. Finally, with some data samples logistic regression can result in failure to converge to a solution. One other issue is the potential for complete separation of the data by a single feature. In this situation, no weight can be estimated for the feature in question; it is essentially infinite. References "],["ensemble-models.html", "Chapter 9 Ensemble models 9.1 Creating ensemble models 9.2 Parallel and sequential learners 9.3 Example of ensemble modeling for a continuous target", " Chapter 9 Ensemble models Many different machine learning models have been developed and new variants continue to be explored. This leads to questions such as, Which algorithm is best? and Which algorithm is best for a particular problem context or data set? As might be expected, there is no simple, definitive answer to these questions. An interesting comparison the performance in terms of error of five algorithms is shown in Figure 9.1 (Seni and Elder 2010). None of the algorithms performed best on all the data sets. Figure 9.1: Relative Performance of 5 Algorithms on 6 Datasets Observations of model performance such as shown in Figure 9.1 led to the idea of combining the predictions from two or more models, a process called ensemble learning. Ensemble models can lead to lower prediction errors when the predictions of diverse individual models are independent and aggregated to make a final estimate. Everyday examples of the effectiveness of combining many independent estimates was the theme of the book The Wisdom of Crowds (Surowiecki 2004). A famous example of the power of combining estimates is given below. In 1907 Sir Francis Galton published an article in Nature about combining individual estimates. He attended the West of England Fat Stock and Poultry Exhibition where he was intrigued by a weight guessing contest. The goal was to guess the weight of an ox when it was butchered and dressed. Around 800 people entered the contest and wrote their guesses on tickets. The person who guessed closest to the butchered weight of the ox won a prize. After the contest Galton borrowed the tickets and found the median value of the guesses. He discovered that the median guess across all entrants was 1,207 pounds, only 9 pounds from the actual weight of 1,198 pounds. (The mean value of the guesses was exactly 1,198) 9.1 Creating ensemble models There are several ways to generate several different base learners that can be combined to create an ensemble model. Different algorithms can be used, such as decision trees, logistic regression and neural networks. Changing the parameters of a single model, such as the number of branches in a decision tree or the number of nodes in neural networks. Using different subsets of predictor variables. Using different subsets of observations. While different algorithms can be used as a basis for building ensemble models, in this chapter the focus is on decision tree, both for classifications and regression. Ensemble models use weaknesses of decision trees to create an advantage. Tree models have been around for a long time, but until the development of ensemble models, their use was limited for two reasons. First, decision trees are unstable to different samples. That is, considerably different decision trees can result if the data set is changed slightly. This, in turn, is due to two characteristics of tree models. (a) The models produced are not globally optimum. Once a split is made, no further consideration is given to altering that split based on splits being made further down the tree. (b) If two potential predictor variables, A and B, are highly correlated, then if variable A is selected for a split, this effectively makes variable B essentially unimportant. That is to be expected. However, if a slightly different sample is selected, for example by randomly splitting a data set into training and test subsets, variable B might be selected first, making variable A apparently less important. Thus, changing the seed for the random split could result in quite different results. There are two main benefits of ensemble models (Brownlee 2020): Reduced variation (reliability) in predictions with different data samples. Improved prediction accuracy. 9.1.1 Reduced variation Performance evaluation of models is typically based on the accuracy of the predictions. This might be measured by the mean square error in the case of regression trees or errors made in classification trees. While these assessments are important, it is also important to consider the variation of performance as different data samples are analyzed. One way to assess this variation is to use a k-fold partitioning of a data set. If k samples are taken and submitted to the tree model, the performance will typically vary. The range or standard deviation of this variation will provide an indication of the likely performance on unseen data. It is possible that an ensemble model might not improve the average performance across k samples compared with a single tree model and yet have a smaller range of performance values. 9.1.2 Improved performance Ensemble models can perform better on average than a single tree model because errors are averaged out. By creating multiple tree models, problems with local optima can be avoided. Since different samples from a data set are likely to lead to different predictors and variation in performance, the risk of relying on a single model can be avoided. 9.2 Parallel and sequential learners Tree ensemble models require two steps: Creating a set of distinct predictive tree models. Combining the predictions from the set of models to produce an overall prediction. Ensemble models can be classified into two types based on how the set of predictive models are created: parallel learners and sequential learners. Parallel learners generate a set of strong (or complex) models and the independence of the models is used to average out errors. Examples include Bagging and Random Forests. With sequential learners, consecutive weak models are created, and errors captured at each stage of the process. The errors at each stage become the target variables for the next stage. Examples of sequential ensemble models include AdaBoost, Gradient Tree Boosting, and XGBoost. 9.2.1 Bagging (Bootstrap Aggregating) Bagging multiple data sets by bootstrap sampling and then aggregating the results to create predictions is an early parallel ensemble approach (L. Breiman 1966). By bootstrap sampling of n observations, k repeated samples of size n are selected with replacement. Figure 9.2 shows a diagram of the bagging approach. Repeated sampling means that some observations are left out of a given sample and some observations are included more than once. For regression trees, the average of the k predictions across becomes the ensemble prediction. For classification trees, a majority vote is taken to obtain the predicted class. Figure 9.2: Creating a bagged ensemble 9.2.2 Random Forests A random forest, another example of a parallel ensemble model, introduces another element of sampling to increase the diversity of models created. Instead of using all available predictors to create each tree, a sample of predictor variables is taken. So, two forms of randomization are involved: sampling observations with replacement and sampling of features. The diagram for random forests would appear similar to that for bagging shown in Figure 9.2, with the exception that each tree would be created with a randomized subset of the predictors. 9.2.3 AdaBoost Boosting is the process of building a large, iterative decision tree by fitting a sequence of smaller decision trees, called layers. The tree at each layer consists of a small number of splits. In fact, the trees can be tiny  only a few splits. The process for boosting is shown in Figure 9.3. For regression trees, the process begins by creating a weak tree with just a few splits. This is called the base learner. Predictions are made from the initial split and the residual error is computed. The residuals have the information that the model could not explain. This residual error then becomes the target for a second weak tree. The idea is that the successive trees will tend to correct the errors, resulting in a more accurate model. As trees are added, the existing trees are not changed or revisited, making this another example of a greedy algorithm. Figure 9.3: Creating a boosted tree The final prediction is an aggregation of the results. For regression trees, a one approach for creating an ensemble prediction is to simply average the predictions from the multiple levels of the small intermediate trees. For classification trees, a majority vote is taken to produce a prediction. An early boosting algorithm, called AdaBoost, was developed by Freund and Schapire (Freund and Schapire 1996) and used for categorical target variables (although subsequent extensions to continuous targets were implemented). Two aspects of the AdaBoost model make this a powerful method. First, at each stage, a weak model is created and the errors in prediction are captured. The errors from a stage become the target for the next stage. Thus, as the model is faced with increasingly difficult predictions, the prediction accuracy improves. The second important aspect of AdaBoost is that at each stage, the errors in prediction from observations in the previous stage are given more weight compared with the observations that are accurate. In the initial stage, all weights are equal. At the end of the process, AdaBoost creates a weighted ensemble of the predictions made at each, with weights proportional to the accuracy of each stage. 9.2.4 Gradient Boosting Machines Gradient Boosting Machines (GBM) (Friedman 2001) build on the approach of the AdaBoost algorithm by incorporating gradient descent for optimization. These models have been popular and successfully applied in several contexts. GBM models are quite flexible and have been implemented for time-to-event, Poisson regression, and multinomial classification problems. In this section the focus will be on predicting continuous and binary target variable. To implement the model, weights must be calculated for each predictor variable that will minimize prediction error, called a loss function in the context of the GBM models. For continuous responses, a typical loss function is based on the square of actual minus predicted values. For binary responses the loss function is the negative of the log-likelihood (Natekin and Knoll 2013). A brute force approach would be to form a grid of all possible values (with some degree of increments) of each predictor. This unguided search process would be grossly inefficient to use in building a tree model. Instead, a calculus-based approach of finding a local minimum which is based on the error between the actual and predicted values of the target variable. The gradient (sort of multivariable slopes or derivatives) directs changes in the model parameters with the steepest slopes toward minimization. The minimization process proceeds iteratively in small steps. (The step size can be set by the user. Large steps will reduce computer processing time but may miss a local minimum. Small steps are more likely to find a local minimum at the cost of increased time.) The number of iterations required depends on the particular problem and the step size and can range from a few to thousands or more. This method does not guarantee a global minimum, so algorithms typically incorporate multiple runs through the data with different starting values. Friedman, the creator of Gradient Boosting Machines, updated his model calling it Stochastic Gradient Boosting (Friedman 2002). This model inserted a step prior to the construction of each tree in the sequence which took a sample of the data. The tree was then formed on this sample and the loss function calculated. At the next iteration, another random sample of the full data set is selected, and so on. This reportedly increased the accuracy of the predictions as compared with GBM. 9.2.5 XGBoost Tianqi Chen and Carlos Guestrin (Chen and Guestrin 1996) developed XGBoost (which stands for eXtreme Gradient Boosting) that has become one of the most popular ensemble modeling techniques in recent years, primarily due to its use in Kaggle competitions (Adebayo 2020) and is considered state of the art by many practitioners (Kunapuli 2021). XGBoost is very flexible and customizable, it is open source and runs on many operating systems, provides options such as regularization to reduce overfitting, and has many parameters to tune the algorithm to specific applications. Furthermore, it executes faster than most comparable models. 9.3 Example of ensemble modeling for a continuous target The file ToyotaCorolla.csv contains 1,436 records on used cars on sale during the summer of 2004 in The Netherlands. In addition to price of each automobile, there are details on more than 30 attributes, such as weight, age, number of kilometers, horsepower, and optional accessories such as power steering, central locking, and tow bars. A KNIME workflow (Figure 9.4) was used to create and compare ordinary least regression (OLS) and Gradient Boosted Trees to predict the prices of the Toyota Corollas. Figure 9.4: Workflow for comparison of OLS with Gradient Boosted Trees The nodes in the workflow are described in Table 9.1, Table 9.1: Descriptions of nodes in the OLS vs. GBT workflow Node Label Description 1 File Reader Read the file ToyotaCorolla.csv 2 Data preparation Metanode Column Filter: Remove ID and Model; Rule Engine: Collapse infrequent colors to Other; One to Many: Create dummy variables; Column Filter: Drop one level of each dummy set. 3 Partitioning Create 70/30 split into training and test partitions. 4 Linear Regression Learner Run OLS with Price as target. 5 Regression Predictor Compute predicted Price using OLS model. 6 Numeric Scorer Calculate performance metrics for OLS model. 7 Gradient Boosted Trees Learner Run GBT with Price as target. 8 Gradient Boosted Trees Predictor Compute predicted Price using GBT model. 9 Numeric Scorer Calculate performance metrics for GBT model. The results of the comparative analysis are shown below in Table 9.2 Table 9.2: Comparative performance of OLS and GBT Metric OLS GBT R2 0.896 0.924 RMSE 1244.000 1064.400 The results show that the Gradient Boosted Tree performed slightly better than ordinary regression, especially in terms of the Root Mean Square Error (RMSE). A separate comparison was run (the workflow is not shown) in which a 10-fold cross-validation for both OLS and GBT was performed. This was done to compare the stability of both models in terms of producing comparable accuracy. One of the advantages of ensemble models is reduced variation with different samples of a data set. This is important since it implies that ensemble models would perform as expected with new data sets. The results of the stability assessment are shown in Table 9.3. Table 9.3: Comparison of OLS and GBT stability Metric OLS GBT Mean R2 0.898 0.909 Stdev R2 0.032 0.013 Range R2 0.119 0.042 Mean RMSE 1118.837 1074.502 Stdev RMSE 116.257 92.774 Range RMSE 349.285 251.704 The results in the table show that the stability of the ensemble model was better than the OLS model over 10 replications both in terms of R^2 and RMSE. The mean R^2 was only slightly better for GBT and the mean RMSE was only about 4% better with GBT. In summary, for the Toyota Corolla data set, the overall performance in terms of accuracy was nearly the same for the single OLS model versus the GBT ensembles, but GBT exhibited greater consistency of performance. This, of course, will not necessarily be the case for different data sets and different ensemble models, so tests are needed in each actual situation. References "],["naive-bayes.html", "Chapter 10 Naive Bayes 10.1 A thought problem 10.2 Bayes Theorem applied to predictive analytics 10.3 Illustration of Nave Bayes with a toy data set 10.4 The assumption of conditional independence 10.5 Nave Bayes with continuous predictors 10.6 Laplace Smoothing 10.7 Example using nave Bayes with churn data 10.8 Spam detection using nave Bayes", " Chapter 10 Naive Bayes 10.1 A thought problem A police officer has a breathalyzer which indicates false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. Suppose on a given evening 1 in 1,000 drivers are driving with alcohol over the legal limit. A traffic checkpoint stop is set up, drivers are selected at random, and the selected drivers are required to take a breathalyzer test. Over the limit? Assume that a particular driver is found to be over the legal limit for alcohol for according to the breathalyzer. Assume nothing else about the driver. What is the probability that driver really is over the limit? Many people have answered that the probability is as high as 0.95, but the correct probability is about 0.02. How can the proper probability that the person is really drunk be estimated? This calls for Bayes Theorem. Bayes theorem is a formula that describes how to update the prior probability of an event when additional evidence is made available. Prior probabilities from a Bayesian perspective are based on the known likelihoods from historical data. In the example of random checks of drivers, the prior is 1/1000 or .001 that the driver is drunk. To estimate the probability of identifying a drunk driver, the results of the breathalyzer can be used to update the probability estimate. The revised probability is called the posterior probability. To determine the posterior probability, Bayes theorem can be used. The goal is to find the probability that the driver is drunk given that the breathalyzer indicated he/she is drunk, which can be represented as: p(drunk|POS), where POS means that the breathalyzer indicates that the driver is drunk_ Bayes Theorem tells us that: p(drunk|POS) = [p(POS|drunk) X p(drunk)] / p(POS) where p(POS) = p(POS|drunk) X p(drunk) + p(POS|Sober) X p(Sober) We have the following data: p(drunk) = 0.001 p(Sober) = 0.999 p(POS|drunk) = 1.00 (the breathalyzer is 100% accurate if the person is actually drunk) p(POS|sober) = 0.05 (the breathalyzer mistakenly reports a sober driver is drunk 5% of the time) Given the data and a positive indication on the breathalyzer test for a randomly selected driver, what is the probability that the person is drunk? The numerator of Bayes formula = [p(POS|drunk) X p(drunk)] = [1.00 X 0.001] =0.001 The denominator of Bayes formula = p(POS|drunk) X p(drunk) + p(POS|Sober) X p(Sober) = 1.0 X 0.001 + 0.05 X 0.999 = 0.001 + .04995 = 0.05095 Substituting the numerator and denominator into Bayes theorem yields: p(drunk|POS) = 0.001 / 0.05095 = .0196 The framework of Bayes theorem can be applied to a supervised analytics problem. 10.2 Bayes Theorem applied to predictive analytics Consider a case of a cable service provider that has over two million subscribers. The company decides to perform a test market to predict whether current customers will subscribe to a new service. Reverend Bayes. The test involved sending an offer to a random sample of 1,000 current customers. This can be cast as a Bayesian model. Using the results of the test, the company would like a predictive model to use for the rest of its customers. The test results were that 400 customers bought the new service, so the prior probability of purchase was 0.40. This is illustrated in Figure 10.1 which has a grid representing the 1,000 customers in the test. Figure 10.1: Overall results of the test market. If this prior probability is applied to the entire subscriber base, then the company would expect to have 400,000 positive responses. The process of contacting customers via mail, email, and telephone to offer the new service had a cost, so the company wanted to know if there was a way to make the contacting process more efficient. That is, was there a way of increasing the positive rate?. It turned out that the company had data on the gender and age (young, or old) of its subscribers and thus this information was available on those in the test market. Using gender, the probability of purchase can be refined. It turns out that there were 600 female customers in the test, 300 of whom subscribed to the new service and 400 males, 100 of whom subscribed. The probability was further refining using age, with results shown in Figure 10.2. Figure 10.2: Conditional results of the test market. By simply counting the number of customers in each shaded area, the posterior probabilities of each segment could be calculated. prob (Subscribing | male, young) = 50/ (50+180) = .217 prob (Subscribing | male, old) = 50/ (50+120) = .294 prob (Subscribing |female, young) = 180/ (180 + 150) = .545 prob (Subscribing | female, old) = 120/ (120+150) = .444 So, this small example shows that the Bayes model can be used for predicting the classification of new observations. To classify a new case, find all of the observations in the sample with exactly the same descriptive characteristics. With this set of observations, count the number of positive and negative outcomes and apply the counting scheme discussed above. This approach does not work if there are many predictors. Many practical predictive modeling problems have many predictors. So, the Bayesian idea works in theory, but not always in practice. As a more practical example, assume you want to predict a binary target class with true or false as the outcomes using 15 binary predictors. Assume that you it needed at least 50 observations in each one of the resulting cells to make a reasonable estimate of the true versus false values of the binary target. The very minimum number of observations you would need is 50 x 215 = 1.638.400 observations. Even this may not be enough because the distribution of observations may not be uniform and many of the cells will have too few observations. The solution to this problem is to use the Nave Bayes model. The word solution is in quotes because the problem is not really solved. Instead, an approximation, which works well in many practical situations, is used. The approximation is based on the assumption that the predictor variables operate independently of one another. That is, naive Bayes assumes that the presence of a specific feature is unrelated to the presence of any other feature. If the predictors operate independently, then the joint probabilities of multiple variables can be simply estimated as the product of the individual probabilities. 10.3 Illustration of Nave Bayes with a toy data set The small data set consists of 14 observations with the target variable play tennis and weather characteristics thought to affect the decision to play or not play. (Play Tennis: Simple Dataset with Decisions about Playing Tennis, n.d.) The observations are shown in Table 10.1. Table 10.1: The tennis data set. Observation Play Tennis Outlook Temperature Humidity Wind 1 No Sunny Hot High Weak 2 No Sunny Hot High Strong 3 Yes Overcast Hot High Weak 4 Yes Rain Mild High Weak 5 Yes Rain Cool Normal Weak 6 No Rain Cool Normal Strong 7 Yes Overcast Cool Normal Strong 8 No Sunny Mild High Weak 9 Yes Sunny Cool Normal Weak 10 Yes Rain Mild Normal Weak 11 Yes Sunny Mild Normal Strong 12 Yes Overcast Mild High Strong 13 Yes Overcast Hot Normal Weak 14 No Rain Mild High Strong Using the data in Table 10.1, the following probabilities were calculated: Figure 10.3: Probabilities for the Nave Bayes model. The calculations shown in Figure 10.3 were simply obtained by counting. For example, to obtain the conditional probability of Sunny given Not playing, note that five observations were for Sunny conditions. Of those five observations, three indicated Sunny, so the conditional probability is 3/5 = .60. Similar calculations were done for each of the probabilities in the table. To obtain probabilities of playing versus not playing for Outlook = Sunny, Temperature = Mild, Humidity = High, and Wind = Strong, the following calculations were made using the naive Bayes model: The value for playing tennis: Prob(Outlook=Sunny Given Playing tennis = Yes) = 0.222 times Prob(Temperature=Mild Given Playing tennis = Yes) = 0.444 times Prob(Humidity=High Given Playing tennis = Yes) = 0.333 times Prob(Wind=Weak Given Playing tennis = Yes) = 0.333 times Prob(Playing tennis = Yes) = 0.644 which equals = 0.222 X 0.444 X 0.333 X 0.333 X 0.643 = 0.0071 ================================================================== The value for not playing tennis: Prob(Outlook=Sunny Given Playing tennis = Yes) X Prob(Temperature=Mild Given Playing tennis = Yes) X Prob(Humidity=High Given Playing tennis = Yes) X Prob(Wind=Weak Given Playing tennis = Yes) X Prob(Playing tennis = Yes) which equals = 0.600 X 0.400 X 0.800 X 0.600 X 0.357 = 0.0412 ================================================================== * The probability of playing = 0.0071 / (0.0071 + 0.0412) = .1465 Since the probability of playing is less than 0.50, the prediction is Not play Similar calculations were completed for each of the 14 observations with a summary of the predictions in Table 10.2. Thirteen of the 14 predictions were correct using the Nave Bayes model. Table 10.2: Prediction accuracy with tennis data set using naive Bayes. Observation Play Tennis Probability Prediction 1 No 0.205 No 2 No 0.079 No 3 Yes 0.999 Yes 4 Yes 0.536 Yes 5 Yes 0.933 Yes 6 No 0.822 Yes* 7 Yes 1.000 Yes 8 No 0.340 No 9 Yes 0.861 Yes 10 Yes 0.902 Yes 11 Yes 0.578 Yes 12 Yes 0.999 Yes 13 Yes 1.000 Yes 14 No 0.278 No Note: * Indicates prediction error. 10.4 The assumption of conditional independence Referring to Figure 10.4, what is the probability of getting a three on a roll of the die, red on the spinner, and heads on a flip of a coin? Since the three experiments are independent, the probability of is simply 1/6 X 1/4 X 1/2 = 1/48 = .0208. Figure 10.4: Illustration of independence. This is what nave Bayes analysis assumes about the effects of the predictors on the target class in a supervised model. 10.5 Nave Bayes with continuous predictors For simplicity, the previous examples only had categorical predictors, but Nave Bayes can be used with continuous predictors. There are two approaches that can be used for continuous predictors. A simple solution is to discretize the continuous variables into a few categories. However, doing so is sometimes subjective. For instance, in categorizing temperature, someone may select 80 degrees as the cutoff at which temperature can be considered as High, whereas another person (from the tropics!) may choose to select 90 degrees as the border between Medium and High. This subjectivity causes obvious loss of information. But it can still be used as a quick way to get going before applying naive Bayes classification. Another method is to represent continuous variables with a probability density function. Typically, the normal or Gaussian distribution is used, but some software programs can use other distributions. The normal distribution is convenient since a continuous variable can be represented using just its mean and standard deviation. Some software implementations of Nave Bayes offer the choice of other distribution function, e.g., Poisson. The way this works is demonstrated in Figure 10.5. Consider a continuous variable, V, that is a predictor of a categorical variable Y which is either True or False. Observations on V in the data sample are grouped according to the Y values. The means and standard deviations of each group are computed and used to form the two normal density functions shown in Figure 10.5. The conditional probabilities Prob(X|Target = False) and Prob(X|Target = True) which are needed for the nave Bayes model are then obtained from the density functions. This method assumes that the normal distribution usefully represents the variable V. Figure 10.5: Demonstration of working with a continuous variable in nave Bayes. 10.6 Laplace Smoothing The nave Bayes algorithm can have a problem in certain situations, especially with small sample sizes. The problem happens if a particular value does not occur with frequency greater than zero in any level of a predictor. In this case, the conditional probability becomes zero and since the conditional probabilities are multiplied in a chain, this causes all posterior probabilities that included the level to be zero. (This was actually the case in the tennis example illustrated earlier. For the condition not playing tennis, the overcast level of the weather never occurred.) To avoid this, a Laplace Smoother (Kuhn and Johnson 2016) is used. There are several ways to incorporate the smoother with the simplest being to add one to every count in the combination of predictor values. 10.7 Example using nave Bayes with churn data The churn data set was analyzed using naive Bayes in KNIME. The KNIME workflow is shown in Figure 10.6. The same preprocessing of the churn data was included and SMOTE was used to balance the target values in the training data. Figure 10.6: Workflow for nave Bayes using churn data. Node descriptions for the workflow in Figure 10.6 are in Table 10.3. Table 10.3: Node descriptions for naive Byes with churn data. Node Label Description 1 File Reader Read the file ChurnData.csv. 2 Math Formula Compute square root of total charges 3 Partitioning Stratified sampling on Churn; 70/30 split. 4 SMOTE Oversample minority cases 5 Nave Bayes Learning Run nave Bayes on training data. 6 Nave Bayes Predictor Use the nave Bayes model to predict test data. 7 ROC Curve Create ROC curve and AUC. 8 Scorer Calculate performance metrics and confusion matrix. The evaluation metrics results for nave Bayes are shown in Table 10.4. For comparison, the metrics from a basic decision tree as well as three ensemble models are also shown. The nave Bayes model performed comparably. The area under the ROC curve was greater than decision trees, but lower than that for the ensemble models. Interestingly, nave Bayes traded specificity (lower) for sensitivity (higher). Overall, however, while nave Bayes is a contender for classification, it does not perform as well as more complex models. Table 10.4: Comparative performance of nave Bayes with ensemble models. Model ROC AUC Accuracy Sensitivity Specificity Nave Bayes 0.820 0.702 0.831 0.655 Decision tree 0.803 0.745 0.754 0.742 Random forest 0.837 0.758 0.770 0.754 GBT 0.846 0.758 0.807 0.740 XGBoost 0.846 0.752 0.783 0.740 10.8 Spam detection using nave Bayes Email has provided a convenient mode of communication that is used throughout the world by millions of people for business and personal messages. The huge number of unsolicited commercial messages most people receive daily soon, however, is at best an annoyance and at worst a means for deception or even criminal activity. The proliferation and variety of these unsolicited email messages, now called spam or junk mail, led to the development of software programs detect and screen out such emails. Spam filters have been developed to sift through email messages to separate the ham from the spam. The challenge in designing spam filters is to make the algorithm selective enough to identify spam while not flagging legitimate messages. It has been estimated that about 45% of global e-mail traffic was spam. (Spam Statistics and Facts, n.d.) Nave Bayes has been used as the machine learning engine for spam filters because of its simplicity, speed, and accuracy. Many enhancements of the basic nave Bayes model have been made to improve its performance and other algorithms have been used such as k-nearest neighbors, support vector machines.(Karimovich and Salimbayevich 2020) (Ma and Thida 2020) A data set of 5,556 messages labeled as spam or ham email messages (SMS Spam Collection Data Set, UCI Machine Learning Repository, n.d.) was downloaded and analyzed using KNIME. Example messages in the data set are in Table 10.5: Table 10.5: Examples of spam and ham email messages. Class Message ham What you doing? how are you? ham Siva is in hostel aha:-. spam Sunshine Quiz! Win a super Sony DVD recorder if you can name the capital of Australia? Text MQUIZ to 82277. spam PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S.I.M. points. Call 08718738001 Identifier Code: 49557 Expires 26/11/04 The text file was converted into a file consisting of a bag of words.13 The KNIME workflow is in Figure 10.7. and node descriptions are in Table . The workflow a table with created 5,572 rows (one for each message) and 12,230 columns with indicators for terms. (The details for textual analysis will be covered in more detail in the chapter on Text Analytics.) Figure 10.7: Pre-processing workflow in KNIME for SPAM/HAM example. Table 10.6: Node descriptions for naive Bayes with SPAM / HAM data workflow. Node Label Description 1 Excel Reader Read the Excel file SPAMHAM.xlsx. 2 Strings to Document Convert observations to documents. 3 Punctuation Erasure Remove all punctuation from the documents. 4 N Chars Filter Removes all terms with less than 3 characters. 5 Number Filter Remove all terms that consist of numbers. 6 Case Converter Convert all terms to lower case. 7 Stop Word Filter Remove stop words using built-in list. 8 Bag Of Words Creater Changes each document to individual terms, one term in each row; creates a table with 90,102 rows. 9 Document Vector Create a vector with one row per document and columns with binary indicator for presence of term (0 or 1). 10 Category To Class Adds the appropriate string class variable (either spam or ham). 11 Number to String Convert the numbers in the bag of words to strings. 12 Table Writer Write bag of words to file SPAMHAM.table. The bag of words data from the preprocessing step was submitted to naive Bayes in KNIME (Figure 10.8. Descriptions of each node used to run the nave Bayes model are in Table @ref(tab:naiveBayesHam ). Figure 10.8: Nave Bayes workflow in KNIME for SPAM/HAM example. The results of the nave Bayes analysis of the spam data set show quite good accuracy (over 98%). As shown in the confusion matrix created for the test References "],["deep-learning.html", "Chapter 11 Deep learning", " Chapter 11 Deep learning source of images: https://appliedmachinelearning.blog/2018/12/26/tensorflow-tutorial-from-scratch-building-a-deep-learning-model-on-fashion-mnist-dataset-part-1/ Earlier versions of neural networks such as the first perceptrons were shallow, composed of one input and one output layer and usually one or two hidden layers in between. As computers became more powerful during the 1990s, the feasibility of neural networks with more than two hidden layers became possible for solving more complex problems. Around the year 2000 limitations in the number of layers were being reached. Computer power was not the issue. Instead, it was found that the back propagation algorithm was hampered as layers were added resulting in gradients becoming too small (vanishing gradients) for effecting training. In other cases, the gradients would become so huge (exploding gradients) that precision suffered. In either situation, deep learning with many hidden layers would not work. Several advances in neural networks were made that overcame the problems and enabled deep learning. One of the most important was the use of a different activation function called ReLU (rectified linear activation functions). The ReLU function is shown in Figure 11.1. \\[ f(x) = \\begin{cases} 0, &amp; \\text{if } x &lt; 0\\\\ x, &amp; \\text{if } x\\geq 0\\\\ \\end{cases} \\] Figure 11.1: The ReLU function Networks with three or more hidden layers are considered deep learning models. So, deep-learning networks are distinguished from the more commonplace single-or two-hidden-layer neural networks by their depth. (Figure 11.2, from (Vzquez 2018)) Figure 11.2: Neural nets versus deep learning Deep learning is getting lots of attention lately and for good reason. Its achieving results that were not possible before. [Sarker (2021)} lists more than 40 published applications of deep learning, including: Detecting corona virus using x-ray images. Predicting air quality in cities. Recommendation systems. Speech recognition. Smart parking system. Plant disease detection. References "],["k-nearest-neighbors.html", "Chapter 12 k Nearest Neighbors 12.1 k nearest neighbors and memory-based learning 12.2 Typical applications 12.3 What is kNN? 12.4 A two-dimensional graphic example of kNN 12.5 Example of kNN: Diagnosing heart disease 12.6 kNN for continuous targets 12.7 kNN for multiclass target variables", " Chapter 12 k Nearest Neighbors 12.1 k nearest neighbors and memory-based learning K nearest neighbors (kNN) is another example of a supervised model, a very simple supervised model. It may be the simplest, most intuitive model that is used for data mining. Despite the simplicity of the model, it can work quite well with large data sets. It is also one of the top 10 data mining tools in terms of popularity of usage. (Top 10 Machine Learning Algorithms You Should Know in 2021, n.d.) kNN is an example of a family of algorithms known as instance-based or memory-based learning that classify new objects by their similarity to previously known objects. kNN does not require many assumptions about the input or predictor data or anything about error distributions. Formally, it is a non-parametric model. In parametric models (such as multiple regression), you must make assumptions about the distribution of the error term and then estimates are made of the parameters of the distribution. No such considerations are involved with kNN. A training phase is used to determine the best value for k (the number of neighbors used to compute similarity), but otherwise no training is needed. It can be directly applied to all of the data. If you have a data set, you simply apply kNN. An important disadvantage of kNN is that the model must be run through all the data available each time a new observation needs to be classified. This can be time consuming for large data sets. With logistic regression or OLS, you only save the model and apply it to new data, which is much faster. Since there is no model per se in K-nearest neighbors, the model essentially is created each time an analysis is needed. 12.2 Typical applications If we look at typical applications, the list is much like other predictive models such as logistic regression and decision trees, such as: Flagging fraudulent insurance claims. Predicting customer response to promotional offers. Selecting the most effective treatment for a medical condition. Classifying free-text responses. Recommending the next offer to a customer in retail settings. Searching for similar documents. Recommender systems. 12.3 What is kNN? KNN is a method for classifying objects based on similarity. It is called a lazy algorithm, which means is that it does not use the training data points to do any generalization and is contrasted with eager algorithms. The differences are described in Table 9.1. In other words, there is no explicit training phase, or it is very minimal. Most of the lazy algorithms  especially kNN  make decisions based on the entire data set. Some of the distinctions between lazy and eager learners are shown here. With lazy learners The data is stored, not learned from it. Classifications are made as soon as a new observation is received. The model is richer since it does not rely on a single pre-specified model. Time to classify new observations can be considerable with large data sets. Learning time is non-existent (except for determining the optimal value of k) and the need for minor preprocessing. With eager learners A model is developed (learned) based on the training data. The model is used to classify new observations as they are received. The model depends upon a single function that is derived in the learning phase. Classification of new observations is very fast. Learning time can be considerable. The algorithm works like this. A labeled data set with a known categorical target vector, Y (which may be binary or multichotomous) and a matrix X with p potential predictors or features is used. Each case is considered a point in a multidimensional feature space. This allows the computation of distances among the points (or cases) based on locations (values) in the feature space. KNN is then used to classify a new observation that is described by p variables (X1, X2, , Xp). To do so, the algorithm computes the distance of the new observation to every other observation in the data set. Euclidean distance is typically used, but other metrics such as the absolute value of differences, squared Euclidean distance, Jaccard distance, Manhattan distance, or cosine distance are also used. In most cases it is important to standardize the X variables prior to determining distances to avoid having the variables with the largest numerical values dominating the computed distances. The distances of the new observation to each row of the data set are computed and are ranked from smallest to largest. The k smallest distances (the nearest neighbors) selected. No assumptions about the form of the relationship between Y and the Xis is made; no parameters are estimated. A majority vote of its k neighbors is taken and the category with the smallest distance is selected as the predicted class. k  the number of nearest points or neighbors to consider in making an assignment to a category  is usually an odd number so that no ties are formed; if an even number is set for k, then random choices for Y class are made for ties.) Since a majority is used to make predictions, kNN can be applied to targets with two or more than two possible outcomes. There is a trade-off implicit here. While there is only a minimal training phase used to determine the best k, deployment of kNN is not efficient in terms of processing time and memory requirements. More time is needed because all data points might take part in determining a prediction. Furthermore, all of the data must be stored and available for deployment. 12.4 A two-dimensional graphic example of kNN A very simple example is used to illustrate how kNN works. Figure 9.1 shows a plot of characteristics of different types of homes. The two predictors are area in square feet and number of rooms. Both variables are standardized to zero mean and unit standard deviation. Toward the center of the chart is an unknown type of home. The area and number of rooms for the unknown were standardized using the same mean and standard deviation as was computed for the known data. The distances of the unknown to every data point in the original set are represented by the arrows. The three closest known observations are circled, assuming for this example the k = 3. Two of the closest homes are flats and one is an apartment. Using the majority rule, the unknown is classified as a flat. (See Figure 12.1) Figure 12.1: Graphical illustration of kNN. 12.5 Example of kNN: Diagnosing heart disease This database is part of a larger study of factors associated with heart disease collected at the Cleveland Clinic. The data set contains test results on 303 patients measured on 13 attributes plus a target variable indicating the presence or absence of heart disease. This data set has been used in several published studies on machine learning. The variables are listed in Figure 12.2 along with an indication of each as numeric or categorical. For each of the categorical variables, the levels are shown as well. Figure 12.2: Variables in the Cleveland Heart Disease study. KNIME was used to determine the best value for k by finding the accuracy for values of k from 1 to 20. A plot of Accuracy versus k is shown in Figure 12.3, which indicates that the best k was 7. Figure 12.3: Accuracy versus k with heart data. The KNIME workflow is shown in Figure 12.4. Descriptions of each node in the workflow are given in Table 12.1. Figure 12.4: kNN workflow for heart disease data. Table 12.1: Workflow nodes for kNN with heart data Node Label Description 1 File Reader Read the file heat_data.csv 2 Data Explorer Obtain histograms and descriptive statistics. 3 Number To String Create 70/30 split into training and test partitions. 4 Nave Bayes Learner Run nave Bayes on training data. 5 Nave Bayes Predictor Use the nave Bayes model to predict test data. 6 Scorer Calculate performance metrics and confusion matrix. 12.5.1 Results The confusion matrix for the kNN model with k = 7 is in Table 12.2. Table 12.2: Confusion matrix for predicting heart disease with kNN Actual Healthy Heart disease Totals Healthy 45 3 48 Heart disease 7 34 41 Totals 52 37 89 Accuracy measures for the kNN analysis are in Table 12.3 Table 12.3: Accuracy metrics for kNN with heart disease data Model ROC AUC Accuracy Sensitivity Specificity kNN 0.915 0.888 0.829 0.938 12.6 kNN for continuous targets While kNN is primarily a method for classification, it can also be used with continuous target variables much like ordinary least squares (OLS) regression. KNIME does not include a node for kNN regression, so a small R Snippet was created to use the package FNN. One advantage of kNN regression is that non-linear relationships can be easily captured. Ordinary regression requires transformations and/or adding predictors to capture non-linearities. A simple data set with a single predictor (X) and a continuous target(Y) was created. A scatterplot of the data is shown in Figure 12.5. Note that the relationship is non-linear and slightly concave upward. Figure 12.5: Simulated data with non-linear relationship between Y and X. A KNIME workflow was created, shown in Figure 12.6, to compare kNN and OLS. For this small demonstration data, a test data subset was not created. Figure 12.6: KNIME workflow to compare kNN regression with OLS. KNIME does not have a node to perform kNN regression, so this R code was inserted in Node 7 which contains an R Snippet. Note that this R code will have to be customized for new problems. library(FNN) mydata &lt;- as.data.frame(knime.in) TrainData &lt;- as.data.frame(mydata[,1]) TrainTarget &lt;- mydata[,2] TestData &lt;- as.data.frame(mydata[,1]) YKNN = knn.reg(train = TrainData, test = TestData, y = TrainTarget, k = 3) knime.out &lt;- as.data.frame(cbind(mydata,YKNN$pred)) The metrics for the two models were: Table 12.4: Comparison of kNN and OLS with non-linear data Metric kNN OLS R-squared 0.992 0.935 Mean absolute error 2.545 7.209 Root mean squared error 3.132 8.694 Mean absolute percentage error 0.645 1.937 The metrics show that kNN more accurately captured the relationship in the data. OLS could be improved by creating a polynomial model, of course, but the point is that kNN regression did not require a model to be specified a priori. Plots of the predicted Y (on the y-axis) and the actual Y values (on the x-axis) are show in Figures 12.7 and 12.8 for the two models. Note that the OLS model did not capture the non-linearity and created a downward concave result. Figure 12.7: OLS results. Figure 12.8: kNN results. 12.7 kNN for multiclass target variables KNN is also effective with target variables that have more than two classes. An example data set was obtained from the UCI Machine Learning Repository to illustrate kNN with a multiclass target. The data set has 214 observations, a 6-level categorical target, Type of glass, and nine continuous predictors. The kNN workflow from KNIME is shown in Figure 12.9. The model was run with a 50/50 split between the training and test rows and k was set to two, which was found to result in the highest accuracy. Figure 12.9: KNIME workflow for kNN analysis of glass data. The confusion matrix for predicting type of glass in the test data set is shown in Table @ref[tab:GlassConfusionMatrix]. The accuracy varies by type of glass and the overall accuracy is just over 73%. While this may not seem very good, predicting to six classes with a small data set is not an easy task for the model. Table 12.5: Confusion matrix for glass data Variables Buliding float Buliding nonfloat Vehicle windows Containers Tableware Headlamps Buliding float 24 6 5 0 0 0 Buliding nonfloat 4 30 4 0 0 0 Vehicle windows 1 2 5 0 0 0 Containers 0 2 0 4 0 0 Tableware 0 1 0 1 2 0 Headlamps 2 1 0 1 0 12 References "],["tree-models.html", "Chapter 13 Tree models 13.1 Classification trees 13.2 Forming classification trees 13.3 Varieties of classification tree algorithms 13.4 Criteria for splitting and growing a tree 13.5 Overfitting 13.6 Example of a classification tree 13.7 Regression trees 13.8 Strengths and weaknesses", " Chapter 13 Tree models Decision trees (aka tree-based models) are commonly used in data mining to perform predictive analytics, typically with a single categorical dependent variable and multiple predictor variables (which can be continuous or categorical). There are two major types of decision trees: classification trees and regression trees. Classification trees are discussed in the first section and regression trees are discussed in the second section. Decision tree algorithms are automatic in that the independent variables are selected by searching for optimal splits using a measure of purity or effect size. 13.1 Classification trees As an example, the results of a classification tree could inform customer targeting for a marketing campaign. Consider the hypothetical results of a tree that is based on four predictor variables: Is the customers income greater or less than $70k? How old is the customer? Is the customer a college graduate? Is the customer male or female? A classification tree based on past data on purchases might produce a tree-like structure as shown in Figure 13.1. Notice that some of the branches terminate before all the variables are considered. This is because further splitting of the branch does not lead to any more useful differences. Figure 13.1: Initial calculations for the regression tree. Note that after splitting by income, the next variable selected for splitting differs depending on the income level. For income less than or equal to $70k, the next split is on age. For income greater than $70k, the split is on education. This is known as an interaction effect. Finally, note that the splits can be either on continuous variables such as age or on nominal variables such as gender. The process starts with the root node, which represents entire data set. The process proceeds by creating branches where the data is split into sub nodes. The final splits result in leaf or terminal nodes. From the hypothetical tree shown in Figure 13.1, a series of rules are produced: If Income  $70,000 AND Age  30 THEN probability of purchase = 14%. If Income  $70,000 AND Age &gt; 30 AND Female, THEN probability of purchase = 40%. If Income  $70,000 AND Age &gt; 30 AND Male, THEN probability of purchase = 63%. If Income &gt; $70,000 AND No college, THEN probability of purchase = 38%. If Income &gt; $70,000 AND College graduate, THEN probability of purchase = 70%. A classification tree is not an inferential technique that is suitable for statistical hypothesis testing. One way to think about classification trees is that they recursively split data into smaller and smaller branches that are increasingly pure in terms of the target variable. To find a split, the program examines all the input variables and selects the one at each stage that is most effective according to the criteria in the algorithm. This search process involved with forming a tree violates the logical premises of classical statistical testing since the data is used to inform the splits into branches. However, when there are many observations and many variables and in cases where no well-defined theory exists, classification trees can help the researcher to discover relationships that can be tested later a different sample. Some common uses of classification trees are: Market segmentation  identifying segments most likely to purchase. Stratification  dividing cases into high/medium/low risk, for example. Prediction  creating rules and use them to predict future outcomes. Data reduction and variable screening  screening many variables to identify best prospects. Interaction detection  finding variables with effects which differ according to the levels of other variables. Category merging  recoding variables with large numbers of levels into fewer categories without substantial lose of predictive information. This is one attraction of classification trees: the technique works without much thinking. 13.2 Forming classification trees There are exponentially many possible classification trees with a given set of attributes. The number is very large because continuous predictors can be split in many ways and the same predictor can be used again and again as the tree is built. Since it is usually impossible to examine all possible tree structures in each problem, an algorithm is used to grow a tree that is reasonably accurate instead of optimal. Classification trees are known as greedy algorithms since they use a strategy of proceeding stage by stage and once a split in the data is made, the algorithm does not go back after making additional splits to check previous splits. Thus, locally optimum selections are made at each stage. The result is usually a very good model, but not one that is not necessarily optimal. However, there are dangers and costs associated with the approach. You put in a long list of variables and the program selects the best predictors (and the best point for splitting). 13.3 Varieties of classification tree algorithms A wide variety of different models and techniques has been developed all of which fall under the umbrella term of classification trees. Models for classification trees differ in terms of how branches of the tree are split off from the main trunk, in terms of stopping rules, the types of variables that can be used, what is provided in the output, etc. Several algorithms are available for classification trees including CART, C5.0, and CHAID. By no means will these give the same answers to a given problem. One reason is that the programs use different criteria to select which parent nodes to split, as shown in Table 13.1. Table 13.1: Characteristics of three classification tree algorithms. Algorithm Splitting criterion Input variables Target variable Splits CART Gini index Categorical or continuous Categorical Binary C50 Entropy Categorical or continuous Categorical Binary or multiway CHAID Chi-square test Categorical Categorical Binary or multiway Classification trees have been around a long time, but until recently they were frequently discussed in derogatory terms. One of the first models, AID, was called a substitute for thinking. AID was an acronym for automatic interaction detection. Classification trees remain controversial, and some researchers claim that classification trees should not be used. By automatically combing through data sets in search of relationships, the models have the potential to find spurious associations that may appear to be plausible but are only artifacts due to randomness. That is very much true if you use classification trees on small samples and do not develop both training and testing subsets. The criticism is due to past applications where small data sets were used. In the era of plentiful data, these concerns no longer are as important. Data mining, after all, is for large data sets. For continuous predictor variables, all possible splits are considered. Thus, for n distinct values of a predictor, n-1 potential splits are considered. For nominal predictor variables, the number of possible splits into two groups depends upon the number of distinct categories. Table 13.2 gives examples of the number of possible splits as a function of distinct categories. Table 13.2: The number of possible binary splits by number of categories in a nominal predictor. Categories Possible.splits 2 1 3 3 4 7 5 15 6 21 In general, the number of possible splits as a function of categories is given by Sterling Numbers of the Second Kind (Wolfram MathWord: Stirling Numbers of the Second Kind, n.d.). 13.4 Criteria for splitting and growing a tree As noted above, various approaches have been developed for selecting a node to split in forming a classification tree. Three approaches for splitting are the Gini index, entropy, and chi-square. Each of these can measure the purity of a node. This data set has three possible predictors: Feature 1, Feature 2, and Feature 3. So, the first split could be on any of the three predictors. 13.4.1 The Gini index The Gini index of node purity, \\(G_{A}\\) is computed using equation (13.1). \\[\\begin{equation} \\ \\mathit{G_{A}}\\; = 1 - (p_{0}^2\\; + p_{1}^2\\;) \\tag{13.1} \\end{equation}\\] where \\(p_{0}\\) is the proportion of cases in the node that are at level 0 and \\(p_{1}\\) the proportion that are at level 1. Figure 13.2 shows the Gini index as \\(p_{0}\\) and \\(p_{1}\\) are varied. The maximum value for the index is .5 and is 0 for either all cases equal 0 or equal 1. Figure 13.2: The Gini index as a function of p0 and p1. Some algorithms only consider splitting each node into two child nodes while others, such as CHAID can create multi-category nodes. For this example, binary splits in the data set are considered. The overall Gini index for a binary split is computed as the weighted average of the Gini values for the two possible branches. To consider splitting a node in two, \\(G_{A}\\) is computed for each of the resulting nodes, which are labeled \\(G_{A\\_Left}\\;\\) and \\(G_{A\\_Right}\\;\\). The total Gini index is then calculated as the weighted average of \\(G_{A\\_Left}\\;\\) and \\(G_{A\\_Right}\\;\\), where the weights are the proportions of cases in the left and right nodes, given by \\(w_{n\\_Left}\\) and \\(w_{n\\_Right}\\) . So, the Gini index for a split is calculated with equation (13.2). \\[\\begin{equation} \\ \\mathit{G_{A\\_split}\\; = w_{n\\_Left} \\times G_{A\\_Left}\\; + w_{n\\_Right} \\times G_{A\\_Right}\\; }\\; \\tag{13.2} \\end{equation}\\] 13.4.2 Information Gain Information in a node is a measure of impurity, with higher values indicating greater impurity. The expected information in each node for a binary target variable, IInfo_A, is computed using equation (13.3). \\[\\begin{equation} \\ \\mathit{I_{Info\\_A}\\; = \\sum_{i=1}^{2} p_{i}\\; \\times \\log_{2} p_{i}\\;}\\; \\tag{13.3} \\end{equation}\\] where \\(p_{i}\\) is the proportion of cases in the node that are at level i (either 0 or 1). Figure 13.3 shows how expected information varies with \\(p_{0}\\) and \\(p_{1}\\). The maximum value for the index is 1.0 and is 0 for either all cases = 0 or all cases = 1. Figure 13.3: Expected information as a function of p0 and p1. To select a node to split, information gain is computed, which is the sum of the information values for the parent node minus the sum of he expected information values for the two child nodes. To consider splitting a node in two, \\(I_{A}\\) is computed for each of the resulting nodes, which are labeled \\(I_{A\\_Left}\\;\\) and \\(I_{A\\_Right}\\;\\). The information contained in the two child nodes is then calculated as the weighted average of \\(I_{A\\_Left}\\;\\) and \\(I_{A\\_Right}\\;\\), where the weights are the proportions of cases in the left and right nodes, given by \\(w_{n\\_Left}\\) and \\(w_{n\\_Right}\\). So, the information gain for a split is calculated with equation (13.4). \\[\\begin{equation} \\ \\mathit{I_{A\\_split}\\; = w_{n\\_Left} \\times I_{A\\_Left}\\; + w_{n\\_Right} \\times I_{A\\_Right}\\; }\\; \\tag{13.4} \\end{equation}\\] 13.4.3 Chi-square as a splitting criterion This approach to splitting nodes uses the chi-square statistic for selecting a parent node to split. In this case the preferred split is based on which split increases the chi-square value the most, since two nodes that are each pure node in terms of 1s and 0s will have a sum of chi-squares greater than nodes which are balanced. The formula used at each stage of the classification tree where the target is binary (labeled 0 and 1) and binary splits are being considered is equation (13.5). \\[\\begin{equation} \\ \\mathit{\\text{Chi-square}{_A}\\; = \\sum_{i=1}^{2} \\frac{(a_i\\; - e_i\\;)^2 }{e_i}\\;}\\; \\tag{13.5} \\end{equation}\\] where \\(a_{i}\\) and \\(e_{i}\\) are the actual number and expected of 0s in the node and \\(a_{1}\\) and \\(e_{1}\\) are the actual number and expected of 1s in the node. Figure 13.4 shows how shape of the chi-square value varies with different proportions of \\(p_{0}\\) and \\(p_{1}\\). Figure 13.4: Chi-square as a function of p0 and p1. Note that as a node increases in purity in terms of \\(p_{0}\\) or \\(p_{1}\\), chi-square increases. Therefore, the parent node with the highest sum of chi-squares for two child nodes is selected for splitting. To illustrate, consider the toy data in Table 13.3. This data consists of 20 observations, 9 zero responses and 11 responses of 1. Table 13.3: Toy data set Feature 1 Feature 2 Feature 3 Response F1_A F2_A F3_A 0 F1_A F2_A F3_B 1 F1_A F2_A F3_A 1 F1_B F2_B F3_A 1 F1_B F2_B F3_B 1 F1_B F2_B F3_B 0 F1_A F2_B F3_B 1 F1_A F2_B F3_A 0 F1_A F2_B F3_B 1 F1_B F2_B F3_B 1 F1_A F2_B F3_A 0 F1_A F2_B F3_A 0 F1_A F2_A F3_B 1 F1_B F2_B F3_A 0 F1_B F2_B F3_A 0 F1_B F2_B F3_B 1 F1_A F2_A F3_B 1 F1_A F2_A F3_A 0 F1_A F2_A F3_A 0 F1_A F2_A F3_B 1 The Gini index, information gain, and chi-square were computed for candidate splits on Features 1, 2, and 3. All three of the criteria (shown in Table 13.4 led to the same initial split on Feature 3, but this will not always be the case. Table 13.4: Criteria for initial splits.. . Feature to split Gini index Information gain Chi-square Feature 1 0.488 0.011 0.707 Feature 2 0.496 0.512 1.955 Feature 3 0.250 0.695 3.162 The tree so far is shown in Figure 13.5: Figure 13.5: Split #1 Continuing with just the Gini index, the next possible splits are shown in Table 13.5. Table 13.5: Criteria for second splits. Feature to split Gini index Split Node 2 by Feature 1 0.305 Split Node 2 by Feature 2 0.317 Split Node 3 by Feature 1 0.150 Split Node 3 by Feature 2 0.167 The best split at this stage is to split Node 3 by Feature 2. Then, the best split is to split Node 2 by Feature 1. The resulting tree is shown in 13.6. Note that the best split of Node 2 is based on Feature_1 while the best split on Node 3 is based on Feature 2. No further splits were made with this example. Figure 13.6: Final tree 13.5 Overfitting Trees can grow in complexity and are susceptible to overfitting data. Bramer defines overfitting as follows: A classification algorithm is said to overfit to the training data if it generates a classification tree  that depends too much on irrelevant features of the training instances, with the result that it performs well on the training data but relatively poorly on unseen instances. \"Realistically, overfitting will always occur to a greater or lesser extent simply because the training set does not contain all possible instances. It only becomes a problem when the classification accuracy on unseen instances is significantly downgraded. (Bramer 2007) Figure 13.7 shows conceptually how increasing the size of a classification tree by allowing more and more modes will usually increase the accuracy in the training data, but eventually increases error in the test or unseen data. Overfitting the training data fits the model to errors or idiosyncrasies of the training data which are not present in other data sets. Figure 13.7: How overfitting can degrade accuracy. There are two general approaches to avoid overfitting: Simply avoid growing large trees  by providing a stopping rule such as the minimum number of observations in a node or the maximum number of splits. Grow a large tree and cut branches afterwards, which is known as pruning. The full tree is grown (early stopping might additionally be used), and each split is examined to determine if it brings a reliable improvement. 13.6 Example of a classification tree Customer churn occurs when a customer (player, subscriber, user, etc.) ceases his or her relationship with a company. The full cost of customer churn includes both lost revenue as well as the marketing costs involved with replacing those customers with new ones. Reducing customer churn is a key business goal of nearly every online business because it is almost always more difficult and expensive to acquire a new customer than it is to retain a current paying customer. The ability to predict that a particular customer is at a high risk of churning, while there is still time to do something about it, represents an important potential for increasing revenue and profit. In the case of telecom companies, customers may cancel for many reasons, including poor service, availability of specific hardware, and price. Identifying potential churners before they quit can be the first step in efforts to lower the churn rate. It only makes financial sense to offer incentives only to potential churners and not to customers that are going to remain. Analytic techniques can be used to develop predictive models to identify likely churners based on customer characteristics and behaviors. For this example, a data set, TelcoChurn5000.csv, consisting of 5,000 customers of a telecom provider is available with the variables shown in Table 13.6. Table 13.6: Variables in the TelcoChurn5000.csv data set. Variable Description State 2-letter code of the US state of customer residence Months w/ carrier Number of months with the telco provider Intl plan? The customer has international plan (yes/no) Intl mins Total minutes of international calls # intl calls Total number of international calls $ Intl Total charge of international calls Voice mail? The customer has voice mail plan (yes/no) # vmail messages Number of voice-mail messages Call minutes Total minutes of calls # of calls Total number of calls $ Total Total monthly charges # service calls Number of calls to customer service Churn Customer churn (yes/no) A classification tree was developed in KNIME using the workflow is shown in Figure 13.8. Figure 13.8: KNIME workflow for Churn data. The nodes in the workflow are described in Table 13.7, Table 13.7: Descriptions of nodes in the churn workflow. Node Label Description 1 File Reader TelcoChurn5000.csv 2 Statistics Descriptive statistics of Churn data set. 3 Partitioning A 60/40 (Training/Test) random split of the data set stratified on Churn is formed. Set random seed = 123. 4 SMOTE SMOTE (Synthetic Minority Over-sampling Technique) oversamples the class of Churn = no to equal Churn = yes. This was done because the of the imbalance of Churn in the data: 707 yes and 4293 no. 5 Statistics Descriptive statistics on Churn target variable showing balance. 6 Decision Tree Learner Set minimum records per node = 150 to pre-prune the classification tree. 7 Decision Tree Predictor Use the decision tree to create predictions of Churn on the unbalanced Training data (prior to running SMOTE). 8 Scorer Compute performance statistics and classification (confusion) matrix for the Training data. 9 Decision Tree Predictor Use the decision tree to create predictions of Churn on unbalanced Test data. 10 Scorer Compute performance statistics and classification (confusion) matrix for the Test data. 11 Joiner Join accuracy tables for Training and Test data. 12 Excel Writer Write accuracy tables to ChurnAccuracy.xlsx; include Row Key. The performance results for the Training and Test summarized from the Excel file are shown in Table 13.8. Table 13.8: Performance measures for the classification tree. Metric Training data Test data Accuracy 0.953 0.942 Cohens kappa 0.812 0.765 Precision 0.818 0.783 Sensitivity 0.861 0.816 Specificity 0.969 0.963 F-measure 0.839 0.799 The classification tree performed quite well on this data set, with comparable performance for both the Training and Test data as shown in Tables 13.9 and 13.10. Table 13.9: Confusion matrix for Churn Training data. Training Churn=Yes Churn=No Totals Churn=Yes 365 59 424 Churn=No 81 2495 2576 Totals 446 2554 3000 Table 13.10: Confusion matrix for Churn Test data. Training Churn=Yes Churn=No Totals Churn=Yes 231 52 282 Churn=No 64 1653 1717 Totals 295 1705 2000 13.7 Regression trees Decision trees can also be used to predict continuous target variables. In such applications these are known as regression trees. The analysis is much like the case with categorical target variables: the model produces a series of splits using selected predictor variables. With regression trees, much like ordinary regression, you can have continuous or nominal predictors. However, the other assumptions associated with linear regression regarding error distributions and so on, are not really relevant because regression trees are not a classical statistical technique. 13.7.1 How regression trees work Regression trees operate by successively dividing a data set into smaller and smaller groups that are more homogeneous with respect to the target variable. The groups are nodes and each node lower in the tree is more homogeneous in terms of the target variable than those nodes higher in the tree. The model starts with no predictors and then examines each possible predictor in turn to select the best variable for initial split. The process of building a regression tree is illustrated using a simple data set (DemoRegressionTrees.csv) consisting of 100 observations of home prices as the continuous target variable with area in square feet (either 1,000 or 2,000) and quality (either high or average) as the predictors. The first five and last five rows are shown in Table 13.11. Table 13.11: Performance measures for the classification tree. Row Price in 000s Area in sq. ft. Quality 1 203.8 1000 High 2 195.0 1000 Average 3 200.9 1000 High 4 403.2 2000 High 5 402.8 2000 High             96 306.9 2000 High 97 194.3 1000 Average 98 206.2 1000 High 99 310.1 2000 High 100 298.0 2000 Average The tree building process uses the criterion of minimizing the sum of squared errors as binary splits in the predictors are made. The initial sum of squares (\\(SST_{I}\\)) is given by equation (13.6). \\[\\begin{equation} \\ \\mathit{\\text{SST}{_I}\\; = \\sum_{i=1}^{n} (y_{i}\\; + \\bar{y}\\;)^2\\; = 502,301}\\; \\tag{13.6} \\end{equation}\\] where n = the number of observations, \\(y_{i}\\) = the price for observation i and \\(\\bar{y}\\;\\) = the mean value of the prices. Next, splits are considered by area in square feet and quality. When a split is made, the sum of squares of each branch of the split is computed and added together. The sum of squares after splitting is subtracted from \\(SST_{I}\\) and the split with the greatest reduction in sum of squares is selected. The sum of squares for the binary splits is given by (13.7). \\[\\begin{equation} \\ \\mathit{\\text{SST}_{binary}\\; = \\sum_{i=1}^{n_L} (y_{i}\\; + \\bar{y}_L\\;)^2\\; + \\sum_{i=1}^{n_R} (y_{i}\\; + \\bar{y}_R\\;)^2\\;} \\tag{13.7} \\end{equation}\\] where \\(n_{j}\\) = the number of observations in the jth node after split, \\(\\bar{y}_L\\;)\\;\\) = the mean value of the prices in the jth node with j = L for left node and j = R for the right node. The results of the calculations are shown in Figure 13.9. Figure 13.9: Initial calculations for the regression tree. Splitting on area produced the greatest reduction in sum of squares and therefore the split should be made on area. In a larger problem, a recursive process is used, and each predictor is considered. Each potential split of a continuous predictor must be calculated, so that for a variable with k distinct values, k-1 splits considered. There are several different algorithms for regression trees, but a commonly used approach is the CART method. The depth of the tree can be controlled by the programs. (J. F. Breiman L. and Stone 1984) The predictions are made using the terminal nodes at the bottom of the tree. The prediction estimates of the target variable equal the average value of the observations in each terminal node. Simple regression trees have a limitation, as we will see in an example. The limitation stems from the model structure itself because the estimates of the target are only made with the average value in each terminal node. Therefore, if there are only a few terminal nodes, then only a few different prediction values will be made. This happens even though the original target variable has many, perhaps hundreds or thousands of different values if the target is continuous. The predictions are said to have limited cardinality. Cardinality is just the number of distinct values in the set of predicted values. This by itself can limit the R2 and predictive accuracy of regression trees. 13.7.2 Example: Predicting home prices Next, an example again is based on predicting home prices. This data set has 522 observations in the file RealEstatePrices.csv. The variables in the data are listed in Table 13.12. Table 13.12: Performance measures for the classification tree. Variable Values Sales Price Continuous Finished square feet Continuous Air conditioning Yes or No Garage size # of cars Pool Yes or No Year built Continuous Quality Fair, Moderate, or Best Lot size Continuous Adjacent to highway Yes or No A workflow (Figure 13.10) was created in KNIME to predict Sales Price on Test data using regression trees and, for comparison, ordinary linear regression. Figure 13.10: KNIME workflow for Real Estate Example. The nodes in the workflow for this example are described in Table 13.13. Table 13.13: Node descriptions for Real Estate workflow. Node Label Description 1 File Reader Read RealEstateData.csv 2 Partitioning Randomly split data 70/30; set seed to 123. 3 Simple Regression Tree Learner Limit number of levels to 5; Minimum split node size = 10; Minimum node size = 5. 4 Simple Regression Tree Predictor Predict Sales Price on Test data. 5 Numeric Scorer Calculate performance measures on Regression Tree using Test data. 6 Scatter Plot Plot Predicted vs. Actual for Regression Tree on Test data. 7 Linear Regression Learner Run the same data using OLS. 8 Regression Predictor Predict Sales Price on Test data. 9 Numeric Scorer Calculate performance measures on OLS using Test data. 10 Scatter Plot Plot Predicted vs. Actual for OLS on Test data. The results for the two analyses are shown in Table 13.14. Ordinary least squares regression performed slightly better in this example. One reason may be the limitation on the predicted values when using regression trees. There are only as many distinct predictive values as the number of terminal nodes. In the original Selling Price variable there are 131 distinct values. In the predicted values from the regression tree, there are only 30 distinct values, while there are 157 distinct values in the predictions using ordinary regression. Table 13.14: Comparison of regression tree and OLS. Metric Regression tree Ordinary regression R-sqaured 0.813 0.836 mean absolute error 36,544 36,710 mean squared error 3,189,206,925 2,789,026,413 root mean squared error 56,473 52,811 mean signed difference -11,198 -4,908 mean absolute percentage error 0.14 0.14 Since the results from the regression tree and ordinary regression differed, an exploration of combining the two results was conducted. The predictions from the two models were averaged and compared the actual Selling Price data. The performance metrics for the averaged predictions are shown in Table 13.15. Table 13.15: Performance metrics for averaged predictions. Metric Averaged predictions Rsquared 0.853 mean absolute error 33,761 mean squared error 2,510,151,735 root mean squared error 50,101 mean signed difference -8,053 mean absolute percentage error 0.13 The metrics indicate improved accuracy. In particular the root mean squared error (RMSE) was reduced to 50,101 compared with the RMSE for the regression tree of 56,473 and 52,811 for OLS. The RMSE for the averaged model was about 5% lower than the OLS mode. Whether or not this improvement is important, it does illustrate that combining prediction estimates can result in better performance. This is a similar effect to the one discussed in the chapter on ensemble models. 13.8 Strengths and weaknesses Decision trees are widely used in data mining but as with virtually every technique, there are both strengths and weaknesses. Strengths of decision trees Interpretation is usually straightforward and easy to demonstrate and explain. There are few underlying assumptions that must be met. The results are displayed in a tree-like structure, which is intuitively appealing. Rules generated are transparent. Interactions among predictor variables can be identified. Outliers and missing values can be handled without problems (with most algorithms). Non-linear relationships are handled without problems. Predictor variable selection is automatic. Binary, categorical, ordinal, and interval target and predictor variables can be used. Weaknesses of decision trees Slight changes in the data set can produce dramatically different results. Large datasets are needed. Careful validation is required to avoid over-fitting. The data snooping process can be misleading. There is a bias toward selecting categorical predictors with many levels. Considerable judgment and experimentation may be needed to develop a suitable result. References "],["neural-networks.html", "Chapter 14 Neural networks 14.1 What are artificial neural networks? 14.2 The road to machine learning with neural nets 14.3 Example of a neural network 14.4 Training a neural net 14.5 Considerations in using neural nets 14.6 Neural network example", " Chapter 14 Neural networks Artificial neural networks are a class of extremely powerful techniques that have become quite popular in recent years. The reason is that they can produce very accurate predictions when used in supervised data mining applications. These networks are very flexible algorithms that can be applied to different types of modeling including supervised and unsupervised problems. Neural networks can be used in place of or in conjunction with logistic regression and decision trees when there is a categorical dependent variable. Neural networks are very flexible  they also work with continuous dependent variable, so they can be used in a regression-type setting. In applications where regression, logit, decision trees, and other techniques might be used, neural nets can evolve much more complex, more flexible, and potentially more accurate models. The downside is that the models are often difficult to interpret and explain. Neural nets are especially effective where there are many input variables, and these have non-linear relationships with the target variable. Whats fascinating about neural nets is that the model structure needs only to be specified in terms of the number of nodes and hidden layers. The analyst does not have to be concerned about non-linearities and/or interactions among predictors. In a sense, when using neural nets, the computer learns from the data. A specific model is not specified as with regression models. Instead, the process works like this: Heres my data, this is how complicated the net can be. Develop a predictive the model. These are not statistical models but rather powerful computer programs. Thus, no assumptions are made about normality, linearity, etc. This has led to the concept of machine learning. The flexibility and complexity of neural net models is both the source of the attractiveness of neural nets as well as part of the challenges with effectively using them. Neural nets work best when there is many observations where training, validation, and test subsets can be formed. Neural nets can be actually very easy to apply and use with modern software. There are many software programs available. The resulting models when using neural networks can be quite complicated even though in one sense these are just a combination of non-linear regression models. Its the combination of many simple models that makes artificial neural nets complicated. 14.1 What are artificial neural networks? The artificial adjective is used because these models were inspired by attempts to simulate biological neural systems. The first neural networks were not originally developed by data analysts, computer experts, or statisticians. It was the original research into human brain activity that led to the development of the computer models. The artificial neural net works this way, too, although the number of elements in even the most complicated neural networks is nowhere near the billions in the human brain. (The human brain is thought to contain 100 billion neurons.) So, artificial neural nets as used in data mining are nowhere near as proficient or as complicated as the human brain. Despite this, much of the terminology persists from the original research that was done on the human brain, with terminology being used with terms such as neurons, learning, nodes, activation functions, and synapses in machine learning neural networks. 14.1.1 Human neurons to mathematical models Figure 14.1 (Source: (Unal and Baiftci 2021)) is a simplified model of a typical human neuron. In very basic terms, the neuron works as follows. The dendrites receive chemical and electrical signals from other neurons. The soma (nucleus) processes the information from the dendrites and creates an output which is transmitted by the axon. The axon is then connected via synapses to other neurons. With many neurons combined in a network, the result is the powerful capabilities of the human mind. Figure 14.1: Simplified model of human neural net In 1943 McCulloch and Pitts, two neurophysiologists at Yale University, were interested in understanding the anatomy and functioning of the human brain (McCulloch and Pitts 1943). They proposed a mathematical model to explain how human neurons worked to make decisions and create insights. They hypothesized that the human brain works by using millions of relatively simple elements, essentially on-off switches. Their idea was that a very complicated set of behaviors, such as those evidenced by the human brain, can arise from a set of relatively simple units if enough of them are acting in concert or sequence. McCulloch and Pitts proposed that the neurons were activated in a binary manner - either fire or not-fire. The basic element in their model can be stated mathematically as: \\[\\begin{equation} {S} = \\sum_{i=1}^{n} I_{i} W_{i} \\end{equation}\\] \\[y(S) = \\begin{cases} 1, &amp; \\text{if } S\\geq T\\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] Where \\(I_1\\), \\(I_2\\),,\\(I_n\\) are binary input values and \\(W_1\\), \\(W_2\\),, \\(W_n\\) are weights associated with each input, \\(S\\) is the weighted sum of inputs and \\(T\\) is the threshold value for the neuron activation. 14.1.2 Activation functions The weighted sum is submitted to an activation function, which translates the sum into a value based on the range of the function. (Figure 14.2). While it is possible to have a linear activation function, most activation functions are non-linear. Using only linear activations would essentially re-create ordinary regression using neural networks. Figure 14.2: Examples of activation functions used in neural nets Non-linear activation functions enable neural networks to model complex relationships between the inputs and outputs. In fact, neural nets can approximate any function to any desired degree of accuracy. This is known as the universal approximation theorem (Nielsen 2019).14 14.2 The road to machine learning with neural nets Beginning in the 1950s when digital computers became available, computer scientists became aware of the perceptrons based on the work of the Yale University professors. Scientists began trying to teach computers to learn. One example of the problems solved by these early neural networks was how to balance a broom standing upright on a moving car by controlling the motions of the cart back and forth. As the broom starts falling to the left, the cart learns to move to the left keep the room upright. While this was interesting, the promises of this early work were not realized. Scientists began trying to teach computers to learn. One example of the problems solved by these early neural networks was how to balance a broom standing upright on a moving car by controlling the motions of the cart back and forth. As the broom starts falling to the left, the cart learns to move to the left keep the room upright. While this was interesting, the promises of this early work were not realized. The excitement of the early 1950s gave way to disillusionment by the late 1960s. The disillusionment stemmed from the publication of a book by Marvin Minsky and Seymour Papert in 1969 showed some basic problems with perceptrons (Minsky and Papert 1969). For example, a perceptron could not model the so-called XOR (exclusive OR) problem. (Table 14.1.) The effect of Minsky and Paperts paper was that funding for research into neural nets dried up for more than 10 years. Table 14.1: The XOR problem Input A Input B Output 0 0 0 0 1 1 1 0 1 1 1 1 By the early 1980s, however, researchers had devised a way to incorporate multiple layers of perceptrons into models and this multiple layering enabled these models to become extremely flexible. After that flurry of research developed.15 14.3 Example of a neural network The class Iris data set consists of 150 observations 4 measured attributes (sepal length, sepal with, petal length, and petal.width) and one type of Iris as the target (setosa, versicolor, and virginica). The data was divided randomly into a training set (60%) and validation set ((40%). A neural network was fitted to the training data with a single hidden layer that had two nodes. The resultant network is shown with the 19 parameter estimates in Figure 14.3. The circles with 1 represent the constant terms. Figure 14.3: Neural net for Iris data The results are shown in two confusion matrices, one for the training data (Table 14.2) and one for the validation data(Table 14.3). No errors were made with the training data and just two with the validation data. Reduced accuracy with the validation data is expected since this data was not used to create the model. Table 14.2: Neural net results for the Iris training data Training data setosa versicolor virginica setosa 32 0 0 versicolor 0 28 0 viginica 0 0 30 Table 14.3: Neural net results for the Iris validation data Validation data setosa versicolor virginica setosa 18 0 0 versicolor 0 22 2 viginica 0 0 18 14.4 Training a neural net There are several ways that have been developed for estimating the weights in a neural net. We dont say estimation were talking about neural nets. We say training. And we use training data to adjust the weights and the model. Probably the most common structure of a neural network is the so-called feed-forward model. This means that when the network is trained, the input data flows through the model in only one direction toward the output or target. There is no feedback built into the model. (This is not to be confused with the estimation technique called back-propagation, discussed below.) Training a neural net is analogous to finding the coefficients for the best fit in regression model. One key difference, however, is with regression there is a single best fitting linear model which optimizes the fit to the set of training observations. There is no equivalent method for calculating the best set of weights for a neural network. Instead, an optimization model or routine is used to minimize some error function, such as the average squared error. This doesnt guarantee an optimal result, but instead looks for a good result. (It is possible to get local optima.) Probably the most common training method is called the back propagation method. It starts by randomly assigning a set of weights in the model and then calculates the value of the target. This provides the initial model, which most likely is not very good. Then, the error from the initial model is calculated by subtracting the predicted target value calculated by the neural net from the actual value of the target. This error is then fed back through the model and the weights are adjusted up or down to try to minimize the error. The name back propagation is suggested because the errors are sent back to the network. The adjustments made to the model weights are determined through a strategy called gradient descent. A gradient is a partial derivative of a function with more than one input variable. The gradient is measure of how much and in which direction the output of a function changes with small changes in the inputs. The sizes of changes in the gradient are set by the learning rate. Setting the learning rate too high may cause the algorithm to miss the optimum. Setting the learning rate too low is likely to lead to the optimum but at the cost of excessive computer time. The learning process is repeated many times repeated until some criterion is reached, such as a pre-set value of computer processing time, a specified maximum number of iterations, or until the error associated with the weights is negligible. This can be a slow process in terms of the number of iterations are required, but with modern computers many analyses can be completed in a matter of seconds. However, for some problems it could be hours. The actual mechanisms are quite sophisticated, having been developed over a period by mathematicians and computer scientists. 14.5 Considerations in using neural nets 14.5.1 Missing data Neural nets cannot handle missing data, so imputation of values must be performed if any data values are missing. 14.5.2 Representative data The training, verification and test data must be representative of the underlying model. The old computer science adage garbage in, garbage out could not apply more strongly than in neural modeling. If training data is not representative, then the models worth is at best compromised. At worst, it may be useless. It is worth spelling out the kind of problems which can corrupt a training set: 14.5.3 All eventualities must be covered A neural network can only learn from cases that are present. If people with incomes over $100,000 per year might be bad credit risks and your training data does not include anyone with incomes over $40,000 per year, you cannot expect the model to make correct decisions on previously unseen cases. Extrapolation is dangerous with any model, but some types of neural network may make particularly poor predictions in such circumstances. A network learns the easiest features it can. A classic (possibly apocryphal) illustration of this is a vision project designed to automatically recognize tanks. A network is trained on a hundred pictures including tanks, and a hundred not. It achieves a perfect 100% score. When tested on new data, it proves hopeless. The reason? The pictures of tanks are taken on dark, rainy days, the pictures without on sunny days. The network learns to distinguish the (trivial matter of) differences in overall light intensity. To work, the network would need training cases including all weather and lighting conditions under which it is expected to operate - not to mention all types of terrain, angles of shot, and distances. 14.5.4 Unbalanced data sets Since a network minimizes an overall error, the proportion of types of data in the set is critical. A network trained on a data set with 900 good cases and 100 bad will bias its decision towards good cases, as this allows the algorithm to lower the overall error (which is much more heavily influenced by the good cases). If the representation of good and bad cases is different in the real population, the networks decisions may be wrong. A good example would be disease diagnosis. Perhaps 90% of patients routinely tested are clear of a disease. A network is trained on an available data set with a 90/10 split. It is then used in diagnosis on patients complaining of specific problems, where the likelihood of disease is 50/50. The network will react over-cautiously and fail to recognize disease in some unhealthy patients. In contrast, if trained on the complaints data, and then tested on routine data, the network may raise a high number of false positives. In such circumstances, the data set may need to be crafted to take account of the distribution of data (e.g., you could replicate the less numerous cases, or remove some of the numerous cases), or the networks decisions modified by the inclusion of a loss matrix (Bishop, 1995). Often, the best approach is to ensure even representation of different cases, then to interpret the networks decisions accordingly. 14.5.5 The overfitting problem As with other data mining techniques the neural net model is trained on a separate set of data and then tested and validated on separate data sets. This is particularly important when using neural nets. Neural nets can predict too well. That is, given enough flexibility with several hidden layers and a large number of nodes, neural net model can be developed to perfectly predict the target in the training data. The problem is that over fitting like this does not generalize well. In other words, when you take a model that was fit perfectly to the training data, it may not predict the validation or testing data sets very well at all. This is overlearning. With sufficient iterations and enough nodes, neural nets can even fit random data. To illustrate this, the Iris data set was again used. This time, however, the predictor variables were ordered randomly. This meant that the predictors (sepal length, sepal with, petal length, and petal.width) and target (setosa, versicolor, and virginica) were no longer correctly matched This time a larger neural net was specified with two hidden layers, each with 10 nodes. The results are shown in the following two tables. The first confusion matrix is for the randomized training data. (Table 14.4) Note that perfect assignment of the types of Iris flowers was obtained. Table 14.4: Neural net results on training data: Randomized Iris data Training data setosa versicolor virginica setosa 30 0 0 versicolor 0 31 0 viginica 0 0 29 The second confusion matrix, which applied the model to the validation data, showed that model was overfit. (Table 14.5) The model could not accurately predict new data. Table 14.5: Neural net results on validation data: Randomized Iris data Validation data setosa versicolor virginica setosa 8 7 8 versicolor 2 5 6 viginica 10 7 7 14.6 Neural network example The German credit data was used to illustrate a neural network. The data set contains 1,000 observations with 20 predictors and a binary target: Credit risk. A study of credit card defaults in Taiwan is available from the Machine Learning The source is the Repository at UCI (Gromping 2019b) with a detailed report on the corrections provided by (Gromping 2019a). The number of bad credit ratings has been oversampled; the actual prevalence of bad credit is about 5%. The good versus bad ratings are based on the debtors assessment of risk prior to granting credit. There are also unequal costs of errors for this example. To account for the differences in the cost of errors, the cutoff threshold for predicting from the neural net had to be changed. The structure of the matrix and the resultant threshold is in Figure 14.4. The threshold was computed as 0.93 based on the costs and revenues associated with each cell in the 2X2 table using the approach developed by (Elkan 2001). Figure 14.4: Threshold calculations The variables in the German credit data set are shown in Table 14.6. Table 14.6: Variables in the German credit data set. Variable Description Status of existing checking account Status of the debtors checking account with the bank Duration in months Credit duration in months Credit history History of compliance with previous or concurrent credit contracts Purpose Purpose for which the credit is needed Credit amount Credit amount Savings account/bonds Debtors savings Present employment since Duration of debtors employment with current employer Installment rate in percentage of disposable income Credit installments as a percentage of debtors disposable income Personal status and sex Combined information on sex and marital status Other debtors / guarantors Is there another debtor or a guarantor for the credit? Present residence since Length of time (in years) the debtor lives in the present residence Property The debtors most valuable property Age in years Age in years Other installment plans Installment plans from providers other than the credit-giving bank Housing Type of housing the debtor lives in Number of existing credits at this bank Number of credits including (or had) at this bank Job Type of debtors job Number of people being liable to provide maintenance for Binary (1 or 2) Telephone Is there a telephone landline registered on the debtors name? Foreign worker Is the debtor a foreign worker? Score Has the credit contract been complied with (good) or not (bad)? The KNIME workflow for this example is shown below. (Figure 14.5 Figure 14.5: Workflow for neural net analysis of German Credit data set A description of each node is shown Table 14.7. Table 14.7: Description of workflow nodes for German Credit neural net anslysis. Node Label Description 1 File Reader Read CreditScore.csv 2 Category to Number Transfform all categorical variables to dummy indicators. 3 Normalizer Normanlize all variables to 0-1 min-max. 4 Partitioning Create training and validation data sets (70/30 ratio). 5 Rprop MLP Learner Neural net analysis using multi-layer perceptron model. 6 MLP Predictor Predict target using the validation data. 7 Scorer Assess predictions using threshold of 0.50. 8 Rule Engine Change threshold to 0.93. 9 Scorer Assess predictions using threshold of 0.93. The results for the neural nets analysis are shown in Figure 14.6 for both the 0.50 threshold and the 0.93 threshold. Note that increasing the threshold to assign a prediction to the good category to 0.93 reduced the overall performance of the model. While the number of correct good predictions decreased, the number of correct bad predictions increased. Figure 14.6: Results from neural net analysis As noted above, the number of bad credit ratings has been oversampled to be 30% but the actual percentage of bad credit is about 5%. So, the confusion matrices need to be rebalanced to reflect the correct percentages. The rebalanced confusion matrices are shown below. (Figure 14.7) Rebalancing is a straightforward process of adjusting each cell of the matrices so that the row margin totals (the actual numbers of good and bad credit cases) match the population. Figure 14.7: Workflow for neural net analysis of German Credit data set Also shown in Figure14.7 are the costs (and revenues, represented by negative costs) associated with each cell of the confusion matrix. By multiplying each cell of the predictions times the corresponding cell of the cost matrix, an overall net cost or revenue can be estimated. In this example, the predictions using the 0.93 threshold resulted in a revenue of 8,771 compared with the 0.50 default threshold which resulted in a revenue of 4,900. So, despite the reduced accuracy achieved with the higher threshold, the bank would be better off foregoing some good customers to avoid making bad credit decisions. References "],["cluster-analysis.html", "Chapter 15 Cluster analysis 15.1 Approaches to forming clusters", " Chapter 15 Cluster analysis Cluster analysis is an unsupervised set of methods for identifying groups of observations according to a measure of proximity, which could mean either similarity or distance. Clustering can be used to find groups of objects (records, people, items, documents) such that the objects within each group are similar in some sense to one another and distinct from the objects in other groups. The goal is to create within group homogeneity and between group heterogeneity. For example, if three clusters, denoted A with 10 objects, B with 20 objects, and C with 15 objects, are formed from a set of 45 observations, then the 10 objects in cluster A should be similar to one another according to some criterion. The same should be true of clusters B and C. However, it also should be the cases that the observations in cluster A are quite distinct (using the same criterion) from those in cluster B. This should likewise be the cases when considering clusters A and C, and clusters B and C. Cluster analysis is used in many scientific and applied fields since the goal of many studies is to simplify, condense, or classify situations  to take many data points and somehow extract the essential groups or segments from a large number of observations. [^Cluster analysis goes by different names in various disciplines and includes numerical taxonomy, pattern recognition, typology, and clumping.] No assumptions are needed made a priori regarding the number of groups or their structure; the goal is usually one of discovery. In some cases the objective is to find natural groups within a data structure. Discovering natural groups is not always easy because of the huge number of possible ways that groups can be formed. One approach might be to investigate all possible groupings and then decide (using some criterion) which is the best approach. This would be computationally unfeasible. For example, consider the 16 playing cards Figure 15.1. The number of possible clusters of size 2 from the set of 16 is 32,767. The number of possible clusters of size 3 from 16 objects is over 7 million. Figure 15.1: 16 playing cards. To cluster objects, it is necessary to define what criterion is to be used to group the objects. Similarity or distance must be computed between (Case 1 and Case 2), (Case 1 and Case 3) and (Case 2 and Case 3). Euclidean distance is typically used to measure dissimilarity. Euclidean distance can be generalized to any number of dimensions. The distance measure provides a quantitative index of the similarity between records or objects. The most common measure of distance is the squared Euclidean distance. There are other measures, which will be discussed later. 15.1 Approaches to forming clusters 15.1.1 Hierarchical versus partitioning methods There are many different clustering algorithms. One distinction is between hierarchical and partitioning (or non-hierarchical) methods. With the hierarchical approach, clusters are formed sequentially and once a case is grouped with other cases into a cluster, that case is never separated again as the program proceeds. With partitioning methods a pre-set number of clusters is specified and the algorithim proceeds iteratively to assign observations to clusters that are similar within and different across. 15.1.1.1 Hierarchical methods A simple conceptual example of hierarchical clustering is shown in Figure 15.2. Figure 15.2: The hierarchical clustering process. Nine objects (A through I) are assumed to have a set of characteristics (not shown) that are used to compute similarity. [^The type of hierarchical method described here is called the agglomerative approach, since the algorithm proceeds from all observations in individual clusters to all observations in a single cluster. Divisive methods are also available that work in the opposite way, with all objects in a single cluster.] At the start, all nine objects are in separate clusters so there are 36 [9X(9-1)/2] similarity measures. The algorithm ranks the 36 values from most similar to least similar and the two objects most similar (C and D) are joined into a single cluster. This results in eight clusters and the 28 [8X(8-1)/2] similarities among the eight are computed and again ranked from most to least similar. This time, note that there are various ways to compute similarities between clusters with just a single object and those with more than one. This complication will be discussed later. The process of continues until all nine objects are grouped into a single cluster. The hierarchical nature of this approach is evident from the fact that a cluster is formed, the algorithm does not go back at a later stage to check if that cluster is still optimum. Another way of looking at the hierarchical nature of the clustering process for this hypothetical example is with a dendrogram. (Figure 15.3) The closest objects are shown with the shortest links, with the sequence shown by the tree-like structure. Figure 15.3: Example of a dendrogram. 15.1.1.2 Partitioning methods The most popular partitioning algorithm is the widely available kmeans method. The k-means procedure performs a disjoint (each observation is to one and only one cluster) analysis on the basis of Euclidean distances computed using k quantitative variables. The analyst must specify the number of clusters in advance. k-means works like this: Choose initial centroids randomly for specified number of clusters. Assign each observation to nearest centroid based on Euclidean distance, forming a provisional set of clusters. Compute new centroids based in the new cluster. Reassign observations to nearest new centroids, again based on Euclidean distance. Repeat until maximum number of iterations is reached or until the largest change in any cluster centroid is less than a prespecified minimum. The k-means method does not form a tree structure for different numbers of clusters as does hierarchical procedures. Objects grouped with one number of clusters will not necessarily be grouped with a different number of clusters. Typically, the program is run with several different settings for the number of clusters and the results evaluated for the most desirable result (discussed in detail later). As an example, we will start off with the same nine objects as before. Then, the program is run four times with two to five clusters specified. The two clusters are shown in the red boxes. Next we asked the program to find three clusters. On the next slide, a 3 cluster solution is shown in red boxes. The previous two-cluster solution is shown in blue boxes. Notice that the structure has changed and the three cluster solution is not a subset or superset of the two cluster solution. Next we can obtain a 4 cluster solution. We can continue this this far as we like. But notice that the clusters are not formed in a hierarchical manner. The advantage of this approach is that the program doesnt get locked in to an early structure that might not be optimal. 15.1.2 Hard versus soft methods Another distinction in clustering methods is between hard clustering where each observation belongs to a single cluster and soft (or fuzzy) clustering where observations can belong to more than one cluster (Harmouch, n.d.). A typical hard clustering method is K-means. Table 15.1 Table 15.1: Beer brands data set Beer brands Calories Alcohol Cost Budweiser 144 4.7 0.43 Lowenbrau 157 4.9 0.48 Michelob 162 5.0 0.50 Kronenbourg 170 5.2 0.73 Heineken 152 5.0 0.77 Schmidts 147 4.7 0.30 Pabst Blue Ribbon 152 4.9 0.38 Miller Lite 99 4.3 0.43 Bud Light 113 3.7 0.44 Coors Light 102 4.1 0.46 Dos Equis 145 4.5 0.70 BecKs 150 4.7 0.76 Rolling Rock 144 4.7 0.36 Pabst Extra Light 66 2.3 0.38 Tuborg 155 5.0 0.43 Olympia Gold Light 72 2.9 0.46 Schlitz Light 97 4.2 0.47 Table 15.2 Table 15.2: Correlation matrix for beer brands data set Variables Calories Alcohol Cost Calories 1.000 0.923 0.360 Alcohol 0.923 1.000 0.319 Cost 0.360 0.319 1.000 Table 15.3 Table 15.3: Eigenvalues for beer brands data set Eigenvalue PCT of variance 2.127 70.9% 0.797 97.5% 0.076 100.0% 15.1.3 Applying hierarchical clusters Hierarchical clustering is not a single technique but rather a set of related techniques. Some software tools limit the choices to a small number of choices while others offer an almost bewildering list of different approaches. (KNIME is intermediate in the number of choices provided, but by using an R or Python node, many other types of hierarchical clustering can be used.) Some of the questions facing analysts using hierarchical clustering in different software tools include: (Everitt 2011) Should agglomerative or divisive measures be used? What measure of distance (or similarity) be used? Alternatives include Euclidean, cosine, Minkowski, Manhattan, Jaccard, and Tanimoto. Should the variables be weighted so that some variables have more influence on determining clusters or should weights be chosen so that all variables have equal influence? What measure of inter-cluster distance should be used? Alternatives include single linkage, complete linkage, centroid linkage, average linkage, and Wards method. It is frequently recommended that variables should be standardized prior to computing distances. This is usually a good idea, but there is no substitute for careful thought in the context of each individual problem. Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering algorithm. This aspect of the problem is emphasized less in the clustering literature than the algorithms themselves, since it depends on domain knowledge specifics and is less amenable to general research. (Hastie 2009) References "],["references.html", "References", " References "]]
