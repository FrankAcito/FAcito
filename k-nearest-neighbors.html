<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 k Nearest Neighbors | Analytics with KNIME and R</title>
  <meta name="description" content="This is a draft." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 k Nearest Neighbors | Analytics with KNIME and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a draft." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 k Nearest Neighbors | Analytics with KNIME and R" />
  
  <meta name="twitter:description" content="This is a draft." />
  

<meta name="author" content="F Acito" />


<meta name="date" content="2021-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deep-learning.html"/>
<link rel="next" href="tree-models.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover page</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1</b> What is analytics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#some-trends-in-analytics"><i class="fa fa-check"></i><b>1.2</b> Some trends in analytics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#broadening-of-application-areas"><i class="fa fa-check"></i><b>1.2.1</b> Broadening of application areas</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#generalization-of-the-notion-of-data"><i class="fa fa-check"></i><b>1.2.2</b> Generalization of the notion of data</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#a-trend-from-slicing-and-dicing-data-to-more-advanced-techniques"><i class="fa fa-check"></i><b>1.2.3</b> A trend from “slicing and dicing” data to more advanced techniques</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#more-advanced-data-visualization"><i class="fa fa-check"></i><b>1.2.4</b> More advanced data visualization</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-analytics-process-model"><i class="fa fa-check"></i><b>1.3</b> The analytics process model</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html"><i class="fa fa-check"></i><b>2</b> Business understanding and problem definition</a>
<ul>
<li class="chapter" data-level="2.1" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#expert-views"><i class="fa fa-check"></i><b>2.1</b> Expert views</a></li>
<li class="chapter" data-level="2.2" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#understanding-the-business"><i class="fa fa-check"></i><b>2.2</b> Understanding the business</a></li>
<li class="chapter" data-level="2.3" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#identifying-stakeholders"><i class="fa fa-check"></i><b>2.3</b> Identifying stakeholders</a></li>
<li class="chapter" data-level="2.4" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#structured-versus-unstructured-problems"><i class="fa fa-check"></i><b>2.4</b> Structured versus unstructured problems</a></li>
<li class="chapter" data-level="2.5" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#framing-the-problem"><i class="fa fa-check"></i><b>2.5</b> Framing the problem</a></li>
<li class="chapter" data-level="2.6" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#appendix-some-tools-for-problem-definition"><i class="fa fa-check"></i>Appendix: Some tools for problem definition</a>
<ul>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#right-to-left-thinking"><i class="fa fa-check"></i>Right to left thinking</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#reversing-the-problem"><i class="fa fa-check"></i>Reversing the problem</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#open-the-problem-with-whys"><i class="fa fa-check"></i>Open the problem with “whys”</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#challenge-assumptions"><i class="fa fa-check"></i>Challenge assumptions</a></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#chunking"><i class="fa fa-check"></i>Chunking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="business-understanding-and-problem-definition.html"><a href="business-understanding-and-problem-definition.html#problems"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html"><i class="fa fa-check"></i><b>3</b> Introduction to KNIME</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#the-knime-workbench"><i class="fa fa-check"></i><b>3.1</b> The KNIME Workbench</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#elements-of-the-knime-workbench"><i class="fa fa-check"></i><b>3.1.1</b> Elements of the KNIME Workbench</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#learning-to-use-knime"><i class="fa fa-check"></i><b>3.2</b> Learning to use KNIME</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-extensions-and-integrations"><i class="fa fa-check"></i><b>3.3</b> KNIME extensions and integrations</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-1-predicting-heart-disease"><i class="fa fa-check"></i><b>3.4</b> KNIME workflow example #1: Predicting heart disease</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#knime-workflow-example-2-preparation-of-hospital-data"><i class="fa fa-check"></i><b>3.5</b> KNIME workflow example #2: Preparation of hospital data</a></li>
<li class="chapter" data-level="3.6" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#summary-1"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="introduction-to-knime.html"><a href="introduction-to-knime.html#problems-1"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>4</b> Data preparation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-preparation.html"><a href="data-preparation.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="data-preparation.html"><a href="data-preparation.html#obtaining-the-needed-data"><i class="fa fa-check"></i><b>4.2</b> Obtaining the needed data</a></li>
<li class="chapter" data-level="4.3" data-path="data-preparation.html"><a href="data-preparation.html#data-cleaning"><i class="fa fa-check"></i><b>4.3</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data-preparation.html"><a href="data-preparation.html#missing-values"><i class="fa fa-check"></i><b>4.3.1</b> Missing values</a></li>
<li class="chapter" data-level="4.3.2" data-path="data-preparation.html"><a href="data-preparation.html#outliers"><i class="fa fa-check"></i><b>4.3.2</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature engineering</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-preparation.html"><a href="data-preparation.html#data-transformations"><i class="fa fa-check"></i><b>4.4.1</b> Data transformations</a></li>
<li class="chapter" data-level="4.4.2" data-path="data-preparation.html"><a href="data-preparation.html#data-exploration"><i class="fa fa-check"></i><b>4.4.2</b> Data exploration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html"><i class="fa fa-check"></i><b>5</b> Principal components analytics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#approaches-to-dimension-reduction"><i class="fa fa-check"></i><b>5.1</b> Approaches to dimension reduction</a></li>
<li class="chapter" data-level="5.2" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#description"><i class="fa fa-check"></i><b>5.2</b> Description</a></li>
<li class="chapter" data-level="5.3" data-path="principal-components-analytics.html"><a href="principal-components-analytics.html#the-pca-model"><i class="fa fa-check"></i><b>5.3</b> The PCA model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html"><i class="fa fa-check"></i><b>6</b> Evaluating predictive models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#training-testing-and-validation-samples"><i class="fa fa-check"></i><b>6.2</b> Training, Testing, and Validation samples</a></li>
<li class="chapter" data-level="6.3" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-continuous-versus-discrete-targets"><i class="fa fa-check"></i><b>6.3</b> Evaluating continuous versus discrete targets</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-continuous-targets"><i class="fa fa-check"></i><b>6.3.1</b> Evaluating performance with continuous targets</a></li>
<li class="chapter" data-level="6.3.2" data-path="evaluating-predictive-models.html"><a href="evaluating-predictive-models.html#evaluating-performance-with-classification-models"><i class="fa fa-check"></i><b>6.3.2</b> Evaluating performance with classification models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-techniques"><i class="fa fa-check"></i><b>7.2</b> Regression techniques</a></li>
<li class="chapter" data-level="7.3" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-explanation"><i class="fa fa-check"></i><b>7.3</b> Regression for explanation</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-for-prediction"><i class="fa fa-check"></i><b>7.4</b> Regression for prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#revisiting-regression-assumptions"><i class="fa fa-check"></i><b>7.4.1</b> Revisiting regression assumptions</a></li>
<li class="chapter" data-level="7.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction-example-used-toyota-corollas"><i class="fa fa-check"></i><b>7.4.2</b> Prediction example: Used Toyota Corollas</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#appendix-a-brief-history-of-regression"><i class="fa fa-check"></i>Appendix: A brief history of regression</a></li>
<li class="chapter" data-level="" data-path="multiple-regression.html"><a href="multiple-regression.html#problems-2"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#example-with-a-single-predictor"><i class="fa fa-check"></i><b>8.1</b> Example with a single predictor</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-predictive-analytic-in-hr"><i class="fa fa-check"></i><b>8.2</b> Example: Predictive analytic in HR</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#predictor-interpretation-and-importance"><i class="fa fa-check"></i><b>8.3</b> Predictor interpretation and importance</a></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Regularized logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#probability-calibration"><i class="fa fa-check"></i><b>8.5</b> Probability calibration</a></li>
<li class="chapter" data-level="8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluation-of-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Evaluation of logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ensemble-models.html"><a href="ensemble-models.html"><i class="fa fa-check"></i><b>9</b> Ensemble models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ensemble-models.html"><a href="ensemble-models.html#creating-ensemble-models"><i class="fa fa-check"></i><b>9.1</b> Creating ensemble models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ensemble-models.html"><a href="ensemble-models.html#reduced-variation"><i class="fa fa-check"></i><b>9.1.1</b> Reduced variation</a></li>
<li class="chapter" data-level="9.1.2" data-path="ensemble-models.html"><a href="ensemble-models.html#improved-performance"><i class="fa fa-check"></i><b>9.1.2</b> Improved performance</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ensemble-models.html"><a href="ensemble-models.html#parallel-and-sequential-learners"><i class="fa fa-check"></i><b>9.2</b> Parallel and sequential learners</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ensemble-models.html"><a href="ensemble-models.html#bagging-bootstrap-aggregating"><i class="fa fa-check"></i><b>9.2.1</b> Bagging (Bootstrap Aggregating)</a></li>
<li class="chapter" data-level="9.2.2" data-path="ensemble-models.html"><a href="ensemble-models.html#random-forests"><i class="fa fa-check"></i><b>9.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="9.2.3" data-path="ensemble-models.html"><a href="ensemble-models.html#adaboost"><i class="fa fa-check"></i><b>9.2.3</b> AdaBoost</a></li>
<li class="chapter" data-level="9.2.4" data-path="ensemble-models.html"><a href="ensemble-models.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>9.2.4</b> Gradient Boosting Machines</a></li>
<li class="chapter" data-level="9.2.5" data-path="ensemble-models.html"><a href="ensemble-models.html#xgboost"><i class="fa fa-check"></i><b>9.2.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ensemble-models.html"><a href="ensemble-models.html#example-of-ensemble-modeling-for-a-continuous-target"><i class="fa fa-check"></i><b>9.3</b> Example of ensemble modeling for a continuous target</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>10</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="10.1" data-path="naive-bayes.html"><a href="naive-bayes.html#a-thought-problem"><i class="fa fa-check"></i><b>10.1</b> A thought problem</a></li>
<li class="chapter" data-level="10.2" data-path="naive-bayes.html"><a href="naive-bayes.html#bayes-theorem-applied-to-predictive-analytics"><i class="fa fa-check"></i><b>10.2</b> Bayes Theorem applied to predictive analytics</a></li>
<li class="chapter" data-level="10.3" data-path="naive-bayes.html"><a href="naive-bayes.html#illustration-of-naïve-bayes-with-a-toy-data-set"><i class="fa fa-check"></i><b>10.3</b> Illustration of Naïve Bayes with a “toy” data set</a></li>
<li class="chapter" data-level="10.4" data-path="naive-bayes.html"><a href="naive-bayes.html#the-assumption-of-conditional-independence"><i class="fa fa-check"></i><b>10.4</b> The assumption of conditional independence</a></li>
<li class="chapter" data-level="10.5" data-path="naive-bayes.html"><a href="naive-bayes.html#naïve-bayes-with-continuous-predictors"><i class="fa fa-check"></i><b>10.5</b> Naïve Bayes with continuous predictors</a></li>
<li class="chapter" data-level="10.6" data-path="naive-bayes.html"><a href="naive-bayes.html#laplace-smoothing"><i class="fa fa-check"></i><b>10.6</b> Laplace Smoothing</a></li>
<li class="chapter" data-level="10.7" data-path="naive-bayes.html"><a href="naive-bayes.html#example-using-naïve-bayes-with-churn-data"><i class="fa fa-check"></i><b>10.7</b> Example using naïve Bayes with churn data</a></li>
<li class="chapter" data-level="10.8" data-path="naive-bayes.html"><a href="naive-bayes.html#spam-detection-using-naïve-bayes"><i class="fa fa-check"></i><b>10.8</b> Spam detection using naïve Bayes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>11</b> Deep learning</a></li>
<li class="chapter" data-level="12" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>12</b> k Nearest Neighbors</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#k-nearest-neighbors-and-memory-based-learning"><i class="fa fa-check"></i><b>12.1</b> k nearest neighbors and memory-based learning</a></li>
<li class="chapter" data-level="12.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#typical-applications"><i class="fa fa-check"></i><b>12.2</b> Typical applications</a></li>
<li class="chapter" data-level="12.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#what-is-knn"><i class="fa fa-check"></i><b>12.3</b> What is kNN?</a></li>
<li class="chapter" data-level="12.4" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#a-two-dimensional-graphic-example-of-knn"><i class="fa fa-check"></i><b>12.4</b> A two-dimensional graphic example of kNN</a></li>
<li class="chapter" data-level="12.5" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-of-knn-diagnosing-heart-disease"><i class="fa fa-check"></i><b>12.5</b> Example of kNN: Diagnosing heart disease</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#results"><i class="fa fa-check"></i><b>12.5.1</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-continuous-targets"><i class="fa fa-check"></i><b>12.6</b> kNN for continuous targets</a></li>
<li class="chapter" data-level="12.7" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#knn-for-multiclass-target-variables"><i class="fa fa-check"></i><b>12.7</b> kNN for multiclass target variables</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="tree-models.html"><a href="tree-models.html"><i class="fa fa-check"></i><b>13</b> Tree models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="tree-models.html"><a href="tree-models.html#classification-trees"><i class="fa fa-check"></i><b>13.1</b> Classification trees</a></li>
<li class="chapter" data-level="13.2" data-path="tree-models.html"><a href="tree-models.html#forming-classification-trees"><i class="fa fa-check"></i><b>13.2</b> Forming classification trees</a></li>
<li class="chapter" data-level="13.3" data-path="tree-models.html"><a href="tree-models.html#varieties-of-classification-tree-algorithms"><i class="fa fa-check"></i><b>13.3</b> Varieties of classification tree algorithms</a></li>
<li class="chapter" data-level="13.4" data-path="tree-models.html"><a href="tree-models.html#criteria-for-splitting-and-growing-a-tree"><i class="fa fa-check"></i><b>13.4</b> Criteria for splitting and growing a tree</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="tree-models.html"><a href="tree-models.html#the-gini-index"><i class="fa fa-check"></i><b>13.4.1</b> The Gini index</a></li>
<li class="chapter" data-level="13.4.2" data-path="tree-models.html"><a href="tree-models.html#information-gain"><i class="fa fa-check"></i><b>13.4.2</b> Information Gain</a></li>
<li class="chapter" data-level="13.4.3" data-path="tree-models.html"><a href="tree-models.html#chi-square-as-a-splitting-criterion"><i class="fa fa-check"></i><b>13.4.3</b> Chi-square as a splitting criterion</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="tree-models.html"><a href="tree-models.html#overfitting"><i class="fa fa-check"></i><b>13.5</b> Overfitting</a></li>
<li class="chapter" data-level="13.6" data-path="tree-models.html"><a href="tree-models.html#example-of-a-classification-tree"><i class="fa fa-check"></i><b>13.6</b> Example of a classification tree</a></li>
<li class="chapter" data-level="13.7" data-path="tree-models.html"><a href="tree-models.html#regression-trees"><i class="fa fa-check"></i><b>13.7</b> Regression trees</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="tree-models.html"><a href="tree-models.html#how-regression-trees-work"><i class="fa fa-check"></i><b>13.7.1</b> How regression trees work</a></li>
<li class="chapter" data-level="13.7.2" data-path="tree-models.html"><a href="tree-models.html#example-predicting-home-prices"><i class="fa fa-check"></i><b>13.7.2</b> Example: Predicting home prices</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="tree-models.html"><a href="tree-models.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>13.8</b> Strengths and weaknesses</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>14</b> Neural networks</a>
<ul>
<li class="chapter" data-level="14.1" data-path="neural-networks.html"><a href="neural-networks.html#what-are-artificial-neural-networks"><i class="fa fa-check"></i><b>14.1</b> What are artificial neural networks?</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="neural-networks.html"><a href="neural-networks.html#human-neurons-to-mathematical-models"><i class="fa fa-check"></i><b>14.1.1</b> Human neurons to mathematical models</a></li>
<li class="chapter" data-level="14.1.2" data-path="neural-networks.html"><a href="neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>14.1.2</b> Activation functions</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="neural-networks.html"><a href="neural-networks.html#the-road-to-machine-learning-with-neural-nets"><i class="fa fa-check"></i><b>14.2</b> The road to machine learning with neural nets</a></li>
<li class="chapter" data-level="14.3" data-path="neural-networks.html"><a href="neural-networks.html#example-of-a-neural-network"><i class="fa fa-check"></i><b>14.3</b> Example of a neural network</a></li>
<li class="chapter" data-level="14.4" data-path="neural-networks.html"><a href="neural-networks.html#training-a-neural-net"><i class="fa fa-check"></i><b>14.4</b> Training a neural net</a></li>
<li class="chapter" data-level="14.5" data-path="neural-networks.html"><a href="neural-networks.html#considerations-in-using-neural-nets"><i class="fa fa-check"></i><b>14.5</b> Considerations in using neural nets</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="neural-networks.html"><a href="neural-networks.html#missing-data"><i class="fa fa-check"></i><b>14.5.1</b> Missing data</a></li>
<li class="chapter" data-level="14.5.2" data-path="neural-networks.html"><a href="neural-networks.html#representative-data"><i class="fa fa-check"></i><b>14.5.2</b> Representative data</a></li>
<li class="chapter" data-level="14.5.3" data-path="neural-networks.html"><a href="neural-networks.html#all-eventualities-must-be-covered"><i class="fa fa-check"></i><b>14.5.3</b> All eventualities must be covered</a></li>
<li class="chapter" data-level="14.5.4" data-path="neural-networks.html"><a href="neural-networks.html#unbalanced-data-sets"><i class="fa fa-check"></i><b>14.5.4</b> Unbalanced data sets</a></li>
<li class="chapter" data-level="14.5.5" data-path="neural-networks.html"><a href="neural-networks.html#the-overfitting-problem"><i class="fa fa-check"></i><b>14.5.5</b> The overfitting problem</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="neural-networks.html"><a href="neural-networks.html#neural-network-example"><i class="fa fa-check"></i><b>14.6</b> Neural network example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>15</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#approaches-to-forming-clusters"><i class="fa fa-check"></i><b>15.1</b> Approaches to forming clusters</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hierarchical-versus-partitioning-methods"><i class="fa fa-check"></i><b>15.1.1</b> Hierarchical versus partitioning methods</a></li>
<li class="chapter" data-level="15.1.2" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hard-versus-soft-methods"><i class="fa fa-check"></i><b>15.1.2</b> “Hard” versus “soft” methods</a></li>
<li class="chapter" data-level="15.1.3" data-path="cluster-analysis.html"><a href="cluster-analysis.html#applying-hierarchical-clusters"><i class="fa fa-check"></i><b>15.1.3</b> Applying hierarchical clusters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analytics with KNIME and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-nearest-neighbors" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> k Nearest Neighbors</h1>
<div id="k-nearest-neighbors-and-memory-based-learning" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> k nearest neighbors and memory-based learning</h2>
<p>K nearest neighbors (kNN) is another example of a supervised model, a very simple supervised model. It may be the simplest, most intuitive model that is used for data mining. Despite the simplicity of the model, it can work quite well with large data sets. It is also one of the top 10 data mining tools in terms of popularity of usage. <span class="citation">(<a href="#ref-Tavasoli" role="doc-biblioref"><span>“Top 10 Machine Learning Algorithms You Should Know in 2021,”</span> n.d.</a>)</span></p>
<p>kNN is an example of a family of algorithms known as instance-based or memory-based learning that classify new objects by their similarity to previously known objects. kNN does not require many assumptions about the input or predictor data or anything about error distributions. Formally, it is a non-parametric model. In parametric models (such as multiple regression), you must make assumptions about the distribution of the error term and then estimates are made of the parameters of the distribution. No such considerations are involved with kNN.</p>
<p>A training phase is used to determine the best value for k (the number of “neighbors” used to compute similarity), but otherwise no training is needed. It can be directly applied to all of the data. If you have a data set, you simply apply kNN. An important disadvantage of kNN is that the model must be run through all the data available each time a new observation needs to be classified. This can be time consuming for large data sets. With logistic regression or OLS, you only save the model and apply it to new data, which is much faster. Since there is no “model” per se in K-nearest neighbors, the model essentially is created each time an analysis is needed.</p>
</div>
<div id="typical-applications" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Typical applications</h2>
<p>If we look at typical applications, the list is much like other predictive models such as logistic regression and decision trees, such as:</p>
<ul>
<li>Flagging fraudulent insurance claims.</li>
<li>Predicting customer response to promotional offers.</li>
<li>Selecting the most effective treatment for a medical condition.</li>
<li>Classifying free-text responses.</li>
<li>Recommending the next offer to a customer in retail settings.</li>
<li>Searching for similar documents.</li>
<li>Recommender systems.</li>
</ul>
</div>
<div id="what-is-knn" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> What is kNN?</h2>
<p>KNN is a method for classifying objects based on similarity. It is called a “lazy” algorithm, which means is that it does not use the training data points to do any generalization and is contrasted with “eager” algorithms. The differences are described in Table 9.1. In other words, there is no explicit training phase, or it is very minimal. Most of the lazy algorithms – especially kNN – make decisions based on the entire data set. Some of the distinctions between lazy and eager learners are shown here.</p>
<ul>
<li>With lazy learners
<ul>
<li>The data is stored, not learned from it.</li>
<li>Classifications are made as soon as a new observation is received.</li>
<li>The model is richer since it does not rely on a single pre-specified model.</li>
<li>Time to classify new observations can be considerable with large data sets.</li>
<li>Learning time is non-existent (except for determining the optimal value of k) and the need for minor preprocessing.</li>
</ul></li>
<li>With eager learners
<ul>
<li>A model is developed (learned) based on the training data.</li>
<li>The model is used to classify new observations as they are received.</li>
<li>The model depends upon a single function that is derived in the learning phase.</li>
<li>Classification of new observations is very fast.</li>
<li>Learning time can be considerable.</li>
</ul></li>
</ul>
<p>The algorithm works like this. A labeled data set with a known categorical target vector, <strong>Y</strong> (which may be binary or multichotomous) and a matrix <strong>X</strong> with <em>p</em> potential predictors or features is used. Each case is considered a point in a multidimensional feature space. This allows the computation of “distances” among the points (or cases) based on locations (values) in the feature space.</p>
<p>KNN is then used to classify a new observation that is described by <em>p</em> variables (X<sub>1</sub>, X<sub>2</sub>, …, X<sub>p</sub>). To do so, the algorithm computes the distance of the new observation to every other observation in the data set. Euclidean distance is typically used, but other metrics such as the absolute value of differences, squared Euclidean distance, Jaccard distance, Manhattan distance, or cosine distance are also used. In most cases it is important to standardize the <strong>X</strong> variables prior to determining distances to avoid having the variables with the largest numerical values dominating the computed distances.</p>
<p>The distances of the new observation to each row of the data set are computed and are ranked from smallest to largest. The <em>k</em> smallest distances (the “nearest neighbors”) selected. No assumptions about the form of the relationship between <strong>Y</strong> and the X<sub>i</sub>’s is made; no parameters are estimated. A majority “vote” of its <em>k</em> neighbors is taken and the category with the smallest distance is selected as the predicted class. <em>k</em> – the number of nearest points or neighbors to consider in making an assignment to a category – is usually an odd number so that no ties are formed; if an even number is set for <em>k</em>, then random choices for Y class are made for ties.) Since a majority is used to make predictions, kNN can be applied to targets with two or more than two possible outcomes.</p>
<p>There is a trade-off implicit here. While there is only a minimal training phase used to determine the best <em>k</em>, deployment of kNN is not efficient in terms of processing time and memory requirements. More time is needed because all data points might take part in determining a prediction. Furthermore, all of the data must be stored and available for deployment.</p>
</div>
<div id="a-two-dimensional-graphic-example-of-knn" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> A two-dimensional graphic example of kNN</h2>
<p>A very simple example is used to illustrate how kNN works. Figure 9.1 shows a plot of characteristics of different types of homes. The two predictors are area in square feet and number of rooms. Both variables are standardized to zero mean and unit standard deviation.</p>
<p>Toward the center of the chart is an “unknown” type of home. The area and number of rooms for the unknown were standardized using the same mean and standard deviation as was computed for the known data. The distances of the unknown to every data point in the original set are represented by the arrows. The three closest known observations are circled, assuming for this example the k = 3. Two of the closest homes are flats and one is an apartment. Using the majority rule, the unknown is classified as a flat. (See Figure <a href="k-nearest-neighbors.html#fig:knngraphic">12.1</a>)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knngraphic"></span>
<img src="images_knn/GraphicalkNN.PNG" alt="Graphical illustration of kNN." width="75%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.1: Graphical illustration of kNN.
</p>
</div>
</div>
<div id="example-of-knn-diagnosing-heart-disease" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Example of kNN: Diagnosing heart disease</h2>
<p>This database is part of a larger study of factors associated with heart disease collected at the Cleveland Clinic. The data set contains test results on 303 patients measured on 13 attributes plus a target variable indicating the presence or absence of heart disease. This data set has been used in several published studies on machine learning. The variables are listed in Figure <a href="k-nearest-neighbors.html#fig:HeartVariables">12.2</a> along with an indication of each as numeric or categorical. For each of the categorical variables, the levels are shown as well.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:HeartVariables"></span>
<img src="images_knn/VariableListHeart.PNG" alt="Variables in the Cleveland Heart Disease study." width="35%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.2: Variables in the Cleveland Heart Disease study.
</p>
</div>
<p>KNIME was used to determine the best value for k by finding the accuracy for values of <em>k</em> from 1 to 20. A plot of Accuracy versus <em>k</em> is shown in Figure <a href="k-nearest-neighbors.html#fig:accuracyVSk">12.3</a>, which indicates that the best <em>k</em> was 7.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:accuracyVSk"></span>
<img src="images_knn/ValueOfKHeartData.PNG" alt="Accuracy versus k with heart data." width="75%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.3: Accuracy versus k with heart data.
</p>
</div>
<p>The KNIME workflow is shown in Figure <a href="k-nearest-neighbors.html#fig:kNNWorkflow1fig">12.4</a>. Descriptions of each node in the workflow are given in Table <a href="k-nearest-neighbors.html#tab:kNNnodelist1">12.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kNNWorkflow1fig"></span>
<img src="images_knn/HeartKNNWorkFlow.PNG" alt="kNN workflow for heart disease data." width="85%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.4: kNN workflow for heart disease data.
</p>
</div>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:kNNnodelist1">Table 12.1: </span>Workflow nodes for kNN with heart data
</caption>
<thead>
<tr>
<th style="text-align:center;">
Node
</th>
<th style="text-align:left;">
Label
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
1
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
File Reader
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Read the file heat_data.csv
</td>
</tr>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
2
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Data Explorer
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Obtain histograms and descriptive statistics.
</td>
</tr>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
3
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Number To String
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Create 70/30 split into training and test partitions.
</td>
</tr>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
4
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Naïve Bayes Learner
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Run naïve Bayes on training data.
</td>
</tr>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
5
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Naïve Bayes Predictor
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Use the naïve Bayes model to predict test data.
</td>
</tr>
<tr>
<td style="text-align:center;width: 2em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
6
</td>
<td style="text-align:left;width: 12em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Scorer
</td>
<td style="text-align:left;width: 30em; vertical-align: topcolor: black !important;background-color: white !important;padding: 2px;">
Calculate performance metrics and confusion matrix.
</td>
</tr>
</tbody>
</table>
<div id="results" class="section level3" number="12.5.1">
<h3><span class="header-section-number">12.5.1</span> Results</h3>
<p>The confusion matrix for the kNN model with k = 7 is in Table <a href="k-nearest-neighbors.html#tab:confusionkNNheart">12.2</a>.</p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:confusionkNNheart">Table 12.2: </span>Confusion matrix for predicting heart disease with kNN
</caption>
<thead>
<tr>
<th style="text-align:left;">
Actual
</th>
<th style="text-align:center;">
Healthy
</th>
<th style="text-align:center;">
Heart disease
</th>
<th style="text-align:center;">
Totals
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Healthy
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
45
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
3
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;">
48
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
Heart disease
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
7
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; border-right:1px solid;vertical-align: top">
34
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;">
41
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px; Border-top: 1px solid;width: 8em; border-right:1px solid;vertical-align: top">
Totals
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px; Border-top: 1px solid;width: 8em; border-right:1px solid;vertical-align: top">
52
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px; Border-top: 1px solid;width: 8em; border-right:1px solid;vertical-align: top">
37
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px; Border-top: 1px solid;">
89
</td>
</tr>
</tbody>
</table>
<p>Accuracy measures for the kNN analysis are in Table <a href="k-nearest-neighbors.html#tab:accuracykNNheart">12.3</a></p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:accuracykNNheart">Table 12.3: </span>Accuracy metrics for kNN with heart disease data
</caption>
<thead>
<tr>
<th style="text-align:center;">
Model
</th>
<th style="text-align:center;">
ROC AUC
</th>
<th style="text-align:center;">
Accuracy
</th>
<th style="text-align:center;">
Sensitivity
</th>
<th style="text-align:center;">
Specificity
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 10em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
kNN
</td>
<td style="text-align:center;width: 10em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
0.915
</td>
<td style="text-align:center;width: 10em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
0.888
</td>
<td style="text-align:center;width: 10em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
0.829
</td>
<td style="text-align:center;width: 10em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
0.938
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="knn-for-continuous-targets" class="section level2" number="12.6">
<h2><span class="header-section-number">12.6</span> kNN for continuous targets</h2>
<p>While kNN is primarily a method for classification, it can also be used with continuous target variables much like ordinary least squares (OLS) regression. KNIME does not include a node for kNN regression, so a small R Snippet was created to use the package FNN. One advantage of kNN regression is that non-linear relationships can be easily captured. Ordinary regression requires transformations and/or adding predictors to capture non-linearities.</p>
<p>A simple data set with a single predictor (X) and a continuous target(Y) was created. A scatterplot of the data is shown in Figure <a href="k-nearest-neighbors.html#fig:kNNnonlinear">12.5</a>. Note that the relationship is non-linear and slightly concave upward.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kNNnonlinear"></span>
<img src="images_knn/Nonlinearscatterplot.PNG" alt="Simulated data with non-linear relationship between Y and X." width="50%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.5: Simulated data with non-linear relationship between Y and X.
</p>
</div>
<p>A KNIME workflow was created, shown in Figure <a href="k-nearest-neighbors.html#fig:workflowNonlinear">12.6</a>, to compare kNN and OLS. For this small demonstration data, a test data subset was not created.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:workflowNonlinear"></span>
<img src="images_knn/workflowForRegression.PNG" alt="KNIME workflow to compare kNN regression with OLS." width="50%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.6: KNIME workflow to compare kNN regression with OLS.
</p>
</div>
<p>KNIME does not have a node to perform kNN regression, so this R code was inserted in Node 7 which contains an R Snippet. Note that this R code will have to be customized for new problems.</p>
<div class="nobullet">
<ul>
<li>library(FNN)</li>
<li>mydata &lt;- as.data.frame(knime.in)</li>
<li>TrainData &lt;- as.data.frame(mydata[,1])</li>
<li>TrainTarget &lt;- mydata[,2]</li>
<li>TestData &lt;- as.data.frame(mydata[,1])</li>
<li>YKNN = knn.reg(train = TrainData, test = TestData, y = TrainTarget, k = 3)</li>
<li>knime.out &lt;- as.data.frame(cbind(mydata,YKNN$pred))<br />
</li>
</ul>
</div>
<p>The metrics for the two models were:</p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:OLSvsKNN">Table 12.4: </span>Comparison of kNN and OLS with non-linear data
</caption>
<thead>
<tr>
<th style="text-align:left;">
Metric
</th>
<th style="text-align:center;">
kNN
</th>
<th style="text-align:center;">
OLS
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 15em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
R-squared
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 6em; border-right:1px solid;">
0.992
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 6em; border-right:1px solid;">
0.935
</td>
</tr>
<tr>
<td style="text-align:left;width: 15em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
Mean absolute error
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
2.545
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
7.209
</td>
</tr>
<tr>
<td style="text-align:left;width: 15em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
Root mean squared error
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
3.132
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
8.694
</td>
</tr>
<tr>
<td style="text-align:left;width: 15em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
Mean absolute percentage error
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
0.645
</td>
<td style="text-align:center;width: 6em; border-right:1px solid;color: black !important;background-color: white !important;padding: 2px;">
1.937
</td>
</tr>
</tbody>
</table>
<p>The metrics show that kNN more accurately captured the relationship in the data. OLS could be improved by creating a polynomial model, of course, but the point is that kNN regression did not require a model to be specified a priori.</p>
<p>Plots of the predicted Y (on the y-axis) and the actual Y values (on the x-axis) are show in Figures <a href="k-nearest-neighbors.html#fig:olsResultsScatterplot">12.7</a> and <a href="k-nearest-neighbors.html#fig:kNNResultsScatterplot">12.8</a> for the two models. Note that the OLS model did not capture the non-linearity and created a downward concave result.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:olsResultsScatterplot"></span>
<img src="images_knn/OlScatterPlot.PNG" alt="OLS results." width="40%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.7: OLS results.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kNNResultsScatterplot"></span>
<img src="images_knn/kNNScatterplot.PNG" alt="kNN results." width="40%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.8: kNN results.
</p>
</div>
</div>
<div id="knn-for-multiclass-target-variables" class="section level2" number="12.7">
<h2><span class="header-section-number">12.7</span> kNN for multiclass target variables</h2>
<p>KNN is also effective with target variables that have more than two classes. An example data set was obtained from the UCI Machine Learning Repository to illustrate kNN with a multiclass target. The data set has 214 observations, a 6-level categorical target, Type of glass, and nine continuous predictors. The kNN workflow from KNIME is shown in Figure <a href="k-nearest-neighbors.html#fig:kNNGlassWorkflow">12.9</a>. The model was run with a 50/50 split between the training and test rows and k was set to two, which was found to result in the highest accuracy.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kNNGlassWorkflow"></span>
<img src="images_knn/knnGlassExample.PNG" alt="KNIME workflow for kNN analysis of glass data." width="75%" style="background-color: #9ecff7; padding:1px; display: inline-block;" />
<p class="caption">
Figure 12.9: KNIME workflow for kNN analysis of glass data.
</p>
</div>
<p>The confusion matrix for predicting type of glass in the test data set is shown in Table @ref[tab:GlassConfusionMatrix]. The accuracy varies by type of glass and the overall accuracy is just over 73%. While this may not seem very good, predicting to six classes with a small data set is not an easy task for the model.</p>
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:GlassConfusionMatrix">Table 12.5: </span>Confusion matrix for glass data
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variables
</th>
<th style="text-align:center;">
Buliding float
</th>
<th style="text-align:center;">
Buliding nonfloat
</th>
<th style="text-align:center;">
Vehicle windows
</th>
<th style="text-align:center;">
Containers
</th>
<th style="text-align:center;">
Tableware
</th>
<th style="text-align:center;">
Headlamps
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Buliding float
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
24
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
6
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
5
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Buliding nonfloat
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
4
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
30
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
4
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Vehicle windows
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
1
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
5
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Containers
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
4
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Tableware
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
1
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
1
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
</tr>
<tr>
<td style="text-align:left;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
Headlamps
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
2
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
1
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 17em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
1
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
0
</td>
<td style="text-align:center;color: black !important;background-color: white !important;padding: 2px;width: 8em; ">
12
</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Tavasoli" class="csl-entry">
<span>“Top 10 Machine Learning Algorithms You Should Know in 2021.”</span> n.d. <a href="https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article">https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deep-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tree-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TextbookDraft.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
